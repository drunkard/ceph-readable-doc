

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Erasure Coding Direct Reads &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" />
    <link rel="next" title="last_epoch_started" href="../../last_epoch_started/" />
    <link rel="prev" title="Erasure coding enhancements" href="../enhancements/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../internals/">Ceph 内幕</a></li>
          <li class="breadcrumb-item"><a href="../../">OSD 开发者文档</a></li>
          <li class="breadcrumb-item"><a href="../">纠删码编码的归置组</a></li>
      <li class="breadcrumb-item active">Erasure Coding Direct Reads</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/dev/osd_internals/erasure_coding/direct_reads.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../../" class="icon icon-home"> Ceph
          

          
          </a>

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_guide/">开发者指南</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../internals/">Ceph 内幕</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../balancer-design/">Ceph 如何均衡（读写、容量）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../blkin/">Tracing Ceph With LTTng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../blkin/#tracing-ceph-with-blkin">Tracing Ceph With Blkin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../bluestore/">BlueStore Internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ceph_krb_auth/">如何配置好 Ceph Kerberos 认证的详细文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephfs-fscrypt/">CephFS Fscrypt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephfs-mirroring/">CephFS Mirroring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephfs-reclaim/">CephFS Reclaim Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephfs-snapshots/">CephFS 快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephx/">Cephx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cephx_protocol/">Cephx 认证协议详细阐述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../config/">配置管理系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../config-key/">config-key layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../context/">CephContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../continuous-integration/">Continuous Integration Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../corpus/">资料库结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cpu-profiler/">Oprofile 的安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cputrace/">CpuTrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../crush-msr/">CRUSH MSR (Multi-step Retry)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cxx/">C++17 and libstdc++ ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../deduplication/">去重</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../delayed-delete/">CephFS delayed deletion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dev_cluster_deployment/">开发集群的部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dev_cluster_deployment/#id5">在同一机器上部署多套开发集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../development-workflow/">开发流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../documenting/">为 Ceph 写作文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dpdk/">Ceph messenger DPDKStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../encoding/">序列化（编码、解码）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../erasure-coded-pool/">纠删码存储池</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../file-striping/">File striping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../freebsd/">FreeBSD Implementation details</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generatedocs/">Ceph 文档的构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../health-reports/">Health Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../iana/">IANA 号</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kclient/">Testing changes to the Linux Kernel CephFS driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kclient/#step-one-build-the-kernel">Step One: build the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kclient/#step-two-create-a-vm">Step Two: create a VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kclient/#step-three-networking-the-vm">Step Three: Networking the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kubernetes/">Hacking on Ceph in Kubernetes with Rook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libcephfs_proxy/">Design of the libcephfs proxy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libs/">库体系结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../logging/">集群日志的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../logs/">调试日志</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../macos/">在 MacOS 上构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mempool_accounting/">What is a mempool?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mempool_accounting/#some-common-mempools-that-we-can-track">Some common mempools that we can track</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../messenger/">Messenger notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mon-bootstrap/">Monitor bootstrap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mon-elections/">Monitor Elections</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mon-on-disk-formats/">ON-DISK FORMAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mon-osdmap-prune/">FULL OSDMAP VERSION PRUNING</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../msgr2/">msgr2 协议（ msgr2.0 和 msgr2.1 ）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../network-encoding/">Network Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../network-protocol/">网络协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../object-store/">对象存储架构概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../osd-class-path/">OSD class path issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../peering/">互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../perf/">Using perf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../perf_counters/">性能计数器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../perf_histograms/">Perf histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../placement-group/">PG （归置组）说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pool-migration-design/">Design of Pool Migration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../quick_guide/">开发者指南（快速）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rados-client-protocol/">RADOS 客户端协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rbd-diff/">RBD 增量备份</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rbd-export/">RBD Export &amp; Import</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rbd-layering/">RBD Layering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../release-checklists/">Release checklists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../release-process/">Ceph Release Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../seastore/">SeaStore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../sepia/">Sepia 社区测试实验室</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../session_authentication/">Session Authentication for the Cephx Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../testing/">测试笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../versions/">Public OSD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../vstart-ganesha/">NFS CephFS-RGW Developer Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../wireshark/">Wireshark Dissector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../zoned-storage/">Zoned Storage Support</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../">OSD 开发者文档</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../async_recovery/">异步恢复</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../backfill_reservation/">Backfill Reservation</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../">纠删码编码的归置组</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../#id2">术语</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../#id3">内容列表</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../last_epoch_started/">last_epoch_started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../log_based_pg/">Log Based PG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../manifest/">Manifest</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../map_message_handling/">Map and PG Message handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mclock_wpq_cmp_study/">QoS Study with mClock and WPQ Schedulers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../osd_overview/">OSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../partial_object_recovery/">Partial Object Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../past_intervals/">OSDMap Trimming and PastIntervals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pg/">PG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pg_removal/">PG Removal</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pgpool/">PGPool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../recovery_reservation/">Recovery Reservation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../refcount/">Refcount</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../scrub/">Scrub internals and diagnostics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../snaps/">快照</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../stale_read/">Preventing Stale Reads</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../watch_notify/">关注通知</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../wbthrottle/">回写抑制</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../mds_internals/">MDS 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../radosgw/">RADOS 网关开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ceph-volume/">ceph-volume 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../crimson/">Crimson developer documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../crimson/crimson/">Crimson (Tech Preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="erasure-coding-direct-reads">
<h1>Erasure Coding Direct Reads<a class="headerlink" href="#erasure-coding-direct-reads" title="Permalink to this heading"></a></h1>
<p>This document covers the design for EC Direct Reads, it also covers split
reads for large replica IOs.</p>
<section id="basic-design">
<h2>Basic design<a class="headerlink" href="#basic-design" title="Permalink to this heading"></a></h2>
<section id="small-reads">
<h3>Small reads<a class="headerlink" href="#small-reads" title="Permalink to this heading"></a></h3>
<p>Currently, all reads in an erasure-coded pool are directed to the primary
OSD of each PG. For a small (sub-chunk) read, this results in the following
message flow:</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-a05f4000212fd05b39a7747085a2de1a31072f36.png"/>
</p>
<p>The proposed direct read mechanism allows the client to bypass the primary and
interact directly with the target OSD, streamlining the message flow:</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-e584656f2e60e79dfa9ee5e7b20ce9ed6543c976.png"/>
</p>
<p>In the event of OSD unavailability or a read error, the client will fall back
to the legacy, primary-mediated read method to ensure data can be rebuilt from
the coding shards.</p>
</section>
<section id="medium-sized-reads">
<h3>Medium sized reads<a class="headerlink" href="#medium-sized-reads" title="Permalink to this heading"></a></h3>
<p>Reads are classified as ‘medium’ when they span multiple data shards but are
contained within a single stripe. The standard message flow for such a read is:</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-d7924e067fff1e55848a691feda26f0c9b8e6810.png"/>
</p>
<p>With the direct read mechanism, the message flow is optimized as follows:</p>
<p class="ditaa">
<img src="../../../../_images/ditaa-cdfe5f1728cc9fcaf00204f9aa2156bf0047bb31.png"/>
</p>
<p>Upon successful receipt of all data shards, the client is responsible for
reconstructing the original data by concatenating the buffers in the correct
sequence.</p>
</section>
<section id="large-size-reads">
<h3>Large size reads<a class="headerlink" href="#large-size-reads" title="Permalink to this heading"></a></h3>
<p>Large reads are those that exceed the size of a single data stripe. While each
OSD can still perform a single read operation, the client cannot merely
concatenate the data buffers. It must perform a de-striping operation to
reconstruct the object’s data from the various stripe units.</p>
</section>
</section>
<section id="objecter">
<h2>Objecter<a class="headerlink" href="#objecter" title="Permalink to this heading"></a></h2>
<p>The client’s Objecter component is responsible for orchestrating direct reads
through the following process:</p>
<p>Prior to initiating a direct read, the Objecter performs a preliminary,
best-effort check based on its current OSD map to verify that all required
OSDs are available. The possibility of race conditions arising from a stale OSD
map is considered a rare event with minimal performance impact. If any
required OSD is marked down, the read immediately falls back to the primary.</p>
<p>If all necessary OSDs are available, a new <code class="docutils literal notranslate"><span class="pre">SplitRead</span></code> object is instantiated
to manage the direct read operation.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">SplitRead</span></code> object calculates the target OSDs and submits read requests to
each one in parallel.</p>
<p>Upon completion of all parallel reads, the <code class="docutils literal notranslate"><span class="pre">SplitRead</span></code> object inspects the
results for failures.</p>
<ol class="loweralpha simple">
<li><p>If all reads were successful, it de-stripes (for large reads) or
concatenates (for medium reads) the data and completes the client’s
original read request.</p></li>
<li><p>If any of the reads failed, it will discard any partial data and reissue
the entire read operation to the primary OSD. While client-side recovery
using parity shards is theoretically possible, it is avoided by design.
Implementing the complex EC plugin architecture within every client is
considered undesirable. Given that OSD failures are infrequent events to
which the cluster adapts rapidly, the cost-benefit analysis strongly
favors this simpler, more robust fallback mechanism.</p></li>
</ol>
</section>
<section id="ec-backend">
<h2>EC Backend<a class="headerlink" href="#ec-backend" title="Permalink to this heading"></a></h2>
<p>The Erasure Coding backend will be enhanced to implement synchronous reads. This
functionality will be utilised exclusively by the direct-read mechanism and
will be identified by the <code class="docutils literal notranslate"><span class="pre">BALANCED_READ</span></code> flag on an EC read operation. There
will be a single attempt at the read; should this attempt fail, the I/O
operation will be failed back to the client without further retries at this
layer.</p>
</section>
<section id="missing-objects">
<h2>Missing objects<a class="headerlink" href="#missing-objects" title="Permalink to this heading"></a></h2>
<p>In scenarios where an object is temporarily unavailable due to recovery
operations, the OSD will fail the direct read request. A new error code (name
to be determined) will be introduced that instructs the client to not only
retry the I/O but also to disable direct reads for the affected PG until the
next epoch. This mechanism prevents the introduction of excessive latency
during cluster recovery activities.</p>
</section>
<section id="torn-writes">
<h2>Torn Writes<a class="headerlink" href="#torn-writes" title="Permalink to this heading"></a></h2>
<p>A torn write is a potential consistency issue that arises from race conditions
between read and write operations. This condition will be mitigated through a
version-checking mechanism. For any multi-OSD read, the “OI” (Object Info)
attribute on the primary is read <em>in parallel</em> with the data reads from the
other shards. A version mismatch is considered an infrequent event. Executing
this check concurrently with the data reads provides significant latency
improvements, especially for workloads that are bandwidth-limited due to large
chunk sizes. If a mismatch is detected between the version list from the
primary and the versions read from the other shards, the operation will fall
back to the legacy read mechanism.</p>
</section>
<section id="changes-to-default-chunk-sizes">
<h2>Changes to default chunk sizes<a class="headerlink" href="#changes-to-default-chunk-sizes" title="Permalink to this heading"></a></h2>
<p>To verify RADOS object versions, medium-sized reads frequently necessitate an
additional I/O operation to the primary OSD. To reduce the frequency of these
multi-shard reads, the default chunk size will be increased. While recent
enhancements to erasure coding have removed many of the performance
limitations of large chunks in Ceph, a key trade-off persists concerning
storage efficiency for small objects.</p>
<p>The primary limitation of an increased chunk size manifests when the average
object size is smaller than the chunk size. In this case, data chunks are left
partially empty, and storage overhead increases. For small files, a K+M pool
essentially degrades to a 1+M array, wasting space. Consequently, the default
chunk size must be increased judiciously. Our data shows that we cannot
practically increase chunk sizes beyond the current recommendation of 16k.
However, future designs may allow for packing of multiple small objects into a
single EC stripe. This will mitigate the effect of the small-object penalty,
and we should then increase the chunk size. We believe that 256k is optimal for
HDD and 32k for SSD, but careful performance testing will be required.</p>
</section>
<section id="read-write-ordering">
<h2>Read-write ordering<a class="headerlink" href="#read-write-ordering" title="Permalink to this heading"></a></h2>
<p>If the <code class="docutils literal notranslate"><span class="pre">RWORDERING</span></code> flag is not set on an I/O operation, a read may be
reordered with respect to any preceding, uncommitted write operations.</p>
<p>For example, consider the following sequence of operations on a single object:</p>
<ol class="arabic simple">
<li><p>Client submits <code class="docutils literal notranslate"><span class="pre">Write_1</span></code> containing data ‘AAA’ to the object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Write_1</span></code> completes. The object’s content is now ‘AAA’.</p></li>
<li><p>Client submits <code class="docutils literal notranslate"><span class="pre">Write_2</span></code> containing data ‘BBB’ to the same object.</p></li>
<li><p>Before <code class="docutils literal notranslate"><span class="pre">Write_2</span></code> completes, the client submits a <code class="docutils literal notranslate"><span class="pre">Read</span></code> operation.</p></li>
</ol>
<p>Under these conditions, the read operation is guaranteed to return the data from
either the completed <code class="docutils literal notranslate"><span class="pre">Write_1</span></code> (‘AAA’) or the subsequent <code class="docutils literal notranslate"><span class="pre">Write_2</span></code> (‘BBB’),
but never a torn result containing a mixture of both (e.g., ‘AAB’). This
provides a consistency guarantee analogous to that of balanced reads in
replicated pools.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">RWORDERING</span></code> flag is set, the operation must use the legacy,
primary-mediated read path to guarantee strict ordering. This ensures strict
ordering is maintained, particularly in failure scenarios. If a direct read were
attempted and then retried via the fallback path to the primary, the timing
change could violate the ordering guarantee. Forcing the use of the
primary-mediated path from the beginning prevents this ambiguity and is
consistent with the equivalent implementation for replicated pools.</p>
</section>
<section id="read-instability">
<h2>Read Instability<a class="headerlink" href="#read-instability" title="Permalink to this heading"></a></h2>
<p>Read instability is where read data changes after being first read, without a
subsequent write.</p>
<p>Consider the following sequence:</p>
<ol class="arabic simple">
<li><p>Write A (completes)</p></li>
<li><p>Write B (does not complete yet)</p></li>
<li><p>Read, get data B.</p></li>
<li><p>Client performs some activity, which assumes B is committed to disk.</p></li>
<li><p>Write is rolled back (and failed, or never completed).</p></li>
</ol>
<p>Even though the write B had not completed, it is often assumed by the client
that B will never be rolled back.</p>
<p>Balanced replica reads deal with this scenario by rejecting a non-primary read
if uncommitted writes exist in the log for this object. Such a mechanism is
necessary for EC direct reads too if none of the reads apply to the primary.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A similar mechanism for replica is implemented here:
<a class="reference external" href="https://github.com/ceph/ceph/pull/49380">https://github.com/ceph/ceph/pull/49380</a></p>
</div>
</section>
<section id="kernel-client-modifications">
<h2>Kernel Client Modifications<a class="headerlink" href="#kernel-client-modifications" title="Permalink to this heading"></a></h2>
<p>Kernel-mode clients that use Ceph (such as the <code class="docutils literal notranslate"><span class="pre">krbd</span></code> driver for block
devices) will also require modification. The logic described in this document,
which is implemented in the user-space <code class="docutils literal notranslate"><span class="pre">librados</span></code> library, must be replicated
within the kernel drivers.</p>
<p>This involves implementing the <code class="docutils literal notranslate"><span class="pre">SplitRead</span></code> mechanism, including the
calculation of target OSDs, the parallel submission of I/O requests, and the
subsequent reassembly (concatenation or de-striping) of data. This is a
non-trivial task, as kernel development brings additional complexity regarding
memory management, asynchronous I/O handling, and ensuring system stability.
The design and implementation of these kernel-level changes are recognised as
a necessary work item, but the specific approach will require a separate,
detailed investigation. Additionally, the challenge of submitting code to the
kernel itself may prove an interesting challenge!</p>
</section>
<section id="performance-qos-and-throttling">
<h2>Performance, QoS and throttling<a class="headerlink" href="#performance-qos-and-throttling" title="Permalink to this heading"></a></h2>
<p>The direct read mechanism fundamentally alters the I/O pattern from the
perspective of the OSDs. Where previously a client read resulted in a single
I/O operation to the primary OSD, this design will now submit multiple, smaller
I/O operations in parallel to different OSDs.</p>
<p>The balance of network operations will change and this will need to be carefully
assesed in performance evaluation, so that we can provide sufficient data to the
end user, to help make their decision about enabling this optional feature.</p>
<p>The splitting will significantly reduce the network traffic withing the cluster.
The network load on the public, client network will see a more complex change:</p>
<ul class="simple">
<li><p>Small IOs (sub-chunk) will see no increase in traffic at all.</p></li>
<li><p>Very large IOs will see an increase in the number of messages, but this is
likely insignificant compared to the associated data transfers</p></li>
<li><p>For the intermediate IOs, the performance considerations are more nuanced, the
splitting up of IOs will help to further distribute network traffic, but will
lead to a larger overhead, as the IO count will increase.</p></li>
</ul>
<section id="cpu-load">
<h3>CPU Load<a class="headerlink" href="#cpu-load" title="Permalink to this heading"></a></h3>
<p>As part of the implementation, the impact on CPU utilization on the client will
be measured. The “destripe” mechanism is CPU intensive and results in
non-contiguous buffers.  This may be significant for some applications where
the ceph client is completing for CPU resources with the client application.</p>
<p>It is important that the CPU “cost” of this feature is negligible when it is
switched off: the bypass code must be trivially simple.</p>
</section>
<section id="throttling-considerations">
<h3>Throttling Considerations<a class="headerlink" href="#throttling-considerations" title="Permalink to this heading"></a></h3>
<p>This change presents a challenge for the existing throttling framework. It is
critical that the throttling behavior for a single logical client read remains
consistent, regardless of how many physical OSD operations it is split into.
The precise method for ensuring that the throttling scheduler correctly
accounts for these fragmented operations as a single logical unit is an open
problem that requires further investigation. As such, the design for this
component is not yet finalised and will be addressed during the implementation
phase.</p>
</section>
</section>
<section id="replica-balanced-reads">
<h2>Replica Balanced Reads<a class="headerlink" href="#replica-balanced-reads" title="Permalink to this heading"></a></h2>
<p>For large, bandwidth constrained I/O patterns, direct EC reads are expected to
show significant improvements in latency compared to replica reads. Considering
this, the replica read mechanism will also be adapted to utilise the
<code class="docutils literal notranslate"><span class="pre">SplitRead</span></code> object for large, bandwidth constrained I/O. This will allow the
read workload to be split across multiple replica OSDs, preventing a single OSD
from becoming a bottleneck and improving overall performance for these
workloads. A separate PR will be used for replica reads, but for now we will
continue to use the same design document.</p>
<section id="splitting-reads">
<h3>Splitting Reads<a class="headerlink" href="#splitting-reads" title="Permalink to this heading"></a></h3>
<p>Early prototypes suggest that replica IOs should be split if:</p>
<ul class="simple">
<li><p>SSD: If replica IO &gt;= 2 x 128k, IO will be split into I/Os of at least 128k</p></li>
<li><p>HDD: If replica IO &gt;= 2 x 256k, IO will be split into I/Os of at least 256k</p></li>
</ul>
<p>Further performance measurements will be conducted to verify these changes and
a user-parameter will be provided to adjust these thresholds if required.</p>
</section>
</section>
<section id="plugin-support">
<h2>Plugin Support<a class="headerlink" href="#plugin-support" title="Permalink to this heading"></a></h2>
<p>Potentially this could be made to work with all plugins. However, to reduce
testing overhead, we will restrict to the following plugins:</p>
<ul class="simple">
<li><p>Jerasure</p></li>
<li><p>ISA-L</p></li>
</ul>
<p>We will set supporting LRC as a stretch goal, but this is dependent on the
enhanced EC work supporting LRC, which is not currently required for Umbrella.</p>
</section>
<section id="supported-ops">
<h2>Supported Ops<a class="headerlink" href="#supported-ops" title="Permalink to this heading"></a></h2>
<p>Further investigation will be performed during implementation, so the following
may change. The intent is to support ops such that the vast majority of IO in
realistic work loads of RBD, CephFS and RGW perform EC direct reads. At the
time of writing, the following seems like a reasonable limitation:</p>
<ul class="simple">
<li><p>Objecter ops must contain a single op</p></li>
<li><p>Read</p></li>
<li><p>Sparse read</p></li>
</ul>
<p>It is possible, but more complex to permit:</p>
<ul class="simple">
<li><p>Multiple reads/sparse reads</p></li>
<li><p>Attribute reads (when performed with a read or sparse reads)</p></li>
</ul>
<section id="sparse-reads">
<h3>Sparse Reads<a class="headerlink" href="#sparse-reads" title="Permalink to this heading"></a></h3>
<p>Sparse reads are currently treated as full reads by EC. Direct reads will
support sparse reads. A new synchronous code path through the OSD EC Backend is
required and implementing this such that it supports sparse reads is simple
enough to be worth implementing</p>
<p>No attempt will be made to support sparse reads on any other EC reads. This
means that in failure scenarios, the sparseness of the backend will disappear.
This would likely be a significant issue for encryption, so will need to be
addressed before encryption is supported. No client that supports EC can have a
dependency on sparse reads being fully implemented, so this should not cause
any regressions in the clients.</p>
</section>
</section>
<section id="object-user-version">
<h2>Object (user) Version<a class="headerlink" href="#object-user-version" title="Permalink to this heading"></a></h2>
<p>Clients can (and regularly do) request an “object version”. In the OSD this is
known as the “user version” and is store in the OI along with the OSD’s
interval version generated by primary log PG. They do this by passing a non-null
pointer to objver in the read request. The objver is not always up-to-date on
the non-primary OSDs. This means that any op requesting the objver must send a
read to the primary, even if such a read is otherwise empty. This will have a
significant impact on performance of small reads. Requesting objver was
previously negligible form a performance point of view, as such a review of
RBD, CephFS and RGW will be conducted to determine whether objver is sometimes
requested without being necessary.</p>
<p>For replica split reads, the user version is up to date with the data on all
shards, so the object version will be requested from a single arbitrary read.</p>
</section>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this heading"></a></h2>
<section id="ceph-test-rados-io-sequence">
<h3>ceph_test_rados_io_sequence<a class="headerlink" href="#ceph-test-rados-io-sequence" title="Permalink to this heading"></a></h3>
<section id="small-vs-medium-vs-large-reads">
<h4>Small vs Medium vs Large Reads<a class="headerlink" href="#small-vs-medium-vs-large-reads" title="Permalink to this heading"></a></h4>
<p>Already covered by the IO exerciser through existing design of
ceph_test_rados_io_sequence. Running through this should give us coverage of
various reads.</p>
</section>
<section id="id1">
<h4>Objecter<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h4>
<p>Already covered by IO exerciser, currently sends all IOs through librados which
calls Objecter. Running through this should give us coverage of various reads.</p>
</section>
<section id="stale-maps-fallback-path">
<h4>Stale Maps/Fallback path<a class="headerlink" href="#stale-maps-fallback-path" title="Permalink to this heading"></a></h4>
<p>IO Exerciser has two injects for reads currently:</p>
<ol class="arabic simple">
<li><p>Type = 0 injects EIO when reading data, shard indicates which shard to fail.
The inject is sent to the primary OSD</p></li>
<li><p>Type = 1 makes reads pretend a shard is missing which will immediately
trigger extra reads for a decode, shard indicates which shard to fail. The
inject is sent to the primary OSD</p></li>
</ol>
<p>As it stands, these will not work with Direct Reads and we will need to make
modifications to both the IO Exerciser so they send to the shard they want to
inject, along with modifications to the injects so they can be called on the
synchronous read path.</p>
</section>
<section id="id2">
<h4>Missing Objects<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h4>
<p>We could add a Type = 2 inject similar to Type = 0 where instead of returning
EIO it returns our new error code. We would need to either have a hook into
objecter to tell it to start sending direct reads again in the same epoch, or
force a new epoch following this. This would allow us to exercise the fallback
path of the client.</p>
<p>Possibly we might want to enhance the —testrecovery flag to be something like
—testrecovery=“read:2” or something to ensure we are able to use this new
inject specifically.</p>
</section>
<section id="id3">
<h4>Read/Write Ordering<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h4>
<p>A new ConcurrentWriteRead Op could be added which asserts the value that is
read afterwards is either entirely the original data or entirely the new write
and throw a miscompare either. It would be possible to keep stats and output
which of the two hits we get to ensure we get coverage of both results
occurring/assert if the ordering is wrong when we do not have the ordering flag
set.</p>
<p>Along with this OP a new sequence to test this with various small/medium/large
writes/reads would probably be needed to exercise this.</p>
</section>
<section id="kernel-level-testing">
<h4>Kernel level testing<a class="headerlink" href="#kernel-level-testing" title="Permalink to this heading"></a></h4>
<p>This area needs more consideration. Currently not covered by the
ceph_test_rados_io_sequence. Need to investigate if there is an equivalent to
librados for kernel space which the IO exerciser can use.</p>
<p>If not, then falling back to testing kRBD specifically may require a different
script which can exercise different object sizes and stale map responses.</p>
</section>
<section id="miscellaneous">
<h4>Miscellaneous<a class="headerlink" href="#miscellaneous" title="Permalink to this heading"></a></h4>
<p>It is possible to set the balanced reads flag directly on the IO itself to have
fine-grained control on a per-io basis of whether we are using balanced reads
or not. The IO exerciser can at the point of doing a read decide if it wants to
test doing a balanced read or not. The ReadOp can take an optional parameter
to override this, if we want to have an op in a sequence which will always or
never be balanced and an optional parameter can be added to the (Failed and
non-failed) ReadOp(s) in interactive mode for if you wish to specify the
value.</p>
</section>
</section>
<section id="ceph-test-rados">
<h3>ceph_test_rados<a class="headerlink" href="#ceph-test-rados" title="Permalink to this heading"></a></h3>
<p>ceph_test_rados is an existing tool that was very useful in the development of
optimised ec when looking for issues. There are no enhancements planned in this
area, but use of this tool will be heavily used during development to check for
regressions.</p>
</section>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../enhancements/" class="btn btn-neutral float-left" title="Erasure coding enhancements" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../last_epoch_started/" class="btn btn-neutral float-right" title="last_epoch_started" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>