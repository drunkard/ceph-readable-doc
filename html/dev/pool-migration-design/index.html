

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Design of Pool Migration &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="开发者指南（快速）" href="../quick_guide/" />
    <link rel="prev" title="PG （归置组）说明" href="../placement-group/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../internals/">Ceph 内幕</a></li>
      <li class="breadcrumb-item active">Design of Pool Migration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/dev/pool-migration-design.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../" class="icon icon-home"> Ceph
          

          
          </a>

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/">开发者指南</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../internals/">Ceph 内幕</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../balancer-design/">Ceph 如何均衡（读写、容量）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blkin/">Tracing Ceph With LTTng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blkin/#tracing-ceph-with-blkin">Tracing Ceph With Blkin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bluestore/">BlueStore Internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph_krb_auth/">如何配置好 Ceph Kerberos 认证的详细文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-fscrypt/">CephFS Fscrypt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-mirroring/">CephFS Mirroring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-reclaim/">CephFS Reclaim Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-snapshots/">CephFS 快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx/">Cephx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx_protocol/">Cephx 认证协议详细阐述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config/">配置管理系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config-key/">config-key layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../context/">CephContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../continuous-integration/">Continuous Integration Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../corpus/">资料库结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpu-profiler/">Oprofile 的安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cputrace/">CpuTrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../crush-msr/">CRUSH MSR (Multi-step Retry)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cxx/">C++17 and libstdc++ ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deduplication/">去重</a></li>
<li class="toctree-l2"><a class="reference internal" href="../delayed-delete/">CephFS delayed deletion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployment/">开发集群的部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployment/#id5">在同一机器上部署多套开发集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development-workflow/">开发流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../documenting/">为 Ceph 写作文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dpdk/">Ceph messenger DPDKStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../encoding/">序列化（编码、解码）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../erasure-coded-pool/">纠删码存储池</a></li>
<li class="toctree-l2"><a class="reference internal" href="../file-striping/">File striping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../freebsd/">FreeBSD Implementation details</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generatedocs/">Ceph 文档的构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../health-reports/">Health Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../iana/">IANA 号</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/">Testing changes to the Linux Kernel CephFS driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-one-build-the-kernel">Step One: build the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-two-create-a-vm">Step Two: create a VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-three-networking-the-vm">Step Three: Networking the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes/">Hacking on Ceph in Kubernetes with Rook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libcephfs_proxy/">Design of the libcephfs proxy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libs/">库体系结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logging/">集群日志的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logs/">调试日志</a></li>
<li class="toctree-l2"><a class="reference internal" href="../macos/">在 MacOS 上构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mempool_accounting/">What is a mempool?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mempool_accounting/#some-common-mempools-that-we-can-track">Some common mempools that we can track</a></li>
<li class="toctree-l2"><a class="reference internal" href="../messenger/">Messenger notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-bootstrap/">Monitor bootstrap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-elections/">Monitor Elections</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-on-disk-formats/">ON-DISK FORMAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-osdmap-prune/">FULL OSDMAP VERSION PRUNING</a></li>
<li class="toctree-l2"><a class="reference internal" href="../msgr2/">msgr2 协议（ msgr2.0 和 msgr2.1 ）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-encoding/">Network Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-protocol/">网络协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../object-store/">对象存储架构概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd-class-path/">OSD class path issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../peering/">互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf/">Using perf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_counters/">性能计数器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_histograms/">Perf histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../placement-group/">PG （归置组）说明</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Design of Pool Migration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#design">Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reuse-of-existing-design">Reuse of Existing Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="#read-write-ordering">Read/Write Ordering</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-pool-migration-issues">Other Pool Migration Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="#walkthrough-of-how-pool-migration-might-work">Walkthrough of how Pool Migration Might Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-and-test-tools">Testing and Test Tools</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../quick_guide/">开发者指南（快速）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rados-client-protocol/">RADOS 客户端协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-diff/">RBD 增量备份</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-export/">RBD Export &amp; Import</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-layering/">RBD Layering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-checklists/">Release checklists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-process/">Ceph Release Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../seastore/">SeaStore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sepia/">Sepia 社区测试实验室</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session_authentication/">Session Authentication for the Cephx Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../testing/">测试笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../versions/">Public OSD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vstart-ganesha/">NFS CephFS-RGW Developer Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../wireshark/">Wireshark Dissector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../zoned-storage/">Zoned Storage Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd_internals/">OSD 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mds_internals/">MDS 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../radosgw/">RADOS 网关开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph-volume/">ceph-volume 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../crimson/">Crimson developer documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../crimson/crimson/">Crimson (Tech Preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="design-of-pool-migration">
<h1>Design of Pool Migration<a class="headerlink" href="#design-of-pool-migration" title="Permalink to this heading"></a></h1>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading"></a></h2>
<p>The objective for pool migration is to be able to migrate all RADOS objects
from one pool to another within the same cluster non-disruptively. This
functionality is planned for the Umbrella release.</p>
<p>Use cases for pool migration:</p>
<ul class="simple">
<li><p>This provides the ability to change the erasure code profile (and in
particular the choice of K and M) non-disruptively. Implementing this as a
non-disruptive migration between pools is simpler and no less efficient than
trying to perform this type of transformation in place.</p></li>
<li><p>Converting between replica and erasure coded pools. Changes being made to add
OMAP and class support to EC pools will remove the need to have a separate
replica pool for metadata when using RBD, CephFS or RGW, this should make
these migrations viable in conjunction with this work.</p></li>
<li><p>The general use case of wanting to migrate data between two pools.</p></li>
</ul>
<p>By non-disruptive we mean that there will be no time where I/O or applications
need to switch from using the old pool to the new pool, not even a very short
outage at the start or end of the migration (as for example is required by RBD
live migration). The migration will however need to read and write every object
in the pool so there will be a performance impact during the migration -
similar to that when PGs are backfilling. The same techniques and controls that
are used when splitting/merging PGs and backfilling individual PGs will be used
by pool migration to manage this impact.</p>
<p>For the first release we will require that the target pool is empty to begin
with (this means we don’t need to worry about objects with the same name).
See the section on avoiding name collisions for more details. Supporting merging of
pools (either where constraints prevent object name collisions or where these
collisions are resolved automatically during the migration) is a possible
future enhancement, but it’s not clear what use cases this solves.</p>
<p>During a pool migration restrictions are placed on the source pool, it is not
permitted to modify the number of PGs (to cause splits or merges). See sections
on stopping changes to the number of PGs during migration and on the CLI and UI
for more details. Deletion of the source pool will not be permitted during a
migration. Other actions such as rebalancing are permitted but perhaps should
be discouraged as the data is being moved anyway.</p>
<p>During a pool migration restrictions are placed on the target pool, it is not
permitted to migrate this pool to another (i.e. no daisy chained or cyclical
migrations). Splits, merges and rebalancing are permitted. Deletion of the
target pool will not be permitted during a migration. Once a pool has finished
migrating it is permited to start a new migration of the target pool of a
previous migration.</p>
<p>For the first release there is no option to cancel, suspend or reverse a pool
migration once it has started.</p>
<p>For the first release there is no plan to have the clients update their
references to the pool once the migration has completed, they will continue
to reference the old pool and objector (<code class="docutils literal notranslate"><span class="pre">librados</span></code>) will reroute the request
to the new pool. The <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> will retain stub information for the old pool
redirecting to the new pool.</p>
<p>The feature requires changes to client code; all clients and daemons will
need to be upgraded before a pool migration is permitted. The two main clients,
objector (in <code class="docutils literal notranslate"><span class="pre">librados</span></code>) and the kernel client will be updated. Updates to
the kernel client are likely to lag the Umbrella release. Where the clients are
integrated into other products (e.g. <code class="docutils literal notranslate"><span class="pre">ODF</span></code>) these products will need to
incorporate the new clients before the feature can be used.</p>
<p>For the first release there is no plan to support pool migration between Ceph
clusters. Theoretically this could be added later building upon the first
release code but would require substantial extra effort. It would require
clients to be able to update references to the cluster and pool once the
migration had completed and for clients to be able to redirect I/O to a
different cluster. There would be extra authentication challenges as all OSDs
and clients in the source cluster would need to be able to submit requests to
the target cluster.</p>
</section>
<section id="design">
<h2>Design<a class="headerlink" href="#design" title="Permalink to this heading"></a></h2>
<section id="reuse-of-existing-design">
<h3>Reuse of Existing Design<a class="headerlink" href="#reuse-of-existing-design" title="Permalink to this heading"></a></h3>
<p>Let’s start by looking at existing code or features that we can copy / reuse
/ refactor / take inspiration from, we don’t want to reinvent the wheel or
repeat past mistakes.</p>
<section id="backfill">
<h4>Backfill<a class="headerlink" href="#backfill" title="Permalink to this heading"></a></h4>
<p>Backfill is a process run by a PG to recover objects on an OSD that has either
just been added to the PG (starts with no objects) or has been absent from the
PG for a while (has some objects that are up to date, some that are stale, is
probably missing new objects and may have objects that are no longer needed
because they were deleted while it was absent).</p>
<p>Backfill takes a long time so I/O must be permitted to continue while the
backfill happens. It uses the fact that all objects have a hash and that it is
possible to list objects in hash order. This means that backfill can recover
objects in hash order and can simply keep a watermark hash value to track what
progress has been made. I/Os to objects with a hash below the watermark are to
an object that has been recovered and need to update all OSDs including the
backfilling OSD. I/Os to objects with a hash above the watermark can ignore the
backfilling OSDs as the backfill process will recover this object later. The
object(s) currently being recovered by the backfill process are locked to
prevent I/O for the short time it takes to backfill an object.</p>
<p>Another property of backfill is that the process is idempotent, while there are
performance benefits to preserving the watermark there is no correctness issues
if the watermark is reset to the start and the backfill process starts again as
repeating the process will determine that objects have already been recovered.
This simplifies the design because the watermark doesn’t have to be rigorously
replicated and checkpointed, although for backfill it is part of <code class="docutils literal notranslate"><span class="pre">pg_info_t</span></code>
so progress is checkpointed fairly frequently.</p>
<p>Relevance to pool migration:</p>
<ul class="simple">
<li><p>Pool migration can list objects in hash order and migrate them to the new pool.</p></li>
<li><p>A watermark can be used to keep track of which objects have been migrated.
There is no need for the watermark to be persistent.</p></li>
<li><p>Clients can cache a copy of the watermark to help direct I/Os to the correct
pool and PG.</p></li>
<li><p>The client’s cached copy can become stale, if I/Os are misdirected they will
be failed providing an up-to-date watermark so the I/O can be retried.</p></li>
<li><p>Backfill recovers all parts of a RADOS object - attributes, data and
OMAP. Large objects are recovered in phases (something like 2MB at a time)
and utililize a temporary object which is renamed at the end of the recovery
for atomicity. If a peering cycle interrupts the process, then the
temporary object is discarded. If pool migration uses this technique it needs
to be aware that a peering cycle might disrupt the target pool but not the
source pool and therefore may need to restart the migration of the object if
the target discards the object.</p></li>
<li><p>Backfill recovers a RADOS head object and its associated snapshots at the
same time and uses locking (e.g.
<code class="docutils literal notranslate"><span class="pre">PrimaryLogPG::is_degraded_or_backfilling_object</span></code> and
<code class="docutils literal notranslate"><span class="pre">PrimaryLogPG::is_unreadable_object</span></code>) to ensure that none of these can be
accessed while they are being recovered because of dependencies between them.
Pool migration needs to migrate the head object and the snapshots at the same
time and needs to ensure we don’t process I/O to the object halfway through
this process.</p></li>
<li><p>Backfill is meant to preserve the space-efficiency of snapshots when
recovering them using the information in the snapset attribute to work out
which regions of the snapshots are clones - see <code class="docutils literal notranslate"><span class="pre">calc_clone_subsets</span></code> /
<code class="docutils literal notranslate"><span class="pre">calc_head_subsets</span></code>. This hasn’t been implemented for EC pools yet and
<a class="reference external" href="https://tracker.ceph.com/issues/72753">tracker 72753</a> shows it currently
isn’t working for replica pools either. We will want to use this (and the way
a <code class="docutils literal notranslate"><span class="pre">PushOp</span></code> re-establishes cloned regions) for migration.</p></li>
</ul>
<p>Unlike backfill, for migration we want the clients to know the watermark so
they can route I/Os to the old/new pool. We don’t care if clients have a
stale watermark - this will just cause a few I/Os to be incorrectly routed to
the old pool which can fail them back to the client and communicate a new
watermark so the I/O can be resubmitted to the new pool.</p>
<p>We deliberately make updating the client’s copy of the watermark lazy - there
could be hundreds or thousands of clients so updating them all the time would
be expensive. Putting the watermark into the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> and issuing new epochs
to distribute it to all the clients would be even more expensive. In contrast
we are thinking about recording which PGs are migrating/have finished migrating
in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> - a rule of thumb would be to try and only update the
<code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> once a second during a migration.</p>
<p>For migration to be able to support direct reads we do need all the OSDs in the
PG to know where the watermark is and for this to be updated as each object is
migrated. Migrating an object involves reading it from the source pool, writing
it to the target pool and then deleting it from the source pool. Other OSDs can
update migration progress as they process the delete request. There will be
some complexity regarding direct reads and migrating an object + its snapshots.
There is already some code that fails direct reads with <code class="docutils literal notranslate"><span class="pre">EAGAIN</span></code> (to redirect
these to the primary) when an object + its snapshots have not all been
recovered, we may need to use this when midway through migrating an object
+ snapshots and then have the primary stall the I/O until the object +
snapshots have all been migrated before failing the I/O again for redirection
to the new pool.</p>
<p>The watermark doesn’t necessarily need to be checkpointed to disk, it is cheap
to find the object with the lowest hash in a PG so we could do this to
recalculate the watermark whenever peering starts migration.</p>
</section>
<section id="scheduling-backfill-recovery">
<h4>Scheduling Backfill / Recovery<a class="headerlink" href="#scheduling-backfill-recovery" title="Permalink to this heading"></a></h4>
<p>Deciding how to prioritize backfill/recovery and how fast to run this process
versus processing I/O from clients is a complex problem. Firstly, a decision
is made as to which PGs should be backfilling/recovering, and which should
wait. This involves messages between OSDs and considers whether I/O is blocked
and how much redundancy the PG has left (for example a replica-3 pool with 2
failures is prioritized over a replica-3 pool with 1 failure). Secondly once a
PG has been selected to backfill/recover the schedule has to decide how
frequently to perform backfill/recovery versus process client I/O. This happens
within the primary OSD using weighted costs.</p>
<p>Relevance to pool migration:</p>
<ul class="simple">
<li><p>Pool migration is less critical than backfill or recovery. It needs to fit
into the same process to determine when a PG should start migrating.</p></li>
<li><p>Once a PG is permitted to start migration the OSD scheduler needs to pace the
work. The overheads for migrating an object are like the overheads for
backfilling an object so hopefully we can just copy the backfill scheduling
for migration.</p></li>
</ul>
<p>The objective is to reuse as much of the scheduler (e.g. <code class="docutils literal notranslate"><span class="pre">mclock</span></code>) as
possible, just teaching it that migration has a lower priority than backfill or
async recovery but higher priority than deep scrub.</p>
<p><code class="docutils literal notranslate"><span class="pre">Mclock</span></code> works by assigning a weighting to each backfill / recovery op and
each client I/O request, it also benchmarks OSDs at startup to get some idea
what the maximum performance of the OSD is. This information is then used to
work out when to schedule background work. The same concepts should work for
migration requests. We will need to assign a weighting to migration work;
this should be similar/identical to the weighting for backfills.</p>
<p>We will take a similar approach for supporting clusers running with
<code class="docutils literal notranslate"><span class="pre">WeightedPriorityQueue</span></code> scheduling.</p>
<p>The expectation is that there should be no need for new tuneable settings
for migration, the existing tuneable settings for backfill/recovery should be
sufficient, we don’t want to further complicate this part of the UI.</p>
</section>
<section id="statistics">
<h4>Statistics<a class="headerlink" href="#statistics" title="Permalink to this heading"></a></h4>
<p>I believe there are a few statistics collected about the performance of
backfill/recovery. We should supplement these with similar statistics
about the process of migrations.</p>
<p>We need to consider OSD stats that are gathered by <code class="docutils literal notranslate"><span class="pre">Prometheus</span></code> and any
progress summary that is presented via <code class="docutils literal notranslate"><span class="pre">HealthCheck</span></code> and/or the UI.</p>
</section>
<section id="copyfrom">
<h4>CopyFrom<a class="headerlink" href="#copyfrom" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">CopyFrom</span></code> is a RADOS op that can copy the contents of an object into a new
object. It is sent to the OSD and PG that will store the new object. The OSD
is responsible for reading the source object which involves sending messages
to another OSD and PG and then writing the data it reads to the new object. If
the object being copied is large, then the copy operation is broken up into
multiple stages and this is made atomic by using a temporary object to store
the new data until the last data has been copied at which point the temporary
object can be renamed to become the new object.</p>
<p>Relevance to pool migration:</p>
<ul class="simple">
<li><p>Pool migration needs to copy objects from the old pool to a new pool - this
will involve one OSD and PG reading the object and another OSD and PG writing
the object.</p></li>
<li><p>Pool migration will want to drive the copy operation from the source side,
so we probably need a <code class="docutils literal notranslate"><span class="pre">CopyTo</span></code> type operation.</p></li>
<li><p>The way messages are sent between OSDs, the way a large object copy is staged
and the use of a temporary object name when staging are all concepts that can be
reused.</p></li>
</ul>
<p>Alternatively, pool migration might want to copy the recover object
implementation in <code class="docutils literal notranslate"><span class="pre">ECBackend</span></code> which is used to recover an object being
recovered or backfilled. This also stages the recovery of large objects
using a temporary object and uses <code class="docutils literal notranslate"><span class="pre">PushOp</span></code> messages to send data to the OSDs
being backfilled. It might be possible to use most of the recover object
process without changes, just changing the <code class="docutils literal notranslate"><span class="pre">PushOp</span></code> messages to be sent to a
different PG and sending the messages for all shards as the entire object is
being migrated.</p>
<p>Lets consider the differences between the backend recovery op and CopyFrom:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CopyFrom</span></code> is a process that runs in <code class="docutils literal notranslate"><span class="pre">PrimaryLogPG</span></code> above either the
replica or <code class="docutils literal notranslate"><span class="pre">ECBackend</span></code> that copies an object from a primary OSD for one PG
to the primary OSD for another PG. In the case of EC the primary OSD may need
to issue <code class="docutils literal notranslate"><span class="pre">SubOp</span></code> commands to other OSDs to read/write the data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_recovery_op</span></code> implemented by replica and EC pools runs on the primary
OSD and reads data (in the case of EC issuing <code class="docutils literal notranslate"><span class="pre">SubOp</span></code> commands to other
OSDs) but then issues <code class="docutils literal notranslate"><span class="pre">PushOp</span></code> commands to write the recovered data to the
destination OSDs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CopyFrom</span></code> working at the <code class="docutils literal notranslate"><span class="pre">PrimaryLogPG</span></code> level ensures that the copied
object is included in the PG stats and gets its own PG log entry so the
update can be rolled forward/backwards and can be recovered by async
recovery.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_recovery_op</span></code> is implemented at the <code class="docutils literal notranslate"><span class="pre">PGBackend</span></code> level and assumes the
PG already has stats and a PG log entry for the object, it is just
responsible for bringing other shards in the PG up to date.</p></li>
<li><p>CopyFrom ends up issuing read and write ops to the PGBackend, it doesn’t
provide techniques for copying a snapshot and preserving its
space-efficiency.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_recovery_op</span></code> is meant to preserve space-efficiency of clones (not
implemented yet for EC pools and replica pools have bugs) – the <code class="docutils literal notranslate"><span class="pre">PushOp</span></code>
message includes a way of describing which parts of an object should be
clones.</p></li>
</ul>
<p>For pool migration we probably want a hybrid implementation. We can probably
re-use a lot of the <code class="docutils literal notranslate"><span class="pre">run_recovery_op</span></code> code to read the object that we want to
migrate, and ideally handle the space-efficiency in snaps. Instead of issuing
PushOps we probably want to issue a new <code class="docutils literal notranslate"><span class="pre">COPY_PUT</span></code> type op to the priamry PG
of the target pool, but passing the same kind of information as a PushOp so we
can keep track of what needs to be cloned. The target pool can then submit a
mixture of write and clone ops to the PGBackend layer to create the object as
well as updating the PG stats and creating a PG log entry.</p>
</section>
<section id="splitting-pgs">
<h4>Splitting PGs<a class="headerlink" href="#splitting-pgs" title="Permalink to this heading"></a></h4>
<p>Normally a pool has a number of PGs that is a power of 2. This is because we
want each PG to hold roughly the same number of objects, and we use the most
significant N bits of the object hash to select which PG to use. However, when
doubling the number of PGs that a pool has this causes approximately half the
objects in the pool to need to be moved to a new PG. We don’t want all this
migration to happen at once; we want it to be paced over time to have less
impact. To deal with this the MGR controls the increase in the number of PGs,
it has a target for how many PGs the pool should have and slowly increases the
number of PGs waiting for PGs to finish recovery before doing further splits.</p>
<p>When a pool has a non-power of 2 number of PGs this means that not all PGs are
the same size. For example, if there are 5 PGs then PGs 0 and 4 will be half
the size of PGs 1 to 3 because the choice between PG 0 and 4 is based on one
extra bit of the object hash. While this is not desirable as a long-term state
it is fine during the splitting process.</p>
<p>Relevance to pool migration:</p>
<ul class="simple">
<li><p>Pool migration needs to migrate all the objects in all the PGs in the old
pool to the new pool. Just like splitting we don’t want to overwhelm the
system while performing the migration.</p></li>
<li><p>Pool migration should therefore migrate one (or a small number) of PGs at
a time.</p></li>
<li><p>A process needs to monitor the progress of migrations, notice when PGs finish
migrating and start the next PG. This could either be in the MON (in which case
it would need to be event driven with OSDs telling the MON when a PG has
finished migrating - somewhat similar to how PG merges work) or it could be
implemented in the MGR (in which case the MGR can poll the state of the PGs and
then tell the MON via a CLI command to start the next PG migration).</p></li>
</ul>
</section>
<section id="direct-i-o-balanced-reads">
<h4>Direct I/O / Balanced Reads<a class="headerlink" href="#direct-i-o-balanced-reads" title="Permalink to this heading"></a></h4>
<p>The EC direct I/O feature is making changes to the client to decide which OSD
to send client I/O requests to, it is building on top of the balanced reads
flag for replica pools which tells the client to distribute read I/Os evenly
across all the OSDs in a replica PG rather than sending them all to the
primary.</p>
<p>Relevance to pool migration:</p>
<ul class="simple">
<li><p>It’s changing code in the client at a similar place to where we want the
client to implement pool migration deciding which pool (and hence PG and OSD)
to send I/O to.</p></li>
<li><p>Direct I/O / balanced reads are permitted to be failed by the OSD that
receives the request with <code class="docutils literal notranslate"><span class="pre">EAGAIN</span></code> to deal with corner cases where the OSD
is unable to process the I/O. In this case the client retries the I/O but
sends it to the primary OSD. A similar retry mechanism is going to be required
when a client issue an I/O to the wrong pool because an object has been
recently migrated. When I/Os are retried, we need to worry about ordering as
this generates opportunities for I/Os to overtake or be reordered. See section
Read/Write ordering below.</p></li>
<li><p>Direct I/O is adding extra information to the pg_pool_t structure that is
part of the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> that gets sent to every Ceph daemon and client by the
monitor. This extra information is being used to determine that direct I/O is
supported and to help work out where to route the I/O request. Pool migration
will similarly need to add details to <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structure so that clients
are aware that a migration is happening.</p></li>
</ul>
</section>
</section>
<section id="read-write-ordering">
<h3>Read/Write Ordering<a class="headerlink" href="#read-write-ordering" title="Permalink to this heading"></a></h3>
<p>Ceph has some fairly strict read / write ordering rules. Once a write has
completed to the client any read must return the new data. Prior to the write
completing a read is expected to return all old data or all new data (a mixture
is not permitted). If writes A and B are issued concurrently one after another
to the same object then write A is expected to be applied before write B –
ordering of the writes is expected to be preserved through the client,
messenger and the OSD. If write A and read B are issued concurrently then there
is scope for read B to overtake write A. There is a flag <code class="docutils literal notranslate"><span class="pre">RWORDERED</span></code> that can
be set that prevents this overtaking from happening.</p>
<p>There are no ordering guarantees when reads or writes are issued to different
objects - these objects are almost certainly stored on different OSDs and even
if they are on the same OSD will be processed by different threads with
different locks so can easily be reordered.</p>
<p>There do not appear to be many uses of the <code class="docutils literal notranslate"><span class="pre">RWORDERED</span></code> flag, RBD and RGW do
not use the flag, CephFS uses the flag in MDS <code class="docutils literal notranslate"><span class="pre">RecoveryQueue</span></code> (calls
<code class="docutils literal notranslate"><span class="pre">filer.probe</span></code> which is implemented in <code class="docutils literal notranslate"><span class="pre">osdc/Filer.cc</span></code>) which I think is
only used in some recovery scenarios.</p>
<p>These rules make it tricky to implement the watermark in the client and use
this to decide which pool to route I/O requests to without using something
equivalent to a new epoch to advance the watermark. The problem is that if the
watermark is advanced without quiescing I/O it is possible that this causes
requests to be reordered.</p>
<p>For example:</p>
<ul class="simple">
<li><p>Write A issued to old pool.</p></li>
<li><p>Write B issued to old pool.</p></li>
<li><p>Write A fails with updated watermark and is retried to new pool.</p></li>
<li><p>Read B with <code class="docutils literal notranslate"><span class="pre">RWORDERING</span></code> issued to new pool.</p></li>
<li><p>Write B fails and needs to be retried to new pool.</p></li>
</ul>
<p>In this example read B has overtaken write B.</p>
<p>Perhaps more concerning is that the rules would also be broken if instead of
Read B we issued another write to B.</p>
<p>The simplest way to prevent reordering violations is to not advance the
watermark while there are outstanding writes (or reads with <code class="docutils literal notranslate"><span class="pre">RWORDERING</span></code> flag
set) in flight. This isn’t idea as it may result it quite a number of I/Os
being failed for retry before the watermark can be updated.</p>
<p>A more sophisticated implementation stalls issuing new writes to objects
with a hash between the old and new watermark while there are other writes
in flight to objects with a hash between the old and new watermark.</p>
</section>
<section id="other-pool-migration-issues">
<h3>Other Pool Migration Issues<a class="headerlink" href="#other-pool-migration-issues" title="Permalink to this heading"></a></h3>
<p>Other topics that we need to think about for pool migration.</p>
<section id="avoiding-name-collisions">
<h4>Avoiding Name Collisions<a class="headerlink" href="#avoiding-name-collisions" title="Permalink to this heading"></a></h4>
<p>For the first release we will require that the target pool is empty when the
migration starts (by having a UI interface that only starts a migration while
a new pool is being created). We can also protect against objects being written
to the target pool during the migration by adding client code to reject
attempts to initiate requests to the target pool (the client code itself is
still permitted to redirect requests from the source pool to the target pool).
Because we will require a minimum client version to use pool migration this
will ensure that all clients include this extra policing. OSDs cannot
themselves implement the policing so there is no protection against a rouge
client – we probably should have migration halt rather that crash if a name
collision is found.</p>
<p>Post first release if there is a use case for merging pools then it is
theoretically possible to deal with name collisions by additionally using the
pool which the client is accessing the object from to uniquify the name. This
would require extra information in the request from the client to the OSDs.</p>
</section>
<section id="stopping-changes-to-the-number-of-pgs-during-migration">
<h4>Stopping Changes to the Number of PGs During Migration<a class="headerlink" href="#stopping-changes-to-the-number-of-pgs-during-migration" title="Permalink to this heading"></a></h4>
<p>During a migration we don’t really want to be changing the number of PGs in
the source pool. There are three reasons why:</p>
<ol class="arabic simple">
<li><p>We don’t really want to be moving objects around in the source pool when we
are about to migrate them - we are probably better off getting on with the
migration than trying to fix any imbalance in the source pool.</p></li>
<li><p>Splitting/merging PGs in the source pool makes it harder to schedule the
migration. Scheduling is done at two levels - we say how many source PGs are
migrating at a time and then control the rate of migration within a source PG.
If we split/merge the source pool this makes selecting which PGs to migrate
more difficult.</p></li>
<li><p>If we block splits and merges and migrate the PGs in reverse order (starting
with the highest numbered PG in the pool) then we can reduce the number of PGs
in the source pool as PGs finish migrating. This helps keeps the overall number
of PGs more manageable.</p></li>
</ol>
<p>In contrast we don’t really care so much about the target pool - we can easily
cope with splits/merges while the migration is in progress. From a performance
perspective we do however want to avoid migrating objects to the target pool
and then having splits/merges occur that copy the objects a second time. That
means that normally we would want to set the number of target pool PGs to be
the same as the source pool at the start of the migrate.</p>
<p>We might also want to default to disable the auto-scaler for the target pool
during the migration as we don’t want it seeing a nearly empty target pool
with loads of PGs and thinking that it should reduce the number of PGs.</p>
</section>
<section id="cli-and-ui">
<h4>CLI and UI<a class="headerlink" href="#cli-and-ui" title="Permalink to this heading"></a></h4>
<p>Pool migration will need a new CLI to start the migration, there will also need
to be a way of monitoring PGs that a migrating and the progress of the
migration. The CLI to start a migration will need to be implemented by the MON
(<code class="docutils literal notranslate"><span class="pre">OSDMonitor.cc</span></code> already implements most of the pool CLI commands) because
the migration will need to update the <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structures in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code>
to record details of the migration.</p>
<p>The new map will then be distributed to clients and OSDs so that they know that
the migration has started. PGs that have been scheduled to start migration will
need to determine at the end of the peering process that they don’t need to
recovery or backfill and that they should attempt to schedule a migration (will
need new PG states <code class="docutils literal notranslate"><span class="pre">MIGRATION_WAIT</span></code> and <code class="docutils literal notranslate"><span class="pre">MIGRATING</span></code>).</p>
<p>We will need to work with the dashboard team to add support for pool migration
to the dashboard and to provide a REST API for starting a migration.</p>
<p>We will want to block some CLIs while a pool migration is taking place:</p>
<ul class="simple">
<li><p>We don’t want to be able to split/merge PGs in the source pool while it is
being migrated (see above).</p></li>
<li><p>We don’t want the target pool to become the source of another migration
(no chaining migrations).</p></li>
</ul>
<p>Some of these CLIs are issued by MGR, in this case we probably will need to
change the MGR code to either cope with the failures and/or to detect that the
pool is migrating and avoid issuing the CLIs. We probably will need both as
although checking if the pool is migrating before issuing a CLI is probably
more efficient, it is exposed to a race hazard where the migrate may start
between the check and CLI being issued.</p>
<p>We need to look at how the progress of things like backfill and recovery are
reported in the UI (possibly by <code class="docutils literal notranslate"><span class="pre">HealthCheck</span></code>?) and think about how to report
the progress of a pool migration. We need to think what are the right units for
reporting progress (e.g. number of objects out of total objects, number of PGs
out of total PGs or just a percentage).</p>
</section>
<section id="backwards-compatibility-software-upgrade">
<h4>Backwards Compatibility / Software Upgrade<a class="headerlink" href="#backwards-compatibility-software-upgrade" title="Permalink to this heading"></a></h4>
<p>Pool migration requires code changes in the Ceph daemons (MON, OSD and possibly
MGR) and to the Ceph clients that issue I/O. We can’t allow a pool migration to
happen while any of these are running old code because the old code won’t
understand that a pool migration is happening. Old clients won’t have any way
of directing I/O to the correct pool, PG and OSD and having OSDs forward all
these requests to the correct OSD would be far too expensive.</p>
<p>Ceph daemons and clients have a set of feature bits indicating what features
they support and there are mechanisms for setting a minimum set of feature
bits that are required by daemons and separately for clients. Once set this
prevents down-level daemons and clients connecting to the cluster. There are
also mechanisms to ensure that once a minimum level has been set that this
cannot be reversed.</p>
<p>Pool migration will need to define a new feature bit and use the existing
mechanisms for setting minimum required levels for daemons and clients. The new
pool migration CLIs will need to fail an attempt to start a migration unless
the minimum levels have been set.</p>
</section>
<section id="end-of-migration">
<h4>End of Migration<a class="headerlink" href="#end-of-migration" title="Permalink to this heading"></a></h4>
<p>When a migration completes, we will have moved all objects from pool A to pool
B, however clients (e.g. RBD, CephFs, RGW, …) will still have pool A embedded
in their own data structures. We don’t want to force all the clients to update
their data structures to point at the new pool, so instead we will retain stub
information about pool A saying that it has been migrated and that all I/O
should now be submitted to pool B.</p>
<p>Retaining a stub <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structure in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> is cheap - there
won’t be thousands of pools and there isn’t that much data stored for the pool.
We will want to ensure that the old pool has no PGs associated with it, we can
do this by reducing the number of PGs it has to 0 and letting the same code
that runs when PGs are merged clean up and delete the old PGs.</p>
<p>We need to think about the consequences of this on the UI interface. While in
the code we start with pool A and create and migrate objects to pool B, from
the perspective of the UI we probably want to show this as a transformation of
pool A and hide the existence of pool B from the user.</p>
<p>An alternative implementation would just show the pool redirection in the UI,
so users would see an RBD image used pool A but would then find that pool A has
been migrated to pool B. This alternative implementation might be better if we
plan to support merging of pools (migration to a non-empty target pool) in the
future.</p>
</section>
</section>
<section id="walkthrough-of-how-pool-migration-might-work">
<h3>Walkthrough of how Pool Migration Might Work<a class="headerlink" href="#walkthrough-of-how-pool-migration-might-work" title="Permalink to this heading"></a></h3>
<section id="initiating-the-pool-migration">
<h4>Initiating the Pool Migration<a class="headerlink" href="#initiating-the-pool-migration" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>User creates a new pool, perhaps they use a new flag <code class="docutils literal notranslate"><span class="pre">--migratefrom</span></code> to
say they want to start a pool migration.</p></li>
<li><p>Starting the migration as part of pool creation means we know the pool is
initially empty.</p></li>
<li><p>Unless the user specifies a number of PGs we can ensure that the newly
created pool has the same number of PGs as the source pool. There is no
requirement that the number of PGs is the same, it just avoids having to
perform a migration and then perform a second copy of data as the number of
PGs is adjusted to cater for the eventual number of objects in the pool.</p></li>
<li><p>The CLI command sets up the <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structures in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> to
indicate that a pool migration is starting. We record that pool A is being
migrated to pool B, and record which PG(s) we are going to start migrating.
If we are going to migrate more than one PG at a time, we probably want to
specify a set of PGs (e.g. 0,1,2,3) that are being migrated. Any PG in the
set is migrating. Any PG not in the set that is higher than the lowest value
in the set is assumed to have completed migration, any PG not in the set
that is lower than the lowest value in the set is assumed to have not
started migration.</p></li>
<li><p>We migrate PGs in reverse order - so for example if a pool has PGs 0-15
then we will start by migrating PG 15.</p></li>
<li><p>MON publishes new <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> as a new epoch.</p></li>
</ol>
</section>
<section id="client">
<h4>Client<a class="headerlink" href="#client" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Clients use the <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structure in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> to work out a
migration is in progress.</p></li>
<li><p>From the range of PGs being migrated they can work out which PGs have been
migrated, which have not started migrating and which are in the process of
migrating.</p>
<ol class="loweralpha simple">
<li><p>If an I/O is submitted to a PG that has been migrated the object hash and
new pool is used to determine which PG and OSD to route the I/O request
to.</p></li>
<li><p>If an I/O is submitted to a PG that has not started migration the object
hash and old pool is used to determine which PG and OSD to route the I/O
request to.</p></li>
<li><p>If an I/O is submitted to a PG that is marked as being migrated the client
checks if it has a cached watermark for this PG. If it does, then it uses
this to decide whether to route the request to the old or new pool. If it
has no cached watermark, it guesses and sends the I/O to the old pool.</p></li>
</ol>
</li>
<li><p>If an I/O is misrouted to the wrong pool the OSD will fail the request
providing an update to the watermark. The client needs to update its cached
copy of the watermark and resubmit the I/O.</p></li>
</ol>
</section>
<section id="osd">
<h4>OSD<a class="headerlink" href="#osd" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>OSDs use the <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structure in the <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> to work out if a PG
needs migrating.</p></li>
<li><p>At the end of peering if the PG needs migrating and is not performing
backfill or recovery it sets the PG state to <code class="docutils literal notranslate"><span class="pre">MIGRATION_WAIT</span></code> and checks
with other OSDs whether they have the resources and free capacity to start
the migration.</p></li>
<li><p>If everything is good the PG state changes to <code class="docutils literal notranslate"><span class="pre">MIGRATING</span></code>, sets the
watermark to 0 and the scheduler is instructed to start scheduling migration
work.</p></li>
<li><p>Migration starts by scanning the next range of objects to be migrated
creating a list of object OIDs.</p></li>
<li><p>Each object is then migrated, with the watermark being updated after the
object has been migrated.</p>
<ol class="loweralpha simple">
<li><p>The primary reads the object and sends it to the primary of the target
PG which then writes the object.</p></li>
<li><p>If the object is large this is done in stages with the target using a
temporary object name which is renamed when the last data is written.</p></li>
<li><p>Once an object has been migrated it is deleted from the source pool.</p></li>
</ol>
</li>
<li><p>Client I/O checks the object hash of the client I/O with the watermark. If
the I/O is below the watermark it is failed for retry to the new pool,
providing the current watermark for the client to cache.</p></li>
<li><p>If a PG completes a migration, then it sends a message to the MON telling
it that the migration has completed.</p></li>
</ol>
</section>
<section id="mon">
<h4>MON<a class="headerlink" href="#mon" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>When MON gets a message from an OSD saying that a migration has completed
it updates the set in the <code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> to record that the PG has finished
migration and that the next PG is starting migration. A new <code class="docutils literal notranslate"><span class="pre">OSDMap</span></code> is
published as a new epoch.</p></li>
<li><p>Because migrations are scheduled in reverse order and objects are deleted
as the migration happens, this means that as PG migrations complete that we
should have empty PGs that can be deleted by simply reducing the number of
PGs that the source pool has. PG migrations might not complete in the order
which they are started so we might have a few empty PGs hanging around that
cannot be deleted until another PG migration completes.</p></li>
<li><p>At the end of the migration there are no more PGs to start migrating, so
the set of migrating PGs diminishes. When the set becomes empty we should
have also reduced the number of PGs for the source pool to zero and at this
point the migration is complete. The MON can make final updates to the
<code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> state to indicate the migration has finished. The
<code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structure needs to be kept so that clients know to direct all
I/O requests to this pool to the new pool instead.</p></li>
<li><p>Pools can be migrated more than once, this can result in multiple stub
<code class="docutils literal notranslate"><span class="pre">pg_pool_t</span></code> structures being kept. We do not want to have to recurse
through these stubs when I/Os are submitted, so at the end of a migration
the MON should attempt to reduce these redirects to a single level.</p></li>
</ol>
</section>
</section>
<section id="testing-and-test-tools">
<h3>Testing and Test Tools<a class="headerlink" href="#testing-and-test-tools" title="Permalink to this heading"></a></h3>
<p>The objectives of testing pool migration are:</p>
<ol class="arabic simple">
<li><p>Validate that all the objects in the source pool are migrated to the target
pool and that their contents (data, attributes and OMAP) are retained.</p></li>
<li><p>Validate that during a migration object can be read (data, attributes, OMAP)
for objects that haven’t yet been migrated, objects that have been migrated
and objects in the middle of being migrated.</p></li>
<li><p>Validate that during a migration objects can be updated (create, delete,
write, update attributes, update OMAPs) for objects that haven’t yet been
migrated, objects that have been migrated and objects in the middle of being
migrated.</p></li>
<li><p>Validating pool migration under error scenarios, including resetting and
failing OSDs.</p></li>
<li><p>Validate that snapshots, clones are migrated and can be used during a pool
migration.</p></li>
<li><p>Validate the UI for pool migration, including restrictions placed on the UI
during the migration.</p></li>
<li><p>Validation of migrating multiple different pools in parallel. Validation of
migration a single pool multiple times in series.</p></li>
<li><p>Validation of pool migration with unreadable objects (excessive medium
errors plus possibly other failures that defeat the redundancy of the
replica/EC pool without taking it offline).</p></li>
<li><p>Validation of software upgrade / compatibility for both daemons (OSD, MON,
MGR) and clients.</p></li>
<li><p>Validation of performance impact during a migration.</p></li>
</ol>
<p>Pool migration makes changes to client code, so all modified clients will need
testing.</p>
<p>Existing tools such as <code class="docutils literal notranslate"><span class="pre">ceph_test_rados</span></code> are good for creating and exercising
a set of objects and performing some consistency checking of objects.</p>
<p>A simple script is probably better for creating a large number of objects and
then validating the contents of the objects. Writing a script is probably
better for being able to test attributes and OMAPs as well. If the script has
two phases (create objects and validate objects) then these phases can be run
at different times (before, during, after pool migration) to test different
aspects. The script could use a command line tool such as rados to create and
validate objects, using pseudo random numbers to generate data patterns,
attributes and OMAP data that could then be validated. The script would need to
run many rados commands in parallel to generate a decent I/O workload. There
may be scripts that already exist that can do this, it may be possible to adapt
ceph_test_rados to do this.</p>
<p>Tools such as <code class="docutils literal notranslate"><span class="pre">VDBench</span></code> can test data integrity of block volumes, either
creating a data set and then validating it, or can continuously create and
update data keeping a journal so it can be validated at any point. However
block volume tools can only test object data, not attributes or OMAPs.</p>
<p>A tool such as <code class="docutils literal notranslate"><span class="pre">FIO</span></code> is best suited for doing performance measurements.</p>
<p>The I/O sequence tool <code class="docutils literal notranslate"><span class="pre">ceph_test_rados_io_sequence</span></code> is probably not useful
for testing pool migration - it specializes it testing a very small number of
objects and focuses on boundary conditions within an object (e.g. EC chunk
size, strip size) and data integrity.</p>
<p>The objective should be to use teuthology to perform most of the testing for
pool migration (at a minimum 1 to 5 in the list above). It should be possible
to add pool migration as an option to existing tests in the RADOS suite,
extending the <code class="docutils literal notranslate"><span class="pre">thrashOSD</span></code> class to include the option of starting a
migration.</p>
</section>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../placement-group/" class="btn btn-neutral float-left" title="PG （归置组）说明" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../quick_guide/" class="btn btn-neutral float-right" title="开发者指南（快速）" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>