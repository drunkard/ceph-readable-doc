

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>通用选项 &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="网络配置参考" href="../network-config-ref/" />
    <link rel="prev" title="配置 Ceph" href="../ceph-conf/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../">Ceph 存储集群</a></li>
          <li class="breadcrumb-item"><a href="../">配置</a></li>
      <li class="breadcrumb-item active">通用选项</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/rados/configuration/common.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../" class="icon icon-home"> Ceph
          

          
          </a>

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph 存储集群</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../">配置</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../storage-devices/">存储设备</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ceph-conf/">配置 Ceph</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">通用选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-network-config">网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">监视器</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-osd-config">认证</a></li>
<li class="toctree-l3"><a class="reference internal" href="#osds">OSDs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">心跳</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-logging-and-debugging">日志记录、调试</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-conf">ceph.conf 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ceph-runtime-config">跑多个集群（已废弃）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../network-config-ref/">网络选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../msgr2/">信使协议 v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auth-config-ref/">认证选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-config-ref/">监视器选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-lookup-dns/">通过 DNS 查询监视器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mon-osd-interaction/">心跳选项（监视器与 OSD 的的交互）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../osd-config-ref/">OSD 选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mclock-config-ref/">DmClock 配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bluestore-config-ref/">BlueStore 配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="../filestore-config-ref/">FileStore 配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="../journal-ref/">日志选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pool-pg-config-ref/">存储池、归置组和 CRUSH 选项</a></li>
<li class="toctree-l3"><a class="reference internal" href="../general-config-ref/">常规选项</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/">运维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">    手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">故障排除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../crimson/crimson/">Crimson (Tech Preview)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="ceph-conf-common-settings">
<span id="id1"></span><h1>通用选项<a class="headerlink" href="#ceph-conf-common-settings" title="Permalink to this heading"></a></h1>
<p>The <a class="reference external" href="../../../start/hardware-recommendations">Hardware Recommendations</a> section provides some hardware guidelines for
configuring a Ceph Storage Cluster. It is possible for a single <a class="reference internal" href="../../../glossary/#term-Ceph-Node"><span class="xref std std-term">Ceph
Node</span></a> to run multiple daemons. For example, a single node with multiple drives
may run one <code class="docutils literal notranslate"><span class="pre">ceph-osd</span></code> for each drive. Ideally, you will  have a node for a
particular type of process. For example, some nodes may run <code class="docutils literal notranslate"><span class="pre">ceph-osd</span></code>
daemons, other nodes may run <code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code> daemons, and still  other nodes may
run <code class="docutils literal notranslate"><span class="pre">ceph-mon</span></code> daemons.</p>
<p>Each node has a name identified by the <code class="docutils literal notranslate"><span class="pre">host</span></code> setting. Monitors also specify
a network address and port (i.e., domain name or IP address) identified by the
<code class="docutils literal notranslate"><span class="pre">addr</span></code> setting.  A basic configuration file will typically specify only
minimal settings for each instance of monitor daemons. For example:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">mon_initial_members</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">ceph1</span>
<span class="na">mon_host</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">10.0.0.1</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The <code class="docutils literal notranslate"><span class="pre">host</span></code> setting is the short name of the node (i.e., not
an fqdn). It is <strong>NOT</strong> an IP address either.  Enter <code class="docutils literal notranslate"><span class="pre">hostname</span> <span class="pre">-s</span></code> on
the command line to retrieve the name of the node. Do not use <code class="docutils literal notranslate"><span class="pre">host</span></code>
settings for anything other than initial monitors unless you are deploying
Ceph manually. You <strong>MUST NOT</strong> specify <code class="docutils literal notranslate"><span class="pre">host</span></code> under individual daemons
when using deployment tools like <code class="docutils literal notranslate"><span class="pre">chef</span></code> or <code class="docutils literal notranslate"><span class="pre">cephadm</span></code>, as those tools
will enter the appropriate values for you in the cluster map.</p>
</div>
</section>
<section id="ceph-network-config">
<span id="id2"></span><h1>网络<a class="headerlink" href="#ceph-network-config" title="Permalink to this heading"></a></h1>
<p>See the <a class="reference external" href="../network-config-ref">Network Configuration Reference</a> for a detailed discussion about
configuring a network for use with Ceph.</p>
</section>
<section id="id3">
<h1>监视器<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h1>
<p>Ceph production clusters typically deploy with a minimum 3 <a class="reference internal" href="../../../glossary/#term-Ceph-Monitor"><span class="xref std std-term">Ceph Monitor</span></a>
daemons to ensure high availability should a monitor instance crash. At least
three (3) monitors ensures that the Paxos algorithm can determine which version
of the <a class="reference internal" href="../../../glossary/#term-Ceph-Cluster-Map"><span class="xref std std-term">Ceph Cluster Map</span></a> is the most recent from a majority of Ceph
Monitors in the quorum.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may deploy Ceph with a single monitor, but if the instance fails,
the lack of other monitors may interrupt data service availability.</p>
</div>
<p>Ceph Monitors normally listen on port <code class="docutils literal notranslate"><span class="pre">3300</span></code> for the new v2 protocol, and <code class="docutils literal notranslate"><span class="pre">6789</span></code> for the old v1 protocol.</p>
<p>By default, Ceph expects that you will store a monitor’s data under the
following path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>/var/lib/ceph/mon/$cluster-$id
</pre></div>
</div>
<p>You or a deployment tool (e.g., <code class="docutils literal notranslate"><span class="pre">cephadm</span></code>) must create the corresponding
directory. With metavariables fully  expressed and a cluster named “ceph”, the
foregoing directory would evaluate to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">ceph</span><span class="o">/</span><span class="n">mon</span><span class="o">/</span><span class="n">ceph</span><span class="o">-</span><span class="n">a</span>
</pre></div>
</div>
<p>For additional details, see the <a class="reference external" href="../mon-config-ref">Monitor Config Reference</a>.</p>
</section>
<section id="ceph-osd-config">
<span id="id4"></span><h1>认证<a class="headerlink" href="#ceph-osd-config" title="Permalink to this heading"></a></h1>
<div class="versionadded">
<p><span class="versionmodified added">New in version Bobtail: </span>0.56</p>
</div>
<p>For Bobtail (v 0.56) and beyond, you should expressly enable or disable
authentication in the <code class="docutils literal notranslate"><span class="pre">[global]</span></code> section of your Ceph configuration file.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">auth_cluster_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>
<span class="na">auth_service_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>
<span class="na">auth_client_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>
</pre></div>
</div>
<p>Additionally, you should enable message signing. See <a class="reference external" href="../auth-config-ref">Cephx Config Reference</a> for details.</p>
</section>
<section id="osds">
<span id="ceph-monitor-config"></span><h1>OSDs<a class="headerlink" href="#osds" title="Permalink to this heading"></a></h1>
<p>Ceph production clusters typically deploy <a class="reference internal" href="../../../glossary/#term-Ceph-OSD-Daemons"><span class="xref std std-term">Ceph OSD Daemons</span></a> where one node
has one OSD daemon running a filestore on one storage drive. A typical
deployment specifies a journal size. For example:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[osd]</span>
<span class="na">osd journal size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">10000</span>

<span class="k">[osd.0]</span>
<span class="na">host</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{hostname}</span><span class="w"> </span><span class="c1">#manual deployments only.</span>
</pre></div>
</div>
<p>By default, Ceph expects to store a Ceph OSD Daemon’s data at the
following path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>/var/lib/ceph/osd/$cluster-$id
</pre></div>
</div>
<p>You or a deployment tool (e.g., <code class="docutils literal notranslate"><span class="pre">cephadm</span></code>) must create the corresponding
directory. With metavariables fully expressed and a cluster named “ceph”, this
example would evaluate to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">ceph</span><span class="o">/</span><span class="n">osd</span><span class="o">/</span><span class="n">ceph</span><span class="o">-</span><span class="mi">0</span>
</pre></div>
</div>
<p>You may override this path using the <code class="docutils literal notranslate"><span class="pre">osd</span> <span class="pre">data</span></code> setting. We don’t recommend
changing the default location. Create the default directory on your OSD host.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1">ssh<span class="w"> </span><span class="o">{</span>osd-host<span class="o">}</span></span>
<span class="prompt1">sudo<span class="w"> </span>mkdir<span class="w"> </span>/var/lib/ceph/osd/ceph-<span class="o">{</span>osd-number<span class="o">}</span></span>
</pre></div></div><p>The <code class="docutils literal notranslate"><span class="pre">osd</span> <span class="pre">data</span></code> path ideally leads to a mount point with a hard disk that is
separate from the hard disk storing and running the operating system and
daemons. If the OSD is for a disk other than the OS disk, prepare it for
use with Ceph, and mount it to the directory you just created</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ssh<span class="w"> </span><span class="o">{</span>new-osd-host<span class="o">}</span></span>
<span class="prompt1">sudo<span class="w"> </span>mkfs<span class="w"> </span>-t<span class="w"> </span><span class="o">{</span>fstype<span class="o">}</span><span class="w"> </span>/dev/<span class="o">{</span>disk<span class="o">}</span></span>
<span class="prompt1">sudo<span class="w"> </span>mount<span class="w"> </span>-o<span class="w"> </span>user_xattr<span class="w"> </span>/dev/<span class="o">{</span>hdd<span class="o">}</span><span class="w"> </span>/var/lib/ceph/osd/ceph-<span class="o">{</span>osd-number<span class="o">}</span></span>
</pre></div></div><p>We recommend using the <code class="docutils literal notranslate"><span class="pre">xfs</span></code> file system when running
<strong class="command">mkfs</strong>.  (<code class="docutils literal notranslate"><span class="pre">btrfs</span></code> and <code class="docutils literal notranslate"><span class="pre">ext4</span></code> are not recommended and no
longer tested.)</p>
<p>See the <a class="reference external" href="../osd-config-ref">OSD Config Reference</a> for additional configuration details.</p>
</section>
<section id="id5">
<h1>心跳<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h1>
<p>During runtime operations, Ceph OSD Daemons check up on other Ceph OSD Daemons
and report their  findings to the Ceph Monitor. You do not have to provide any
settings. However, if you have network latency issues, you may wish to modify
the settings.</p>
<p>See <a class="reference external" href="../mon-osd-interaction">Configuring Monitor/OSD Interaction</a> for additional details.</p>
</section>
<section id="ceph-logging-and-debugging">
<span id="id6"></span><h1>日志记录、调试<a class="headerlink" href="#ceph-logging-and-debugging" title="Permalink to this heading"></a></h1>
<p>Sometimes you may encounter issues with Ceph that require
modifying logging output and using Ceph’s debugging. See <a class="reference external" href="../../troubleshooting/log-and-debug">Debugging and
Logging</a> for details on log rotation.</p>
</section>
<section id="ceph-conf">
<h1>ceph.conf 实例<a class="headerlink" href="#ceph-conf" title="Permalink to this heading"></a></h1>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">fsid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{cluster-id}</span>
<span class="na">mon_initial_members</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{hostname}[, {hostname}]</span>
<span class="na">mon_host</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{ip-address}[, {ip-address}]</span>

<span class="c1">#All clusters have a front-side public network.</span>
<span class="c1">#If you have two network interfaces, you can configure a private / cluster </span>
<span class="c1">#network for RADOS object replication, heartbeats, backfill,</span>
<span class="c1">#recovery, etc.</span>
<span class="na">public_network</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{network}[, {network}]</span>
<span class="c1">#cluster_network = {network}[, {network}] </span>

<span class="c1">#Clusters require authentication by default.</span>
<span class="na">auth_cluster_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>
<span class="na">auth_service_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>
<span class="na">auth_client_required</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">cephx</span>

<span class="c1">#Choose reasonable number of replicas and placement groups.</span>
<span class="na">osd_journal_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{n}</span>
<span class="na">osd_pool_default_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{n}</span><span class="w">  </span><span class="c1"># Write an object n times.</span>
<span class="na">osd_pool_default_min_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{n}</span><span class="w"> </span><span class="c1"># Allow writing n copies in a degraded state.</span>
<span class="na">osd_pool_default_pg_autoscale_mode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{mode}</span><span class="w"> </span><span class="c1"># on, off, or warn</span>
<span class="c1"># Only used if autoscaling is off or warn:</span>
<span class="na">osd_pool_default_pg_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{n}</span>

<span class="c1">#Choose a reasonable crush leaf type.</span>
<span class="c1">#0 for a 1-node cluster.</span>
<span class="c1">#1 for a multi node cluster in a single rack</span>
<span class="c1">#2 for a multi node, multi chassis cluster with multiple hosts in a chassis</span>
<span class="c1">#3 for a multi node cluster with hosts across racks, etc.</span>
<span class="na">osd_crush_chooseleaf_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">{n}</span>
</pre></div>
</div>
</section>
<section id="ceph-runtime-config">
<span id="id7"></span><h1>跑多个集群（已废弃）<a class="headerlink" href="#ceph-runtime-config" title="Permalink to this heading"></a></h1>
<p>Each Ceph cluster has an internal name that is used as part of configuration
and log file names as well as directory and mountpoint names.  This name
defaults to “ceph”.  Previous releases of Ceph allowed one to specify a custom
name instead, for example “ceph2”.  This was intended to faciliate running
multiple logical clusters on the same physical hardware, but in practice this
was rarely exploited and should no longer be attempted.  Prior documentation
could also be misinterpreted as requiring unique cluster names in order to
use <code class="docutils literal notranslate"><span class="pre">rbd-mirror</span></code>.</p>
<p>Custom cluster names are now considered deprecated and the ability to deploy
them has already been removed from some tools, though existing custom name
deployments continue to operate.  The ability to run and manage clusters with
custom names may be progressively removed by future Ceph releases, so it is
strongly recommended to deploy all new clusters with the default name “ceph”.</p>
<p>Some Ceph CLI commands accept an optional <code class="docutils literal notranslate"><span class="pre">--cluster</span></code> (cluster name) option. This
option is present purely for backward compatibility and need not be accomodated
by new tools and deployments.</p>
<p>If you do need to allow multiple clusters to exist on the same host, please use
<a class="reference internal" href="../../../cephadm/#cephadm"><span class="std std-ref">Cephadm</span></a>, which uses containers to fully isolate each cluster.</p>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../ceph-conf/" class="btn btn-neutral float-left" title="配置 Ceph" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../network-config-ref/" class="btn btn-neutral float-right" title="网络配置参考" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>