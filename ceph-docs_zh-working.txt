CEPH官方文档中译
原	文：http://ceph.com/docs/master/start/
翻译人：张绍文<gongfan193@gmail.com>
版	本：v1.3
许	可：同原文


Welcome to Ceph
Ceph uniquely delivers object, block, and file storage in one unified system. Ceph is highly reliable, easy to manage, and free. The power of Ceph can transform your company’s IT infrastructure and your ability to manage vast amounts of data. Ceph delivers extraordinary scalability–thousands of clients accessing petabytes to exabytes of data. Ceph leverages commodity hardware and intelligent daemons to accommodate large numbers of storage hosts, which communicate with each other to replicate data, and redistribute data dynamically. Ceph’s cluster of monitors oversees the hosts in the Ceph storage cluster to ensure that the storage hosts are running smoothly.
ceph独一无二地在一个统一的系统中同时提供了对象、块、和文件存储功能。它可靠性高、管理简单，并且是开源软件。ceph的强大可以改变您公司的IT基础架构和海量数据管理能力。ceph可提供极大的伸缩性——供成千用户访问PB乃至EB级的数据，它用普通硬件和智能守护进程来构成大量存储主机，靠相互通讯来复制数据、并动态地重分布数据。ceph监视器集群监视着存储集群内的所有主机，确保它们运行正常。


关于ceph
One Object Storage: The Ceph Object Store, called RADOS, is the object storage component for CephFS filesystems, Ceph RADOS Gateways, and Ceph Block Devices.
一个对象存储：ceph的对象存储名为RADOS，是CephFS文件系统、RADOS网关、和Ceph块设备的存储部件。
Many Storage Interfaces: You can use CephFS, Ceph RADOS Gateway, or Ceph Block Devices in your deployment. You may also use all three interfaces with the same Ceph Object Store cluster! There’s no reason to build three different storage clusters for three different types of storage interface!
多个存储接口：在你的部署中，可以使用CephFS、Ceph RADOS网关、或者ceph块设备，你也可以同时使用这三个接口，在同一个ceph对象存储集群中！无需为三种不同接口分别建设存储集群！
Use Commodity Hardware!: You can deploy Ceph with commodity hardware. You don’t need to purchase proprietary storage or networking hardware commonly used in systems.
使用普通硬件！：你可以用普通硬件部署ceph，而不需要购买昂贵的专用存储或网络硬件。
 1  开始
 1.1  5分钟快速入门
Thank you for trying Ceph! Petabyte-scale data clusters are quite an undertaking. Before delving deeper into Ceph, we recommend setting up a cluster on a single host to explore some of the functionality. The Ceph 5-Minute Quick Start deploys a Ceph object store cluster on one server machine and a Ceph client on a separate machine, each with a recent Debian/Ubuntu operating system. The intent of this Quick Start is to help you exercise Ceph object store functionality without the configuration and deployment overhead associated with a production-ready object store cluster. Once you complete this quick start, you may exercise Ceph commands on the command line. You may also proceed to the quick start guides for block devices, CephFS filesystems, and the RESTful gateway.
感谢您尝试ceph！PB级数据集群是很大的挑战，在深入ceph前，我们推荐您在单机上安装一个集群来发掘它的功能。5分钟快速入门在单机上部署了一套ceph对象存储集群服务器端，在另一台机器上部署了ceph客户端，二者都基于最新的Debian/Ubuntu操作系统。这篇快速入门意在帮您练习Ceph对象存储功能，而无需配置、部署生产级的对象存储集群。完成本篇快速入门后，您就可以在命令行练习ceph命令了，也可以继续尝试块设备、CephFS文件系统、和RESTful网关。


 1.1.1  安装Debian/Ubuntu
Install Debian/Ubuntu
Install a recent release of Debian or Ubuntu (e.g., 12.04 precise).
安装Debian或Ubuntu的最新版（如12.10）
 1.1.2  安装ceph软件包
Add Ceph Packages
To get the latest Ceph packages, add a release key to APT, add a source location to your /etc/apt/sources.list, update your system and install Ceph.
要安装最新的ceph包，先把发布公钥添加到APT中，再把源位置添加到/etc/apt/sources.list中，更新系统、安装ceph。以下是命令：
wget -q -O- https://raw.github.com/ceph/ceph/master/keys/release.asc | sudo apt-key add -
echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update && sudo apt-get install ceph

Check the Ceph version you are using and make a note of it so that you have the correct settings in your configuration file:
检查下你安装的ceph版本，记下来，以便稍后正确地配置：
ceph -v

If ceph -v reflects an earlier version from what you installed, your ceph-common library may be using the version distributed with the kernel. Once you’ve installed Ceph, you may also update and upgrade your packages to ensure you have the latest ceph-common library installed.
如果ceph -v显示您安装了较老的版本，但ceph-common库可能是随内核发布的。所以安装ceph后，最好更新并升级软件包，以确保你安装了最新的ceph-common库。
sudo apt-get update && sudo apt-get upgrade

If you want to use a version other than the current release, see Installing Debian/Ubuntu Packages for further details.
如果你不想用当前发布的版本，参见Installing Debian/Ubuntu Packages。
 1.1.3  写配置文件
Add a Configuration File
The example configuration file will configure Ceph to operate a monitor, two OSD daemons and one metadata server on your Ceph server machine. To add a configuration file to Ceph, we suggest copying the contents of the example file below to an editor. Then, follow the steps below to modify it.
样板配置文件将为ceph集群配置1个监视器、2个OSD守护进程、和1个元数据服务器。我们建议您把下面的配置样本拷贝到文本编辑器，然后按下列步骤修改它。

[global]

	# For version 0.55 and beyond, you must explicitly enable
	# or disable authentication with "auth" entries in [global].

	auth cluster required = cephx
	auth service required = cephx
	auth client required = cephx

[osd]
	osd journal size = 1000

	#The following assumes ext4 filesystem.
	filestore xattr use omap = true


	# For Bobtail (v 0.56) and subsequent versions, you may
	# add settings for mkcephfs so that it will create and mount
	# the file system on a particular OSD for you. Remove the comment `#`
	# character for the following settings and replace the values
	# in braces with appropriate values, or leave the following settings
	# commented out to accept the default values. You must specify the
	# --mkfs option with mkcephfs in order for the deployment script to
	# utilize the following settings, and you must define the 'devs'
	# option for each osd instance; see below.

	#osd mkfs type = {fs-type}
	#osd mkfs options {fs-type} = {mkfs options}   # default for xfs is "-f"
	#osd mount options {fs-type} = {mount options} # default mount option is "rw,noatime"

	# For example, for ext4, the mount option might look like this:

	#osd mkfs options ext4 = user_xattr,rw,noatime

	# Execute $ hostname to retrieve the name of your host,
	# and replace {hostname} with the name of your host.
	# For the monitor, replace {ip-address} with the IP
	# address of your host.

[mon.a]

	host = {hostname}
	mon addr = {ip-address}:6789

[osd.0]
	host = {hostname}

	# For Bobtail (v 0.56) and subsequent versions, you may
	# add settings for mkcephfs so that it will create and mount
	# the file system on a particular OSD for you. Remove the comment `#`
	# character for the following setting for each OSD and specify
	# a path to the device if you use mkcephfs with the --mkfs option.

	#devs = {path-to-device}

[osd.1]
	host = {hostname}
	#devs = {path-to-device}

[mds.a]
	host = {hostname}

1. Open a command line on your Ceph server machine and execute hostname -s to retrieve the name of your Ceph server machine.
在ceph服务器机器上打开一个终端，执行hostname -s获取其主机名。
2. Replace {hostname} in the sample configuration file with your host name.
用你的主机名替换配置样本中的{hostname}。
3. Execute ifconfig on the command line of your Ceph server machine to retrieve the IP address of your Ceph server machine.
执行ifconfig获取ceph服务器的IP地址。
4. Replace {ip-address} in the sample configuration file with the IP address of your Ceph server host.
用你找到的ceph服务器IP地址替换配置样本中的{ip-address}。
5. Save the contents to /etc/ceph/ceph.conf on Ceph server host.
把修改后的内容保存到ceph服务器的/etc/ceph/ceph.conf文件。
6. Copy the configuration file to /etc/ceph/ceph.conf on your client host.
把那份配置文件也拷贝到客户端的/etc/ceph/ceph.conf。
    sudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf

Tip: Ensure the ceph.conf file has appropriate permissions set (e.g. chmod 644) on your client machine.
提示：确保客户端上的ceph.conf权限位设置得当，如：chmod 644。

New in version 0.55.
Ceph v0.55 and above have authentication enabled by default. You should explicitly enable or disable authentication with version 0.55 and above. The example configuration provides auth entries for authentication. For details on Ceph authentication see Cephx Configuration Reference and Cephx Guide.
0.55新增。
ceph v0.55及以上版本默认启用了认证，使用这些版本时应该显式地启用或禁用认证。配置样本提供了auth条目用于配置认证，详情参见 Cephx Configuration Reference和Cephx Guide。
 1.1.4  部署配置
Deploy the Configuration
You must perform the following steps to deploy the configuration.
你必须执行下列步骤来实现之前的配置。
1. On your Ceph server host, create a directory for each daemon. For the example configuration, execute the following:
在ceph服务器主机上，给每个进程创建相应目录。比如对样本配置，执行下面的命令：
sudo mkdir /var/lib/ceph/osd/ceph-0
sudo mkdir /var/lib/ceph/osd/ceph-1
sudo mkdir /var/lib/ceph/mon/ceph-a
sudo mkdir /var/lib/ceph/mds/ceph-a
2. Execute the following on the Ceph server host:
在ceph服务器上执行下列命令：
cd /etc/ceph
sudo mkcephfs -a -c /etc/ceph/ceph.conf -k ceph.keyring

Among other things, mkcephfs will deploy Ceph and generate a client.admin user and key. For Bobtail and subsequent versions (v 0.56 and after), the mkcephfs script will create and mount the filesystem for you provided you specify osd mkfs osd mount and devs settings in your Ceph configuration file.
除此之外，mkcephfs会部署并生成一个client.admin用户及其密钥。对于Bobtail及之后版本（v0.56及以上），如果你在配置文件里配置了osd mkfs、osd mount和devs，mkcephfs脚本会帮你创建并挂载文件系统。
 1.1.5  启动ceph集群
Start Ceph
Once you have deployed the configuration, start Ceph from the command line of your server machine.
配置、部署完成后，从服务器端命令行启动ceph。
sudo service ceph -a start

Check the health of your Ceph cluster to ensure it is ready.
检查ceph集群健康状态，确保无误：
sudo ceph health

When your cluster echoes back HEALTH_OK, you may begin using Ceph.
如果显示 HEALTH_OK，你就可以试用集群了。
 1.1.6  把密钥环拷贝到客户端
Copy The Keyring to The Client
The next step you must perform is to copy /etc/ceph/ceph.keyring, which contains the client.admin key, from the server machine to the client machine. If you don’t perform this step, you will not be able to use the Ceph command line, as the example Ceph configuration requires authentication.
下一个必须做的是把/etc/ceph/ceph.keyring从服务器复制到客户端，它包含client.admin的密钥。如果不做这步，你就不能用ceph命令行，因为样本配置要求认证。
sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring

Tip: Ensure the ceph.keyring file has appropriate permissions set (e.g., chmod 644) on your client machine.
提示：确保客户端机器上的ceph.keyring文件设置了恰当的权限位。
 1.1.7  继续其他快速入门
Proceed to Other Quick Starts
Once you have Ceph running with both a client and a server, you may proceed to the other Quick Start guides.
ceph客户端和服务器都运行良好后，你可以继续其他快速入门文档。

1. For Ceph block devices, proceed to Block Device Quick Start.
想了解ceph块设备，继续……
2. For the CephFS filesystem, proceed to CephFS Quick Start.
想了解CephFS文件系统，继续……
3. For the RESTful Gateway, proceed to Gateway Quick Start.
想了解RESTful网关，继续……
 1.2  块设备快速入门
Block Device Quick Start
To use this guide, you must have executed the procedures in the 5-minute Quick Start guide first. Execute this quick start on the client machine.
要实践这篇指导，你必须先完成前面的5分钟快速入门。应该在客户端上执行这篇快速入门。

1. Create a block device image.
创建一个块设备映像。
rbd create foo --size 4096
2. Load the rbd client module.
载入rbd客户端模块：
sudo modprobe rbd
3. Map the image to a block device.
把映像映射到块设备。
sudo rbd map foo --pool rbd --name client.admin
4. Use the block device. In the following example, create a file system.
使用块设备。在后面的例子中，创建了一个文件系统。
sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
5. Mount the file system.
挂载文件系统。
sudo mkdir /mnt/myrbd
sudo mount /dev/rbd/rbd/foo /mnt/myrbd

Note: Mount the block device on the client machine, not the server machine. See FAQ for details.
注意：要从客户端挂载块设备，而不是从服务器。详情见FAQ。

See block devices for additional details.
相关的更多信息见块设备。
 1.3  CEPHFS快速入门
CephFS Quick Start
To use this guide, you must have executed the procedures in the 5-minute Quick Start guide first. Execute this quick start on the client machine.
这篇向导基于前面的5分钟快速入门，以下步骤要在其他客户端主机上进行。
 1.3.1  内核空间驱动
Kernel Driver
Mount Ceph FS as a kernel driver.
用内核驱动挂载ceph集群。
sudo mkdir /mnt/mycephfs
sudo mount -t ceph {ip-address-of-monitor}:6789:/ /mnt/mycephfs

Note: Mount the CephFS filesystem on the client machine, not the cluster machine. See FAQ for details.
注意：要在客户端主机上挂载CephFS文件系统，不能在集群主机上。详情参见FAQ。
 1.3.2  用户空间(fuse)
Mount Ceph FS as with FUSE. Replace {username} with your username.
和FUSE一样挂载ceph文件系统，用你自己的用户名替代{username}。
sudo mkdir /home/{username}/cephfs
sudo ceph-fuse -m {ip-address-of-monitor}:6789 /home/{username}/cephfs

 1.3.3  额外信息
Additional Information
See CephFS for additional information. CephFS is not quite as stable as the block device and the object storage gateway. Contact Inktank for details on running CephFS in a production environment.
更多信息参见 CephFS。 CephFS不如块设备和对象存储网关稳定，要在生产环境运行 CephFS请联系Inktank。
 1.4  对象存储快速入门
Object Storage Quick Start
To use this guide, you must have executed the procedures in the 5-minute Quick Start guide first.
这篇向导基于前面的5分钟快速入门。
 1.4.1  安装Apache和FastCGI
Install Apache and FastCGI
The Ceph object storage gateway runs on Apache and FastCGI. Install them on the server machine. Use the following procedure:
ceph对象存储网关运行在Apache和FastCGI之上。按下列步骤安装到服务器上：
1. Install Apache and FastCGI on the server machine.
在服务器上安装Apacha和FastCGI。
sudo apt-get update && sudo apt-get install apache2 libapache2-mod-fastcgi
2. Enable the URL rewrite modules for Apache and FastCGI.
为Apache和FastCGI启用URL重写模块。
sudo a2enmod rewrite
sudo a2enmod fastcgi
3. Add a line for the ServerName in the /etc/apache2/httpd.conf file. Provide the fully qualified domain name of the server machine.
在/etc/apache2/httpd.conf里添加一行ServerName，设置为服务器的全资域名FQDN。
ServerName {fqdn}
4. Restart Apache so that the foregoing changes take effect.
重启Apacha，以确保前面的更改生效。
sudo service apache2 restart
 1.4.2  安装RADOS网关
Install RADOS Gateway
Once you have installed and configured Apache and FastCGI, you may install Ceph’s RADOS Gateway.
安装、配置好Apache和FastCGI后，还要安装ceph的RADOS网关。
sudo apt-get install radosgw

For details on the preceding steps, see RADOS Gateway Manual Install.
前述步骤的详情请参见……
 1.4.3  修改ceph配置文件
Modify the Ceph Configuration File
On the server machine, perform the following steps:
在服务器上执行下面的步骤：
1. Open the Ceph configuration file.
打开ceph配置文件。
cd /etc/ceph
vim ceph.conf
2. Add the following settings to the Ceph configuration file:
把下面的配置添加到ceph配置文件里：
[client.radosgw.gateway]
host = {host-name}
keyring = /etc/ceph/keyring.radosgw.gateway
rgw socket path = /tmp/radosgw.sock
log file = /var/log/ceph/radosgw.log
3. Go to the client machine and copy the configuration file from the server machine to /etc/ceph/ceph.conf on your client machine.
把服务器上的配置文件复制到客户端的/etc/ceph/ceph.conf。
sudo scp {user}@{cluster-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf

Tip： Ensure the ceph.conf file has appropriate permissions set (e.g. chmod 644) on your client machine.
提示：确保客户端的ceph.conf设置了合适的权限位，如：chmod 644。
 1.4.4  创建数据目录
Create a Data Directory
Create a data directory on the cluster server for the instance of radosgw.
在服务器上为radosgw例程创建数据目录。
sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway
 1.4.5  创建网关配置文件
Create a Gateway Configuration File
The example configuration file will configure the gateway to operate with the Apache FastCGI module, a rewrite rule for OpenStack Swift, and paths for the log files. To add a configuration file for the Ceph Gateway, we suggest copying the contents of the example file below to an editor. Then, follow the steps below to modify it.
下例会配置网关与Apache FastCGI模块运作、一个OpenStack Swift重写规则、日志文件路径。要配置ceph网关，我们建议把下面的样本复制到编辑器，然后按后面的步骤修改。
FastCgiExternalServer /var/www/s3gw.fcgi -socket /tmp/radosgw.sock

<VirtualHost *:80>
        ServerName {fqdn}
        ServerAdmin {email.address}
        DocumentRoot /var/www
</VirtualHost>

RewriteEngine On
RewriteRule ^/([a-zA-Z0-9-_.]*)([/]?.*) /s3gw.fcgi?page=$1&params=$2&%{QUERY_STRING} [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

<VirtualHost *:80>

        <IfModule mod_fastcgi.c>
                <Directory /var/www>
                        Options +ExecCGI
                        AllowOverride All
                        SetHandler fastcgi-script
                        Order allow,deny
                        Allow from all
                        AuthBasicAuthoritative Off
                </Directory>
        </IfModule>

        AllowEncodedSlashes On
        ErrorLog /var/log/apache2/error.log
        CustomLog /var/log/apache2/access.log combined
        ServerSignature Off
</VirtualHost>

Replace the {fqdn} entry with the fully-qualified domain name of the server server.
用服务器的全资域名替换{fqdn}。
1. Replace the {email.address} entry with the email address for the server administrator.
用管理员邮件地址替换{email.address}。
2. Save the contents to the /etc/apache2/sites-available directory on the server machine.
把内容保存到服务器的/etc/apache2/sites-avaiable目录下。
3. Enable the site for rgw.conf.
启用rgw.conf配置的站点。
sudo a2ensite rgw.conf
4. Disable the default site.
禁用默认站点。
sudo a2dissite default
See Create rgw.conf for additional details.
详情参见Create rgw.conf。
 1.4.6  添加FastCGI脚本
Add a FastCGI Script
FastCGI requires a script for the S3-compatible interface. To create the script, execute the following procedures on the server machine.
S3兼容接口需要一个FastCGI脚本，执行下列步骤创建脚本：
1. Go to the /var/www directory.
进入/var/www目录。
cd /var/www
2. Open an editor with the file name s3gw.fcgi.
在编辑器里创建s3gw.fcgi空文件。
sudo vim s3gw.fcgi
3. Copy the following into the editor.
把下面的内容复制到编辑器。
#!/bin/sh
exec /usr/bin/radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway
4. Save the file.
保存文件。
5. Change the permissions on the file so that it is executable.
给文件增加可执行权限位。
sudo chmod +x s3gw.fcgi
 1.4.7  生成密钥环和密钥
Generate a Keyring and Key
Perform the following steps on the server machine.
在服务器上执行下列步骤。
1. Create a keyring for the RADOS Gateway.
给RADOS网关创建密钥环。
sudo ceph-authtool --create-keyring /etc/ceph/keyring.radosgw.gateway
sudo chmod +r /etc/ceph/keyring.radosgw.gateway
2. Create a key for the RADOS Gateway to authenticate with the cluster.
创建一个密钥用于RADOS到集群的认证。
sudo ceph-authtool /etc/ceph/keyring.radosgw.gateway -n client.radosgw.gateway --gen-key
sudo ceph-authtool -n client.radosgw.gateway --cap osd 'allow rwx' --cap mon 'allow r' /etc/ceph/keyring.radosgw.gateway
3. Add the key to the Ceph keyring.
把密钥添加到ceph密钥环。
sudo ceph -k /etc/ceph/ceph.keyring auth add client.radosgw.gateway -i /etc/ceph/keyring.radosgw.gateway
 1.4.8  创建用户
Create a User
To use the Gateway, you must create a Gateway user. First, create a gateway user for the S3-compatible interface; then, create a subuser for the Swift-compatible interface.
要使用网关，必须有网关用户。首先给S3兼容接口创建一个网关用户，然后给Swift兼容接口创建一个子用户。
S3网关用户
Gateway (S3) User
First, create a Gateway user for the S3-compatible interface.
首先，给S3兼容接口创建网关用户。
sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}"
For example:
例如：
radosgw-admin user create --uid=johndoe --display-name="John Doe" --email=john@example.com

{ "user_id": "johndoe",
  "rados_uid": 0,
  "display_name": "John Doe",
  "email": "john@example.com",
  "suspended": 0,
  "subusers": [],
  "keys": [
    { "user": "johndoe",
      "access_key": "QFAMEDSJP5DEKJO0DDXY",
      "secret_key": "iaSFLDVvDdQt6lkNzHyW4fPLZugBAI1g17LO0+87"}],
  "swift_keys": []
 }

Creating a user creates an access_key and secret_key entry for use with any S3 API-compatible client.
创建用户时也创建了给S3兼容API客户端用的对应访问密钥和私钥。
Important: Check the key output. Sometimes radosgw-admin generates a key with an escape () character, and some clients do not know how to handle escape characters. Remedies include removing the escape character (), encapsulating the string in quotes, or simply regenerating the key and ensuring that it does not have an escape character.
重要：验证下密钥输出。有时候radosgw-admin生成了包含escape字符的密钥，而有些客户端不知道如何处理它们。矫正包括删除escape字符、把字符串放入引号，或者干脆重新生成密钥，并再次确认。
子用户
Subuser
Next, create a subuser for the Swift-compatible interface.
接下来，创建给Swift兼容接口用的子用户。
sudo radosgw-admin subuser create --uid=johndoe --subuser=johndoe:swift --access=full
{ "user_id": "johndoe",
  "rados_uid": 0,
  "display_name": "John Doe",
  "email": "john@example.com",
  "suspended": 0,
  "subusers": [
    { "id": "johndoe:swift",
      "permissions": "full-control"}],
  "keys": [
    { "user": "johndoe",
      "access_key": "QFAMEDSJP5DEKJO0DDXY",
      "secret_key": "iaSFLDVvDdQt6lkNzHyW4fPLZugBAI1g17LO0+87"}],
  "swift_keys": []}

sudo radosgw-admin key create --subuser=johndoe:swift --key-type=swift
{ "user_id": "johndoe",
  "rados_uid": 0,
  "display_name": "John Doe",
  "email": "john@example.com",
  "suspended": 0,
  "subusers": [
     { "id": "johndoe:swift",
       "permissions": "full-control"}],
  "keys": [
    { "user": "johndoe",
      "access_key": "QFAMEDSJP5DEKJO0DDXY",
      "secret_key": "iaSFLDVvDdQt6lkNzHyW4fPLZugBAI1g17LO0+87"}],
  "swift_keys": [
    { "user": "johndoe:swift",
      "secret_key": "E9T2rUZNu2gxUjcwUBO8n\/Ev4KX6\/GprEuH4qhu1"}]}

This step enables you to use any Swift client to connect to and use RADOS Gateway via the Swift-compatible API.
这一步使得你能用任何Swift客户端通过Swift兼容API连接和使用RADOS网关。

RGW’s user:subuser tuple maps to the tenant:user tuple expected by Swift.
RGW的user:subuser元组和Swift期望的一样映射到了tenant:user元组。
Note: RGW’s Swift authentication service only supports built-in Swift authentication (-V 1.0) at this point. See RGW Configuration for Keystone integration details.
注意：当前RGW的Swift认证服务只支持内建的Swift认证（-V 1.0），要和Keystone集成请参见……
 1.4.9  启用SSL
Enable SSL
Some REST clients use HTTPS by default. So you should consider enabling SSL for Apache on the server machine.
有些REST客户端默认使用HTTPS，所以你应该考虑开启Apache上的SSL。
sudo a2enmod ssl

Once you enable SSL, you should generate an SSL certificate.
启用SSL后，应该生成一个SSL证书。
sudo mkdir /etc/apache2/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/apache2/ssl/apache.key -out  /etc/apache2/ssl/apache.crt

Then, restart Apache.
然后重启Apache。
service apache2 restart

 1.5  加入ceph社区
Get Involved in the Ceph Community!
These are exciting times in the Ceph community! Get involved!
在ceph社区里有激动人心的时刻，加入吧！
频道
简介
联系信息
Blog
Check the Ceph Blog periodically to keep track of Ceph progress and important announcements.
经常检查blog来跟踪ceph进展和重要通告。
http://ceph.com/community/blog/
IRC
As you delve into Ceph, you may have questions or feedback for the Ceph development team. Ceph developers are often available on the #ceph IRC channel particularly during daytime hours in the US Pacific Standard Time zone.
随着您对ceph的深入了解，您也许有问题或回馈给ceph开发团队。ceph开发者经常在IRC的#ceph频道，尤其是美国太平洋标准时区白天工作时间。
Domain: irc.oftc.net
Channel: #ceph
Email List
邮件列表
Keep in touch with developer activity by subscribing to the email list at ceph-devel@vger.kernel.org. You can opt out of the email list at any time byunsubscribing. A simple email is all it takes! If you would like to view the archives, go to Gmane.
你可以订阅位于 ceph-devel@vger.kernel.org的邮件列表来和开发者保持联系，也可以随时离开。您也可以到Gmane查看历史存档。
Subscribe
Unsubscribe
Gmane
Bug Tracker
问题跟踪
You can help keep Ceph production worthy by filing and tracking bugs, and providing feature requests using the Bug Tracker.
您可以使用问题跟踪系统来帮助我们提升ceph稳定性，或提出新功能申请。
http://tracker.newdream.net/projects/ceph
Source Code
源码
If you would like to participate in development, bug fixing, or if you just want the very latest code for Ceph, you can get it at http://github.com. See Ceph Source Code for details on cloning from github.
如果您想参与开发、问题修正，或者想要最新源码，您可以从 http://github.com获取，参见 Ceph Source Code如何从github克隆源码。
http://github.com:ceph/ceph
http://ceph.com/download
Support
支持
If you have a very specific problem, an immediate need, or if your deployment requires significant help, consider commercial support.
如果您有特殊问题、急切的需求、或者您的部署需要大量帮助，请考虑商业支持。
http://inktank.com

 1.6  手动安装ceph
Installing Ceph Manually
Ceph is intended for large-scale deployments, but you may install Ceph on a single host. This guide is intended for Debian/Ubuntu Linux distributions.
ceph为大规模部署设计，但是仍然能安装在单机上。这篇手册适用于Debian/Ubuntu Linux发行版。
1. Install Ceph packages
2. Create a ceph.conf file. See Ceph Configuration Files for details.
3. Deploy the Ceph configuration. See Deploy with mkcephfs for details.
4. Start a Ceph cluster. See Starting a Cluster for details.
5. Mount Ceph FS. See Ceph FS for details.
 2  安装
Installation
The Ceph Object Store is the foundation of all Ceph clusters, and it consists primarily of two types of daemons: Object Storage Daemons (OSDs) and monitors. The Ceph Object Store is based upon the concept of RADOS, which eliminates single points of failure and delivers infinite scalability. For details on the architecture of Ceph and RADOS, refer to Ceph Architecture. All Ceph deployments have OSDs and monitors, so you should prepare your Ceph cluster by focusing first on the object storage cluster.
ceph对象存储是所有ceph集群的基础，它主要包含两种类型的守护进程：对象存储守护进程（ Object Storage Daemons, OSDs）和监视器。ceph对象存储运行于RADOS概念之上，它避免了单点故障、实现了无限的扩展能力。要了解ceph体系结构和RADOS的详情，请参见ceph体系结构部分。所有ceph的部署都有OSD和监视器，所以你首先应该关注对象存储集群，再准备ceph集群。
Recommendations
To begin using Ceph in production, you should review our hardware recommendations and operating system recommendations. Many of the frequently-asked questions in our mailing list involve hardware-related questions and how to install Ceph on various distributions.
忠告
要在生产环境下使用ceph，你得重新审阅硬件推荐和操作系统推荐部分，这些问题经常在我们的邮件列表里提及，以及如何在各种发行版上安装ceph。
 2.1  硬件推荐
Hardware Recommendations
Ceph was designed to run on commodity hardware, which makes building and maintaining petabyte-scale data clusters economically feasible. When planning out your cluster hardware, you will need to balance a number of considerations, including failure domains and potential performance issues. Hardware planning should include distributing Ceph daemons and other processes that use Ceph across many hosts. Generally, we recommend running Ceph daemons of a specific type on a host configured for that type of daemon. We recommend using other hosts for processes that utilize your data cluster (e.g., OpenStack, CloudStack, etc).
ceph为普通硬件设计，这可使构建、维护PB级数据集群的费用相对低廉。规划集群硬件时，需要均衡几方面的因素，包括区域失效和潜在的性能问题。硬件规划要包含把使用ceph集群的ceph守护进程和其他进程恰当分布。通常，我们推荐在一台机器上只运行一种类型的守护进程。我们推荐把使用数据集群的进程（如OpenStack、CloudStack等）安装在别的机器上。

Inktank provides excellent premium support for hardware planning.
Inktank可提供优秀的硬件规划支持。

Tip: Check out the Ceph blog too. Articles like Ceph Write Throughput 1, Ceph Write Throughput 2, Argonaut v. Bobtail Performance Preview, Bobtail Performance - I/O Scheduler Comparison and others are an excellent source of information.
提示：关于Ceph的高品质blog文章也值得参考，如Ceph Write Throughput 1, Ceph Write Throughput 2, Argonaut v. Bobtail Performance Preview, Bobtail Performance - I/O Scheduler Comparison。
 2.1.1  CPU
Ceph metadata servers dynamically redistribute their load, which is CPU intensive. So your metadata servers should have significant processing power (e.g., quad core or better CPUs). Ceph OSDs run the RADOS service, calculate data placement with CRUSH, replicate data, and maintain their own copy of the cluster map. Therefore, OSDs should have a reasonable amount of processing power (e.g., dual core processors). Monitors simply maintain a master copy of the cluster map, so they are not CPU intensive. You must also consider whether the host machine will run CPU-intensive processes in addition to Ceph daemons. For example, if your hosts will run computing VMs (e.g., OpenStack Nova), you will need to ensure that these other processes leave sufficient processing power for Ceph daemons. We recommend running additional CPU-intensive processes on separate hosts.
ceph元数据服务器对CPU敏感，它会动态地重分布它们的负载，所以你的元数据服务器应该有足够的处理能力（如4核或更强悍的CPU）。ceph的OSD运行着RADOS服务、用CRUSH计算数据存放位置、复制数据、维护它自己的集群运行图副本，因此OSD需要一定的处理能力（如双核CPU）。监视器只简单地维护着集群运行图的副本，因此对CPU不敏感；但必须考虑机器以后是否还会运行ceph监视器以外的CPU密集型任务。例如，如果服务器以后要运行用于计算的虚拟机（如OpenStack Nova），你就要确保给ceph进程保留了足够的处理能力，所以我们推荐在其他机器上运行CPU密集型任务。
 2.1.2  内存
RAM
Metadata servers and monitors must be capable of serving their data quickly, so they should have plenty of RAM (e.g., 1GB of RAM per daemon instance). OSDs do not require as much RAM for regular operations (e.g., 200MB of RAM per daemon instance); however, during recovery they need significantly more RAM (e.g., 500MB-1GB). Generally, more RAM is better.
元数据服务器和监视器必须可以尽快地提供它们的数据，所以他们应该有足够的内存，至少每进程1GB。运行OSD的服务器不需要那么多的内存，每进程500MB差不多了。通常内存越多越好。
 2.1.3  数据存储
Data Storage
Plan your data storage configuration carefully. There are significant cost and performance tradeoffs to consider when planning for data storage. Simultaneous OS operations, and simultaneous request for read and write operations from multiple daemons against a single drive can slow performance considerably. There are also file system limitations to consider: btrfs is not quite stable enough for production, but it has the ability to journal and write data simultaneously, whereas XFS and ext4 do not.
要谨慎地规划数据存储配置，因为其间涉及明显的成本和性能折衷。来自操作系统的并行操作和到单个硬盘的多个守护进程并发读、写请求操作会极大地降低性能。文件系统局限性也要考虑：btrfs尚未稳定到可以用于生产环境的程度，但它可以同时记日志并写入数据，而xfs和ext4却不能。
Important: Since Ceph has to write all data to the journal before it can send an ACK (for XFS and EXT4 at least), having the journals and OSD performance in balance is really important!
重要：因为ceph发送ACK前必须把所有数据写入日志（至少对xfs和ext4来说是），因此均衡日志和OSD性能相当重要。
 2.1.3.1  硬盘
Hard Disk Drives
OSDs should have plenty of hard disk drive space for object data. We recommend a minimum hard disk drive size of 1 terabyte. Consider the cost-per-gigabyte advantage of larger disks. We recommend dividing the price of the hard disk drive by the number of gigabytes to arrive at a cost per gigabyte, because larger drives may have a significant impact on the cost-per-gigabyte. For example, a 1 terabyte hard disk priced at $75.00 has a cost of $0.07 per gigabyte (i.e., $75 / 1024 = 0.0732). By contrast, a 3 terabyte hard disk priced at $150.00 has a cost of $0.05 per gigabyte (i.e., $150 / 3072 = 0.0488). In the foregoing example, using the 1 terabyte disks would generally increase the cost per gigabyte by 40%–rendering your cluster substantially less cost efficient.
OSD应该有足够的空间用于存储对象数据。考虑到大硬盘的每GB成本，我们建议用至少1TB的硬盘。建议用GB数除以硬盘价格来计算每GB成本，因为较大的硬盘通常会对每GB成本有较大影响，例如，单价为$75的1TB硬盘其每GB价格为$0.07（$75/1024=0.0732），又如单价为$150的3TB硬盘其每GB价格为$0.05 （$150/3072=0.0488），这样使用1TB硬盘会增加40%的每GB价格，它将表现为较低的经济性。
Tip: Running multiple OSDs on a single disk–irrespective of partitions–is NOT a good idea.
提示：不顾分区而在单个硬盘上运行多个OSD，这样不明智！

Tip: Running an OSD and a monitor or a metadata server on a single disk–irrespective of partitions–is NOT a good idea either.
提示：不顾分区而在运行了OSD的硬盘上同时运行监视器或元数据服务器也不明智！

Storage drives are subject to limitations on seek time, access time, read and write times, as well as total throughput. These physical limitations affect overall system performance–especially during recovery. We recommend using a dedicated drive for the operating system and software, and one drive for each OSD daemon you run on the host. Most “slow OSD” issues arise due to running an operating system, multiple OSDs, and/or multiple journals on the same drive. Since the cost of troubleshooting performance issues on a small cluster likely exceeds the cost of the extra disk drives, you can accelerate your cluster design planning by avoiding the temptation to overtax the OSD storage drives.
存储驱动器受限于寻道时间、访问时间、读写时间、还有总吞吐量，这些物理局限性影响着整体系统性能，尤其在系统恢复期间。因此我们推荐独立的驱动器用于安装操作系统和软件，另外每个OSD守护进程占用一个驱动器。大多数“slow OSD”问题的起因都是在相同的硬盘上运行了操作系统、多个OSD、和/或多个日志文件。鉴于解决性能问题的成本差不多会超过另外增加磁盘驱动器，你应该在设计时就避免增加OSD存储驱动器的负担来提升性能。
You may run multiple OSDs per hard disk drive, but this will likely lead to resource contention and diminish the overall throughput. You may store a journal and object data on the same drive, but this may increase the time it takes to journal a write and ACK to the client. Ceph must write to the journal before it can ACK the write. The btrfs filesystem can write journal data and object data simultaneously, whereas XFS and ext4 cannot.
ceph允许你在每块硬盘驱动器上运行多个OSD，但这会导致资源竞争并降低总体吞吐量；ceph也允许把日志和对象数据存储在相同驱动器上，但这会增加记录写日志并回应客户端的延时，因为ceph必须先写入日志才会回应确认了写动作。btrfs文件系统能同时写入日志数据和对象数据，xfs和ext4却不能。
Ceph best practices dictate that you should run operating systems, OSD data and OSD journals on separate drives.
ceph最佳实践指示，你应该分别在单独的硬盘运行操作系统、OSD数据和OSD日志。
 2.1.3.2  固态硬盘
Solid State Drives
One opportunity for performance improvement is to use solid-state drives (SSDs) to reduce random access time and read latency while accelerating throughput. SSDs often cost more than 10x as much per gigabyte when compared to a hard disk drive, but SSDs often exhibit access times that are at least 100x faster than a hard disk drive.
一种提升性能的方法是使用固态硬盘（SSD）来降低随机访问时间和读延时，同时增加吞吐量。SSD和硬盘相比每GB成本通常要高10倍以上，但访问时间至少比硬盘快100倍。
SSDs do not have moving mechanical parts so they aren’t necessarily subject to the same types of limitations as hard disk drives. SSDs do have significant limitations though. When evaluating SSDs, it is important to consider the performance of sequential reads and writes. An SSD that has 400MB/s sequential write throughput may have much better performance than an SSD with 120MB/s of sequential write throughput when storing multiple journals for multiple OSDs.
SSD没有可移动机械部件，所以不存在和硬盘一样的局限性。但SSD也有局限性，评估SSD时，顺序读写性能很重要，在为多个OSD存储日志时，有着400MB/s顺序读写吞吐量的SSD其性能远高于120MB/s的。
Important: We recommend exploring the use of SSDs to improve performance. However, before making a significant investment in SSDs, we strongly recommend both reviewing the performance metrics of an SSD and testing the SSD in a test configuration to gauge performance.
重要：我们建议发掘SSD的用法来提升性能。然而在大量投入SSD前，我们强烈建议核实SSD的性能指标，并在测试环境下衡量性能。
Since SSDs have no moving mechanical parts, it makes sense to use them in the areas of Ceph that do not use a lot of storage space. Relatively inexpensive SSDs may appeal to your sense of economy. Use caution. Acceptable IOPS are not enough when selecting an SSD for use with Ceph. There are a few important performance considerations for journals and SSDs:
正因为SSD没有移动机械部件，所以它很适合ceph里不需要太多存储空间的地方。相对廉价的SSD很诱人，慎用！可接受的IOPS指标对选择用于ceph的SSD还不够，用于日志和SSD时还有几个重要考量：
Write-intensive semantics: Journaling involves write-intensive semantics, so you should ensure that the SSD you choose to deploy will perform equal to or better than a hard disk drive when writing data. Inexpensive SSDs may introduce write latency even as they accelerate access time, because sometimes high performance hard drives can write as fast or faster than some of the more economical SSDs available on the market!
写密集语义：记日志涉及写密集语义，所以你要确保选用的SSD写入性能和硬盘相当或好于硬盘。廉价SSD可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜SSD相媲美。
Sequential Writes: When you store multiple journals on an SSD you must consider the sequential write limitations of the SSD too, since they may be handling requests to write to multiple OSD journals simultaneously.
顺序写入：在一个SSD上为多个OSD存储多个日志时也必须考虑SSD的顺序写入极限，因为它们要同时处理多个OSD日志的写入请求。
Partition Alignment: A common problem with SSD performance is that people like to partition drives as a best practice, but they often overlook proper partition alignment with SSDs, which can cause SSDs to transfer data much more slowly. Ensure that SSD partitions are properly aligned.
分区对齐：采用了SSD的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致SSD的数据传输速率慢很多，所以请确保分区对齐了。
While SSDs are cost prohibitive for object storage, OSDs may see a significant performance improvement by storing an OSD’s journal on an SSD and the OSD’s object data on a separate hard disk drive. The osd journal configuration setting defaults to /var/lib/ceph/osd/$cluster-$id/journal. You can mount this path to an SSD or to an SSD partition so that it is not merely a file on the same disk as the object data.
SSD用于对象存储太昂贵了，但是把OSD的日志存到SSD、把对象数据存储到独立的硬盘可以明显提升性能。osd journal选项的默认值是/var/lib/ceph/osd/$cluster-$id/journal，你可以把它挂载到一个SSD或SSD分区，这样它就不再是和对象数据一样存储在同一个硬盘上的文件了。
One way Ceph accelerates CephFS filesystem performance is to segregate the storage of CephFS metadata from the storage of the CephFS file contents. Ceph provides a default metadata pool for CephFS metadata. You will never have to create a pool for CephFS metadata, but you can create a CRUSH map hierarchy for your CephFS metadata pool that points only to a host’s SSD storage media. See Mapping Pools to Different Types of OSDs for details.
提升CephFS文件系统性能的一种方法是从CephFS文件内容里分离出元数据。ceph提供了默认的metadata存储池来存储CephFS元数据，所以你不需要给 CephFS元数据创建存储池，但是可以给它创建一个仅指向某主机SSD的CRUSH运行图。详情见Mapping Pools to Different Types of OSDs。
 2.1.3.3  控制器
Controllers
Disk controllers also have a significant impact on write throughput. Carefully, consider your selection of disk controllers to ensure that they do not create a performance bottleneck.
硬盘控制器对写吞吐量也有显著影响，要谨慎地选择，以免产生性能瓶颈。
Tip: The Ceph blog is often an excellent source of information on Ceph performance issues. See Ceph Write Throughput 1 and Ceph Write Throughput 2 for additional details.
提示：ceph blog通常是优秀的ceph性能问题来源，见Ceph Write Throughput 1和Ceph Write Throughput 2。
 2.1.3.4  其他注意事项
Additional Considerations
You may run multiple OSDs per host, but you should ensure that the sum of the total throughput of your OSD hard disks doesn’t exceed the network bandwidth required to service a client’s need to read or write data. You should also consider what percentage of the overall data the cluster stores on each host. If the percentage on a particular host is large and the host fails, it can lead to problems such as exceeding the full ratio, which causes Ceph to halt operations as a safety precaution that prevents data loss.
你可以在同一主机上运行多个OSD，但要确保OSD硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过full ratio的问题，此问题会使ceph中止运作以防数据丢失。
When you run multiple OSDs per host, you also need to ensure that the kernel is up to date. See OS Recommendations for notes on glibc and syncfs(2) to ensure that your hardware performs as expected when running multiple OSDs per host.
如果每台主机运行多个OSD，也得保证内核是最新的。参阅OS Recommendations里关于glibc和syncfs(2)的部分，确保硬件性能可达期望值。
 2.1.4  网络
Networks
We recommend that each host have at least two 1Gbps network interface controllers (NICs). Since most commodity hard disk drives have a throughput of approximately 100MB/second, your NICs should be able to handle the traffic for the OSD disks on your host. We recommend a minimum of two NICs to account for a public (front-side) network and a cluster (back-side) network. A cluster network (preferably not connected to the internet) handles the additional load for data replication and helps stop denial of service attacks that prevent the cluster from achieving active + clean states for placement groups as OSDs replicate data across the cluster. Consider starting with a 10Gbps network in your racks. Replicating 1TB of data across a 1Gbps network takes 3 hours, and 3TBs (a typical drive configuration) takes 9 hours. By contrast, with a 10Gbps network, the replication times would be 20 minutes and 1 hour respectively. In a petabyte-scale cluster, failure of an OSD disk should be an expectation, not an exception. System administrators will appreciate PGs recovering from a degraded state to an active + clean state as rapidly as possible, with price / performance tradeoffs taken into consideration. Additionally, some deployment tools (e.g., Dell’s Crowbar) deploy with five different networks, but employ VLANs to make hardware and network cabling more manageable. VLANs using 802.1q protocol require VLAN-capable NICs and Switches. The added hardware expense may be offset by the operational cost savings for network setup and maintenance. When using VLANs to handle VM traffic between between the cluster and compute stacks (e.g., OpenStack, CloudStack, etc.), it is also worth considering using 10G Ethernet. Top-of-rack routers for each network also need to be able to communicate with spine routers that have even faster throughput–e.g., 40Gbps to 100Gbps.
建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概100MB/s的吞吐量，网卡应该能处理所有OSD硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在OSD数据复制时不能回到 active+clean状态。请考虑部署万兆网卡。通过1Gbps网络复制1TB数据耗时3小时，而3TB（典型配置）需要9小时，相比之下，如果使用10Gbps复制时间可分别缩减到20分钟和1小时。在一个PB级集群中，OSD磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让PG尽快从degraded（降级）状态恢复到active+clean状态。另外，一些部署工具（如Dell的Crowbar）部署了5个不同的网络，但使用了VLAN以提高网络和硬件可管理性。VLAN使用802.1q协议，还需要采用支持VLAN功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用VLAN来处理集群和计算栈（如OpenStack、CloudStack等等）之间的VM流量时，采用10G网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如40Gbps到100Gbps。

Your server hardware should have a Baseboard Management Controller (BMC). Administration and deployment tools may also use BMCs extensively, so consider the cost/benefit tradeoff of an out-of-band network for administration. Hypervisor SSH access, VM image uploads, OS image installs, management sockets, etc. can impose significant loads on a network. Running three networks may seem like overkill, but each traffic path represents a potential capacity, throughput and/or performance bottleneck that you should carefully consider before deploying a large scale data cluster.
服务器应配置底板管理控制器（Baseboard Management Controller, BMC），管理和部署工具也应该大规模使用BMC，所以请考虑带外网络管理的成本/效益平衡，此程序管理着SSH访问、VM映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营3个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈。
 2.1.5  故障域
A failure domain is any failure that prevents access to one or more OSDs. That could be a stopped daemon on a host; a hard disk failure, an OS crash, a malfunctioning NIC, a failed power supply, a network outage, a power outage, and so forth. When planning out your hardware needs, you must balance the temptation to reduce costs by placing too many responsibilities into too few failure domains, and the added costs of isolating every potential failure domain.
故障域指任何导致不能访问一个或多个OSD的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。
 2.1.6  最低硬件推荐
Minimum Hardware Recommendations
Ceph can run on inexpensive commodity hardware. Small production clusters and development clusters can run successfully with modest hardware.
ceph可以运行在廉价的普通硬件上，小型生产集群和开发集群可以在一般的硬件上。
Process
Criteria
Minimum Recommended
ceph-osd
Processor
1x 64-bit AMD-64/i386 dual-core

RAM
500 MB per daemon

Volume Storage
1x Disk per daemon

Network
2x 1GB Ethernet NICs
ceph-mon
Processor
1x 64-bit AMD-64/i386

RAM
1 GB per daemon

Disk Space
10 GB per daemon

Network
2x 1GB Ethernet NICs
ceph-mds
Processor
1x 64-bit AMD-64/i386 quad-core

RAM
1 GB minimum per daemon

Disk Space
1 MB per daemon

Network
2x 1GB Ethernet NICs

Tip: If you are running an OSD with a single disk, create a partition for your volume storage that is separate from the partition containing the OS. Generally, we recommend separate disks for the OS and the volume storage.
提示：如果在只有一块硬盘的机器上运行OSD，要把数据和操作系统分别放到不同分区；一般来说，我们推荐操作系统和数据分别使用不同的硬盘。
 2.1.7  生产集群实例
Production Cluster Example
Production clusters for petabyte scale data storage may also use commodity hardware, but should have considerably more memory, processing power and data storage to account for heavy traffic loads.
PB级生产集群也可以使用普通硬件，但应该配备更多内存、CPU和数据存储空间来解决流量压力。

A recent (2012) Ceph cluster project is using two fairly robust hardware configurations for Ceph OSDs, and a lighter configuration for monitors.
一个最新（2012）的ceph集群项目使用了2个相当强悍的OSD硬件配置，和稍逊的监视器配置。

Configuration
Criteria
Minimum Recommended
Dell PE R510
Processor
2x 64-bit quad-core Xeon CPUs

RAM
16 GB

Volume Storage
8x 2TB drives. 1 OS, 7 Storage

Client Network
2x 1GB Ethernet NICs

OSD Network
2x 1GB Ethernet NICs

Mgmt. Network
2x 1GB Ethernet NICs
Dell PE R515
Processor
1x hex-core Opteron CPU

RAM
16 GB

Volume Storage
12x 3TB drives. Storage

OS Storage
1x 500GB drive. Operating System.

Client Network
2x 1GB Ethernet NICs

OSD Network
2x 1GB Ethernet NICs

Mgmt. Network
2x 1GB Ethernet NICs
 2.2  推荐操作系统
OS Recommendations
 2.2.1  CEPH依赖
As a general rule, we recommend deploying Ceph on newer releases of Linux.
在较新的Linux发行版上部署ceph，这是我们推荐的通用法则。
Linux内核
Ceph Kernel Client: We currently recommend:
v3.6.6 or later in the v3.6 stable series
v3.4.20 or later in the v3.4 stable series
btrfs: If you use the btrfs file system with Ceph, we recommend using a recent Linux kernel (v3.5 or later).
btrfs文件系统：如果您想在btrfs上运行ceph，我们推荐使用一个最新的Linux内核（v3.5或更新）。
GLIBC
syncfs(2): For non-btrfs filesystems such as XFS and ext4 where more than one ceph-osd daemon is used on a single server, Ceph performs signficantly better with the syncfs(2) system call (added in kernel 2.6.39 and glibc 2.14). New versions of Ceph (v0.55 and later) do not depend on glibc support.
 syncfs(2):对非btrfs文件系统（像XFS和ext4）而言，在一台服务器上运行了多个 ceph-osd守护进程时，ceph使用syncfs(2)系统调用时效率高得多（此功能在2.6.39内核和glibc-2.14加入）。
 2.2.2  系统平台
Platforms
The charts below show how Ceph’s requirements map onto various Linux platforms. Generally speaking, there is very little dependence on specific distributions aside from the kernel and system initialization package (i.e., sysvinit, upstart, systemd).
下面的表格展示了ceph需求和各种Linux发行版的对应关系。一般来说，ceph对内核和系统初始化阶段的依赖很少（如sysvinit, upstart, systemd）。

ARGONAUT (0.48)
Distro
Release
Code Name
Kernel
Notes
Testing
Ubuntu
11.04
Natty Narwhal
linux-2.6.38
1, 2, 3
B
Ubuntu
11.10
Oneric Ocelot
linux-3.0.0
1, 2, 3
B
Ubuntu
12.04
Precise Pangolin
linux-3.2.0
1, 2
B, I, C
Ubuntu
12.10
Quantal Quetzal
linux-3.5.4
2
B
Debian
6.0
Squeeze
linux-2.6.32
1, 2
B
Debian
7.0
Wheezy
linux-3.2.0
1, 2
B

BOBTAIL (0.56)
Distro
Release
Code Name
Kernel
Notes
Testing
Ubuntu
11.04
Natty Narwhal
linux-2.6.38
1, 2, 3
B
Ubuntu
11.10
Oneric Ocelot
linux-3.0.0
1, 2, 3
B
Ubuntu
12.04
Precise Pangolin
linux-3.2.0
1, 2
B, I, C
Ubuntu
12.10
Quantal Quetzal
linux-3.5.4
2
B
Debian
6.0
Squeeze
linux-2.6.32
1, 2
B
Debian
7.0
Wheezy
linux-3.2.0
1, 2
B
CentOS
6.3
N/A
linux-2.6.32
1, 2, 3
B, I
Fedora
17.0
Beefy Miracle
linux-3.3.4
1, 2
B
Fedora
18.0
Spherical Cow
linux-3.6.0

B
OpenSuse
12.2
N/A
linux-3.4.0
2
B

NOTES
附注
1: The default kernel has an older version of btrfs that we do not recommend for ceph-osd storage nodes. Upgrade to a recommended kernel or use XFS or ext4.
2: The default kernel has an old Ceph client that we do not recommend for kernel client (kernel RBD or the Ceph file system). Upgrade to a recommended kernel.
3: The installed version of glibc does not support the syncfs(2) system call. Putting multiple ceph-osd daemons using XFS or ext4 on the same host will not perform as well as they could.
1、默认内核btrfs版本较老，不推荐用于ceph-osd存储节点；要升级到推荐的内核，或者改用xfs、ext4。
2、默认内核带的ceph客户端较老，不推荐做内核空间客户端（内核RBD或ceph文件系统），请升级到推荐内核。
3、已安装的glibc版本不支持syncfs(2)系统调用，同一台机器上使用xfs或ext4的ceph-osd守护进程性能一般，它可以更好。

TESTING
测试
B: We continuously build all branches on this platform and exercise basic unit tests. We build release packages for this platform.
I: We do basic installation and functionality tests of releases on this platform.
C: We run a comprehensive functional, regression, and stress test suite on this platform on a continuous basis. This includes development branches, pre-release, and released code.
B：我们持续地在这个平台上编译所有分支、做基本单元测试；也为这个平台构建可发布软件包。
I：我们在这个平台上做基本的安装和功能测试。
C：我们在这个平台上持续地做全面的功能、退化、压力测试，包括开发分支、预发布版本、正式发布版本。
 2.3  Debian/Ubuntu包的安装
Installing Debian/Ubuntu Packages
You may install stable release packages (for stable deployments), development release packages (for the latest features), or development testing packages (for development and QA only). Do not add multiple package sources at the same time.
你可以选择安装稳定发行包（用于稳定部署）、开发发行包（用于发掘最新功能）、或开发测试包（仅用于开发和质检）。不要同时添加多个软件源。
 2.3.1  安装发布密钥
Install Release Key
Packages are cryptographically signed with the release.asc key. Add our release key to your system’s list of trusted keys to avoid a security warning:
软件包都用release.asc密钥加密签名过，把我们的发布密钥加到您系统的可信密钥列表中可避免安全警告：
wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
 2.3.2  添加软件源
Add Release Packages
 2.3.2.1  稳定版——Bobtail
Bobtail is the most recent major release of Ceph. These packages are recommended for anyone deploying Ceph in a production environment. Critical bug fixes are backported and point releases are made as necessary.
bobtail是ceph最新的主要发布，这些软件包适合任何想在生产环境下部署ceph的人。重大缺陷的修复都移植过来了，次版本会适时推出。
Add our package repository to your system’s list of APT sources. See the bobtail Debian repository for a complete list of distributions supported.
把我们的软件库加到APT源列表，所支持发行版的完整列表见 the bobtail Debian repository。
echo deb http://ceph.com/debian-bobtail/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
For the European users there is also a mirror in the Netherlands at http://eu.ceph.com/
欧洲用户可以访问位于荷兰的镜像http://eu.ceph.com/。
echo deb http://eu.ceph.com/debian-bobtail/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
 2.3.2.2  稳定版——Argonaut
Argonaut is the previous major release of Ceph. These packages are recommended for those who have already deployed argonaut in production and are not yet ready to upgrade.
Argonaut是ceph的前一个主要发布，这些包适用于那些已经在生产环境部署了argonaut又没准备好升级的。
Add our package repository to your system’s list of APT sources. See the argonaut Debian repository for a complete list of distributions supported.
把我们的软件库加到APT源列表，所支持发行版的完整列表见the argonaut Debian repository。
echo deb http://ceph.com/debian-argonaut/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
For the European users there is also a mirror in the Netherlands at http://eu.ceph.com/
欧洲用户可以访问位于荷兰的镜像http://eu.ceph.com/。
echo deb http://eu.ceph.com/debian-argonaut/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
 2.3.2.3  开发版软件包
Development Release Packages
Our development process generates a new release of Ceph every 3-4 weeks. These packages are faster-moving than the stable releases, as they get new features integrated quickly, while still undergoing several weeks of QA prior to release.
在我们开发过程中，每3-4周会发布一次，这些包比稳定版变迁得快，因为它们经常集成进新功能，然后经历几周时间的质检考验才能正式发布。
Add our package repository to your system’s list of APT sources. See the testing Debian repository for a complete list of distributions supported.
把我们的软件库加到APT源列表，所支持发行版的完整列表见the testing Debian repository。
echo deb http://ceph.com/debian-testing/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
For the European users there is also a mirror in the Netherlands at http://eu.ceph.com/
欧洲用户可以访问位于荷兰的镜像http://eu.ceph.com/。
echo deb http://eu.ceph.com/debian-testing/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
 2.3.2.4  开发测试软件包
Development Testing Packages
We automatically build Debian and Ubuntu packages for current development branches in the Ceph source code repository. These packages are intended for developers and QA only.
我们自动构建当前开发分支的Debian和Ubuntu包，这些包是给开发者和QA用的。
Packages are cryptographically signed with the autobuild.asc key. Add our autobuild key to your system’s list of trusted keys to avoid a security warning:
这些软件包用autobuild.asc加密签名过，把我们的autobuild密钥加到您系统的可信密钥列表中可避免安全警告：
wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/autobuild.asc' | sudo apt-key add -
Add our package repository to your system’s list of APT sources, but replace {BRANCH} with the branch you’d like to use (e.g., chef-3, wip-hack, master, stable). See the gitbuilder page for a complete list of distributions we build.
把我们的软件库加到APT源列表，并把{branch}替换为你想用的分支（如：chef-3、wip-hack、master、stable）。我们构建的的完整发行版列表见the gitbuilder page。
echo deb http://gitbuilder.ceph.com/ceph-deb-$(lsb_release -sc)-x86_64-basic/ref/{BRANCH} $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
 2.3.3  安装软件包
Installing Packages
Once you have added either release or development packages to APT, you should update APT’s database and install Ceph:
把正式发布或开发包加到APT后，要更新APT数据库，再安装ceph：
sudo apt-get update && sudo apt-get install ceph
 2.4  RPM包的安装
Installing RPM Packages
You may install stable release packages (for stable deployments), development release packages (for the latest features), or development testing packages (for development and QA only). Do not add multiple package sources at the same time.
你可以选择安装稳定发行包（用于稳定部署）、开发发行包（用于发掘最新功能）、或开发测试包（仅用于开发和质检）。不要同时添加多个软件源。
 2.4.1  安装发布密钥
Install Release Key
Packages are cryptographically signed with the release.asc key. Add our release key to your system’s list of trusted keys to avoid a security warning:
软件包都用release.asc密钥加密签名过，把我们的发布密钥加到您系统的可信密钥列表中可避免安全警告：
sudo rpm --import 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc'
 2.4.2  添加发布包
Add Release Packages
 2.4.2.1  稳定版——Bobtail
Bobtail is the most recent major release of Ceph. These packages are recommended for anyone deploying Ceph in a production environment. Critical bug fixes are backported and point releases are made as necessary.
bobtail是ceph最新的主要发布，这些软件包适合任何想在生产环境下部署ceph的人。重大缺陷的修复都移植过来了，次版本会适时推出。
Packages are currently built for the RHEL/CentOS6 (el6), Fedora 17 (f17), OpenSUSE 12 (opensuse12), and SLES (sles11) platforms. The repository package installs the repository details on your local system for use with yum or up2date.
现在我们给RHEL/CentOS6(el6)、Fedora 17(f17)、OpenSUSE 12 (opensuse12)、和SLES (sles11)构建包，软件库包会给你安装软件仓库，具体到系统可能用于yum或up2date。
Replace the``{DISTRO}`` below with the distro codename:
用发行版代码名替换下面命令中的{DISTRO}：
su -c 'rpm -Uvh http://ceph.com/rpm-bobtail/{DISTRO}/x86_64/ceph-release-1-0.el6.noarch.rpm'
For example, for CentOS 6 or other RHEL6 derivatives (el6):
例如，拿CentOS 6或其他RHEL6衍生物(e16)来说：
su -c 'rpm -Uvh http://ceph.com/rpm-bobtail/el6/x86_64/ceph-release-1-0.el6.noarch.rpm'
You can download the RPMs directly from:
你也能直接从下列地址下载RPM：
http://ceph.com/rpm-bobtail
 2.4.2.2  开发版包
Development Release Packages
Our development process generates a new release of Ceph every 3-4 weeks. These packages are faster-moving than the stable releases. Development packages have new features integrated quickly, while still undergoing several weeks of QA prior to release.
在我们开发过程中，每3-4周会发布一次，这些包比稳定版变迁得快，因为它们经常集成进新功能，然后经历 几周时间的质检考验才能正式发布。
Packages are cryptographically signed with the release.asc key. Add our release key to your system’s list of trusted keys to avoid a security warning:
软件包用release.asc加密签名过，把我们的发布密钥放入可信密钥列表中可避免安全警告：
sudo rpm --import 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/autobuild.asc'
Packages are currently built for the CentOS-6 and Fedora 17 platforms. The repository package installs the repository details on your local system for use with yum or up2date.
当前我们给CentOS-6和Fedora 17平台构建软件包，软件库包会安装软件仓库，具体到系统可能用于yum或up2date。
For CentOS-6:
对CentOS 6来说：
su -c 'rpm -Uvh http://ceph.com/rpms/el6/x86_64/ceph-release-1-0.el6.noarch.rpm'
For Fedora 17:
对Fedora 17来说：
su -c 'rpm -Uvh http://ceph.com/rpms/fc17/x86_64/ceph-release-1-0.fc17.noarch.rpm'
You can download the RPMs directly from:
你可以从下列地址直接下载RPM：
http://ceph.com/rpm-testing
 2.4.3  安装软件包
Installing Packages
Once you have added either release or development packages to yum, you can install Ceph:
把发布版或开发版源加入yum后，可以这样安装ceph：
sudo yum install ceph
 2.5  升级ceph
Upgrading Ceph
You can upgrade daemons in your Ceph cluster one-by-one while the cluster is online and in service! The upgrade process is relatively simple:
你可以在集群在线且提供服务时，逐个升级集群中的守护进程！升级过程相对简单：
1. Login to a host and upgrade the Ceph package.
登录主机并升级ceph包。
2. Restart the daemon.
重启守护进程。
3. Ensure your cluster is healthy.
确认集群是健康的。
Important: Once you upgrade a daemon, you cannot downgrade it.
重要：一旦升级了守护进程，就不能再降级。
Certain types of daemons depend upon others. For example, metadata servers and RADOS gateways depend upon Ceph monitors and OSDs. We recommend upgrading daemons in this order:
特定类型的守护进程依赖于其它的，例如元数据服务器和RADOS网关依赖于ceph监视器和OSD，所以我们推荐按下列顺序升级：
1. Monitors (or OSDs)
2. OSDs (or Monitors)
3. Metadata Servers
4. RADOS Gateway
As a general rule, we recommend upgrading all the daemons of a specific type (e.g., all ceph-osd daemons, all ceph-mon daemons, etc.) to ensure that they are all on the same release. We also recommend that you upgrade all the daemons in your cluster before you try to excercise new functionality in a release.
作为一般规则，我们推荐一次性升级同一类型的所有守护进程（如所有ceph-osd守护进程、所有ceph-mon等等）以保证它们都处于同一版本。同时建议您升级完集群中的所有守护进程后再练习那个版本的新功能。
The following sections describe the upgrade process.
下面几段描述了升级过程。
Important: Each release of Ceph may have some additional steps. Refer to release-specific sections for details BEFORE you begin upgrading daemons.
重要：每次版本升级都可能有例外步骤，升级前请参考与此版本相关的部分。
 2.5.1  升级OSD
Upgrading an OSD
To upgrade an OSD peform the following steps:
要升级OSD，执行下列步骤：
1. Upgrade the OSD package:
升级OSD包：
ssh {osd-host}
sudo apt-get update && sudo apt-get install ceph
2. Restart the OSD, where N is the OSD number:
重启OSD，N是OSD号：
service ceph restart osd.N
3. Ensure the upgraded OSD has rejoined the cluster:
确认升级后的OSD加入了集群：
ceph osd stat
Once you have successfully upgraded an OSD, you may upgrade another OSD until you have completed the upgrade cycle for all of your OSDs.
成功升级一个OSD后，再继续其它的，直到完成所有OSD。
 2.5.2  升级监视器
Upgrading a Monitor
To upgrade a monitor, perform the following steps:
执行下列步骤升级监视器：
1. Upgrade the ceph package:
升级ceph包：
ssh {mon-host}
sudo apt-get update && sudo apt-get install ceph
2. Restart the monitor:
重启监视器：
service ceph restart mon.{name}
3. Ensure the monitor has rejoined the quorum.
确认监视器重入法定人数。
ceph mon stat
Once you have successfully upgraded a monitor, you may upgrade another monitor until you have completed the upgrade cycle for all of your monitors.
成功升级一个监视器后，再继续其它的，直到完成所有监视器。
 2.5.3  升级元数据服务器
Upgrading a Metadata Server
To upgrade an MDS, perform the following steps:
执行下列步骤升级MDS：
1. Upgrade the ceph package:
升级ceph包：
ssh {mds-host}
sudo apt-get update && sudo apt-get install ceph
2. Restart the metadata server:
重启元数据服务器：
service ceph restart mds.{name}
3. Ensure the metadata server is up and running:
确认元数据服务器启动且运行着：
ceph mds stat
Once you have successfully upgraded a metadata, you may upgrade another metadata server until you have completed the upgrade cycle for all of your metadata servers.
成功升级一个元数据服务器后，再继续其它的，直到完成所有元数据服务器。
 2.5.4  升级客户端
Upgrading a Client
Once you have upgraded the packages and restarted daemons on your Ceph cluster, we recommend upgrading ceph-common and client libraries (librbd1 and librados2) on your client nodes too.
完成ceph集群上软件包和守护进程的升级后，我们建议也升级一下客户端节点上的ceph-common和客户端库（librbd1和librados2）。
1. Upgrade the package:
升级软件包：
ssh {client-host}
apt-get update && sudo apt-get install ceph-common librados2 librbd1 python-ceph
2. Ensure that you have the latest version:
确认你升级到了最新版本：
ceph --version
 2.5.5  从Argonaut升级到Bobtail
Upgrading from Argonaut to Bobtail
When upgrading from Argonaut to Bobtail, you need to be aware of three things:
从Argonaut升级到Bobtail时，要注意三件事：
1. Authentication now defaults to ON, but used to default to off.
现在默认开启了认证，但以前默认关闭。
2. Monitors use a new internal on-wire protocol
监视器使用了新的内部on-wire协议（一个时间同步协议？）
3. RBD format2 images require updgrading all OSDs before using it.
使用RBD format2格式的映像前要先升级完所有OSD。
See the following sections for details.
详情参见下列段落。
 2.5.5.1  认证
Authentication
The Ceph Bobtail release enables authentication by default. Bobtail also has finer-grained authentication configuration settings. In previous versions of Ceph (i.e., actually v 0.55 and earlier), you could simply specify:
ceph的bobtail版默认启用认证，可配置粒度也更细。在之前版本的ceph中（如v0.55及更早），你可以简单地指定：
auth supported = [cephx | none]
This option still works, but is deprecated. New releases support cluster, service and client authentication settings as follows:
这个选项仍然可用，但已过时。新版可分别支持cluster、service和client认证设置：
auth cluster required = [cephx | none]  # default cephx
auth service required = [cephx | none] # default cephx
auth client required = [cephx | none] # default cephx,none

Important: If your cluster does not currently have an auth supported line that enables authentication, you must explicitly turn it off in Bobtail using the settings below.:
重要：如果你现在的集群上没有用auth supported启用认证，在bobtail里你必须用下列配置显式地关闭认证：
auth cluster required = none
auth service required = none
This will disable authentication on the cluster, but still leave clients with the default configuration where they can talk to a cluster that does enable it, but do not require it.
这会在集群上关闭认证，但默认启用了认证的客户端仍然可以和集群通讯，不是必需的。

Important: If your cluster already has an auth supported option defined in the configuration file, no changes are necessary.
重要：如果你的集群已经配置了auth supported，那就没必要改了。
See Ceph Authentication - Backward Compatibility for details.
详情参见Ceph Authentication - Backward Compatibility。
 2.5.5.2  监视器on-wire协议
Monitor On-wire Protocol
We recommend upgrading all monitors to Bobtail. A mixture of Bobtail and Argonaut monitors will not be able to use the new on-wire protocol, as the protocol requires all monitors to be Bobtail or greater. Upgrading only a majority of the nodes (e.g., two out of three) may expose the cluster to a situation where a single additional failure may compromise availability (because the non-upgraded daemon cannot participate in the new protocol). We recommend not waiting for an extended period of time between ceph-mon upgrades.
我们建议把所有监视器都升级到bobtail版， Bobtail和Argonaut版的监视器混合将不能使用新的on-wire协议，因为此协议要求所有监视器是bobtail或更高版本。只升级大部分节点（如3个中的2个）会使集群陷入一种窘境——再失败一个节点就可能危及可用性，因为未升级的守护进程不能参与新协议。我们建议在ceph-mon升级时不要间隔太长时间。
 2.5.5.3  RBD映像
RBD Images
The Bobtail release supports format 2 images! However, you should not create or use format 2 RBD images until after all ceph-osd daemons have been upgraded. Note that format 1 is still the default. You can use the new ceph osd ls and ceph tell osd.N version commands to doublecheck your cluster. ceph osd ls will give a list of all OSD IDs that are part of the cluster, and you can use that to write a simple shell loop to display all the OSD version strings:
bobtail版支持format 2格式的映像！然而你在升级完所有ceph-osd守护进程前不应该创建或使用format 2格式的RBD映像。注意，默认仍是format 1。你可以用ceph osd ls和ceph tell osd.N version命令检查集群版本，ceph osd ls会列出集群里所有OSD的ID，你可以把它们代进一个简单的shell循环来显示所有OSD版本号：
for i in $(ceph osd ls); do
    ceph tell osd.${i} version
done
 2.6  构建前提
Build Prerequisites
Tip: Check this section to see if there are specific prerequisites for your Linux/Unix distribution.
提示：检查下这段，看看你的Linux/Unix发行版是否具备了必要前提。
Before you can build Ceph source code, you need to install several libraries and tools. Ceph provides autoconf and automake scripts to get you started quickly. Ceph build scripts depend on the following:
在构建ceph源码前，你得先安装几个库和工具。ceph用autoconf和automake脚本来简化构建，这些脚本依赖下列：
autotools-dev
autoconf
automake
cdbs
gcc
g++
git
libboost-dev
libedit-dev
libssl-dev
libtool
libfcgi
libfcgi-dev
libfuse-dev
linux-kernel-headers
libcrypto++-dev
libcrypto++
libexpat1-dev
pkg-config
libcurl4-gnutls-dev
On Ubuntu, execute sudo apt-get install for each dependency that isn’t installed on your host.
在Ubuntu上，执行sudo apt-get install安装缺失依赖。
sudo apt-get install autotools-dev autoconf automake cdbs gcc g++ git libboost-dev libedit-dev libssl-dev libtool libfcgi libfcgi-dev libfuse-dev linux-kernel-headers libcrypto++-dev libcrypto++ libexpat1-dev
On Debian/Squeeze, execute aptitude install for each dependency that isn’t installed on your host.
在Debian/Squeeze上，执行aptitude install安装缺失依赖。
aptitude install autotools-dev autoconf automake cdbs gcc g++ git libboost-dev libedit-dev libssl-dev libtool libfcgi libfcgi-dev libfuse-dev linux-kernel-headers libcrypto++-dev libcrypto++ libexpat1-dev pkg-config libcurl4-gnutls-dev
On Debian/Wheezy, you may also need:
在Debian/Wheezy上，你可能还需要：
keyutils-dev libaio and libboost-thread-dev

Note: Some distributions that support Google’s memory profiler tool may use a different package name (e.g., libgoogle-perftools4).
注意：有些支持Google内存分析器工具的发行版可能用了不同的软件包名字（如libgoogle-perftools4）。
 2.6.1  Ubuntu
uuid-dev
libkeyutils-dev
libgoogle-perftools-dev
libatomic-ops-dev
libaio-dev
libgdata-common
libgdata13
libsnappy-dev
libleveldb-dev
Execute sudo apt-get install for each dependency that isn’t installed on your host.
执行sudo apt-get install安装缺失依赖。
sudo apt-get install uuid-dev libkeyutils-dev libgoogle-perftools-dev libatomic-ops-dev libaio-dev libgdata-common libgdata13 libsnappy-dev libleveldb-dev
 2.6.2  Debian
Alternatively, you may also install:
另外可能还要安装：
aptitude install fakeroot dpkg-dev
aptitude install debhelper cdbs libexpat1-dev libatomic-ops-dev
 2.6.3  openSUSE 11.2（及更高版）
boost-devel
gcc-c++
libedit-devel
libopenssl-devel
fuse-devel (optional)
Execute zypper install for each dependency that isn’t installed on your host.
执行zypper install安装缺失依赖。
zypper install boost-devel gcc-c++ libedit-devel libopenssl-devel fuse-devel
 2.7  下载ceph发布压缩包
Downloading a Ceph Release Tarball
As Ceph development progresses, the Ceph team releases new versions of the source code. You may download source code tarballs for Ceph releases here:
随着开发的推进，ceph团队会经常发布新版源码，你可以从下列地点下载压缩包：
Ceph Release Tarballs
Ceph Release Tarballs (EU mirror)
 2.8  安装git版
Set Up Git
To clone the Ceph git repository, you must have git installed on your local host.
要克隆ceph的git仓库，你得先在本机安装git。
 2.8.1  安装git
Install Git
To install git, execute:
执行下列命令安装git：
sudo apt-get install git
You must also have a github account. If you do not have a github account, go to github.com and register. Follow the directions for setting up git at Set Up Git.
你还得有一个github帐户，如果你没有，去github.com注册一个，然后继续随着安装git版配置git。
 2.8.2  生成SSH密钥对
Generate SSH Keys
If you intend to commit code to Ceph or to clone using SSH (git@github.com:ceph/ceph.git), you must generate SSH keys for github.
如果你要向ceph贡献代码，或者通过SSH克隆（git@github.com:ceph/ceph.git），就必须生成用于github的SSH密钥对。
Tip: If you only intend to clone the repository, you may use git clone --recursive https://github.com/ceph/ceph.git without generating SSH keys.
提示：如果你只想克隆仓库，可以用git clone --recursive  https://github.com/ceph/ceph.git，而不必生成SSH密钥对。
To generate SSH keys for github, execute:
要为github生成SSH密钥对，执行：
ssh-keygen
Get the key to add to your github account (the following example assumes you used the default file path):
把生成的密钥加进你的github帐户（下例假设你用的是默认路径）：
cat .ssh/id_rsa.pub
Copy the public key.
拷贝公钥。
 2.8.3  添加密钥
Add the Key
Go to your your github account, click on “Account Settings” (i.e., the ‘tools’ icon); then, click “SSH Keys” on the left side navbar.
进入你的github帐户，点击“Account Settings”；然后点击左边导航条的“SSH Keys”。
Click “Add SSH key” in the “SSH Keys” list, enter a name for the key, paste the key you generated, and press the “Add key” button.
点击“SSH Keys”列表中的“Add SSH key”、给密钥输入名字、粘贴你生成的密钥、并按下“Add key”按钮。
 2.9  克隆ceph源码仓库
Cloning the Ceph Source Code Repository
To clone the source, you must install Git. See Set Up Git for details.
要克隆源码，必须安装git，详情参见安装git版。
 2.9.1  克隆源码
Clone the Source
To clone the Ceph source code repository, execute:
要克隆ceph源码仓库，执行：
git clone --recursive https://github.com/ceph/ceph.git
Once git clone executes, you should have a full copy of the Ceph repository.
git clone执行完毕后，你就得到了ceph仓库的完整副本。
Tip: Make sure you maintain the latest copies of the submodules included in the repository. Running git status will tell you if the submodules are out of date.
提示：要保证你维护着仓库中子模块的最新副本，用git status查看子模块是否过期。

cd ceph
git status
If your submodules are out of date, run:
如果你的子模块过期了，运行：
git submodule update
 2.9.2  选择分支
Choose a Branch
Once you clone the source code and submodules, your Ceph repository will be on the master branch by default, which is the unstable development branch. You may choose other branches too.
克隆源码和子模块后，ceph仓库默认会位于master分支，它是不稳定开发分支。你也可以选择其它分支：
master: The unstable development branch.
stable: The bugfix branch.
next: The release candidate branch.
git checkout master
 2.10  构建ceph
Building Ceph
Ceph provides automake and configure scripts to streamline the build process. To build Ceph, navigate to your cloned Ceph repository and execute the following:
ceph提供了automake和configure脚本来简化构建过程。要构建ceph，进入你克隆的仓库、执行下列命令：
cd ceph
./autogen.sh
./configure
make

Hyperthreading
You can use make -j to execute multiple jobs depending upon your system. For example, make -j4 for a dual core processor may build faster.
超线程
根据你的系统可以用make -j同时运行多个任务，例如make -j4在双核CPU上会编译得更快。
To install Ceph locally, you may also use:
要把ceph安装到本地，你可以用：
sudo make install
If you install Ceph locally, make will place the executables in usr/local/bin. You may add the Ceph configuration file to the usr/local/bin directory to run an evaluation environment of Ceph from a single directory.
如果你在本地安装ceph，make会把可执行文件放到usr/local/bin下。你可以把ceph配置文件放到usr/local/bin目录下，以搭建ceph评估环境。
 2.11  安装oprofile
Installing Oprofile
The easiest way to profile Ceph’s CPU consumption is to use the oprofile system-wide profiler.
分析ceph CPU消耗情况的最简方法就是用系统级的oprofile。
 2.11.1  安装
Installation
If you are using a Debian/Ubuntu distribution, you can install oprofile by executing the following:
如果你在用Debian/Ubuntu发行版，可以用下列命令安装oprofile：
sudo apt-get install oprofile oprofile-gui
 2.11.2  编译适合分析的ceph
Compiling Ceph for Profiling
To compile Ceph for profiling, first clean everything.
要编译适合分析的ceph，首先清理干净。
make distclean
Then, export the following settings so that you can see callgraph output.
然后，执行下列命令才能看到输出的调用图。
export CFLAGS="-fno=omit-frame-pointer -O2 -g"
Finally, compile Ceph.
最后，编译ceph。
./autogen.sh
./configure
make
You can use make -j to execute multiple jobs depending upon your system. For example:
你可以用make -j同时执行多个任务，例如：
make -j4
 2.11.3  ceph配置
Ceph Configuration
Ensure that you disable lockdep. Consider setting logging to levels appropriate for a production cluster. See Ceph Logging and Debugging for details.
确保禁用了lockdep。生产集群应设置合理的日志级别，参见错误：引用源未找到。
See the CPU Profiling section of the RADOS Operations documentation for details on using Oprofile.
关于Oprofile的使用，请参考RADOS Operations文档的CPU profiling部分。
 2.12  构建ceph包
Build Ceph Packages
To build packages, you must clone the Ceph repository. You can create installation packages from the latest code using dpkg-buildpackage for Debian/Ubuntu or rpmbuild for the RPM Package Manager.
要构建包，比必须克隆ceph仓库，你可以用最新代码构建安装包，Debian/Ubuntu下用dpkg-buildpackage、用rpmbuild构建RPM安装包。
Tip: When building on a multi-core CPU, use the -j and the number of cores * 2. For example, use -j4 for a dual-core processor to accelerate the build.
提示：在多核CPU上构建时，使用-j加CPU核心数乘2可更快地编译，比如双核CPU上用-j4。
 2.12.1  高级包管理器（APT）
Advanced Package Tool (APT)
To create .deb packages for Debian/Ubuntu, ensure that you have cloned the Ceph repository, installed the build prerequisites and installed debhelper:
要为Debian/Ubuntu创建.deb包，先克隆ceph仓库、完成构建前提并安装debhelper：
sudo apt-get install debhelper
Once you have installed debhelper, you can build the packages:
安装debhelper后，就可以构建包了：
sudo dpkg-buildpackage
For multi-processor CPUs use the -j option to accelerate the build.
有多CPU时用-j选项加快构建。
 2.12.2  RPM包管理器
RPM Package Manager
To create .rpm packages, ensure that you have cloned the Ceph repository, installed the build prerequisites and installed rpm-build and rpmdevtools:
要创建.rpm包，先克隆ceph仓库、完成构建前提并安装rpm-build和rpmdevtools：
yum install rpm-build rpmdevtools
Once you have installed the tools, setup an RPM compilation environment:
安装完工具后，设置RPM编译环境：
rpmdev-setuptree
Fetch the source tarball for the RPM compilation environment:
把源码包下载到RPM编译环境：
wget -P ~/rpmbuild/SOURCES/ http://ceph.com/download/ceph-<version>.tar.gz
Or from the EU mirror:
或者从欧洲镜像：
wget -P ~/rpmbuild/SOURCES/ http://eu.ceph.com/download/ceph-<version>.tar.gz
Build the RPM packages:
构建RPM包：
rpmbuild -tb ~/rpmbuild/SOURCES/ceph-<version>.tar.gz
For multi-processor CPUs use the -j option to accelerate the build.
有多CPU时用-j选项加快构建。
 2.13  贡献代码
Contributing Source Code
If you are making source contributions to the Ceph project, you must be added to the Ceph project on github.
如果你打算为ceph贡献代码，必须加入github上的ceph项目。
 3  RADOS对象存储
RADOS Object Store
Ceph’s RADOS Object Store is the foundation for all Ceph clusters. When you use object store clients such as the CephFS filesystem, the RESTful Gateway or Ceph block devices, Ceph reads data from and writes data to the object store. Ceph’s RADOS Object Stores consist of two types of daemons: Object Storage Daemons (OSDs) store data as objects on storage nodes; and Monitors maintain a master copy of the cluster map. A Ceph cluster may contain thousands of storage nodes. A minimal system will have at least two OSDs for data replication.
ceph的RADOS对象存储是所有ceph集群的基础，当你使用类似CephFS文件系统、RESTful网关或ceph块设备的对象存储客户端时，ceph从对象存储读取和写入数据。ceph的RADOS对象存储包括两类守护进程：对象存储守护进程（OSD）把存储节点上的数据存储为对象；监视器集群维护着集群运行图的主拷贝。一个ceph集群可以包含数千个存储节点，最简系统至少需要两个OSD才能做到数据复制。

配置和部署
CONFIG AND DEPLOY
Once you have installed Ceph packages, you must configure. There are a a few required settings, but most configuration settings have default values. Following the initial configuration, you must deploy Ceph. Deployment consists of creating and initializing data directories, keys, etc.
安装ceph包后必须配置。多数选项都有默认值，但有几个地方必须更改才能使用；最初的步骤有初始化数据目录、密钥等。
运维
Operations
Once you have a deployed Ceph cluster, you may begin operating your cluster.
部署后就可以开始操作ceph集群了。
APIs
Most Ceph deployments use Ceph block devices, the gateway and/or the CephFS filesystem. You may also develop applications that talk directly to the Ceph object store.
大多数ceph部署都使用了ceph块设备、网关和/或CephFS文件系统。也可以开发程序直接和ceph对象存储对接。
 3.1  配置
Configuration
Ceph can run with a cluster containing thousands of Object Storage Devices (OSDs). A minimal system will have at least two OSDs for data replication. To configure OSD clusters, you must provide settings in the configuration file. Ceph provides default values for many settings, which you can override in the configuration file. Additionally, you can make runtime modification to the configuration using command-line utilities.
ceph作为集群时可以包含数千个对象存储设备（OSD）。最简系统至少需要二个OSD做数据复制。要配置OSD集群，你得把配置写入配置文件。ceph对很多选项提供了默认值，你可以在配置文件里覆盖它们；另外，你可以使用命令行工具修改运行时配置。

When Ceph starts, it activates three daemons:
ceph启动时要激活三种守护进程：
ceph-osd (mandatory)必备
ceph-mon (mandatory)必备
ceph-mds (mandatory for cephfs only)	cephfs必备
Each process, daemon or utility loads the host’s configuration file. A process may have information about more than one daemon instance (i.e., multiple contexts). A daemon or utility only has information about a single daemon instance (a single context).
各进程、守护进程或工具都会读取配置文件。一个进程可能需要不止一个守护进程例程的信息（如多个上下文）；一个守护进程或工具只有关于单个守护进程例程的信息（单上下文）。
Note: Ceph can run on a single host for evaluation purposes.
注意：试用时Ceph可运行在单机上。
 3.1.1  硬盘和文件系统
 3.1.1.1  准备硬盘
HARD DISK PREP
Ceph aims for data safety, which means that when the application receives notice that data was written to the disk, that data was actually written to the disk. For old kernels (<2.6.33), disable the write cache if the journal is on a raw disk. Newer kernels should work fine.
ceph注重数据安全，就是说，它收到数据已写入硬盘的通知时，数据确实已写入硬盘。使用较老的内核（版本小于2.6.33）时，如果日志在原始硬盘上，就要禁用写缓存；较新的内核没问题。

Use hdparm to disable write caching on the hard disk:
用hdparm禁用硬盘的写缓冲功能。
sudo hdparm -W 0 /dev/hda 0

In production environments, we recommend running OSDs with an operating system disk, and a separate disk(s) for data. If you run data and an operating system on a single disk, create a separate partition for your data before configuring your OSD cluster.
在生产环境，建议您在系统盘运行OSD，在另外的硬盘里放数据。如果必须把数据和系统放在一个硬盘里，最好给数据分配一个单独的分区。
 3.1.1.2  文件系统
FILE SYSTEMS
Ceph OSDs rely heavily upon the stability and performance of the underlying file system.
ceph的OSD依赖于底层文件系统的稳定性和性能。

Note: We currently recommend XFS for production deployments. We recommend btrfs for testing, development, and any non-critical deployments. We believe that btrfs has the correct feature set and roadmap to serve Ceph in the long-term, but XFS and ext4 provide the necessary stability for today’s deployments. btrfs development is proceeding rapidly: users should be comfortable installing the latest released upstream kernels and be able to track development activity for critical bug fixes.
注意：当前，我们推荐部署生产系统时使用xfs文件系统；推荐用btrfs做测试、开发和其他不太要紧的部署。我们相信，长期来看btrfs适合ceph的功能需求和发展方向，但是xfs和ext4能提供当前部署所必需的稳定性。btrfs开发在迅速推进，用户应该有能力经常更新到最新内核发布，而且能跟踪严重bug的修正进度。

Ceph OSDs depend on the Extended Attributes (XATTRs) of the underlying file system for various forms of internal object state and metadata. The underlying file system must provide sufficient capacity for XATTRs. btrfs does not bound the total xattr metadata stored with a file. XFS has a relatively large limit (64 KB) that most deployments won’t encounter, but the ext4 is too small to be usable.
ceph的OSD有赖于底层文件系统的扩展属性（XATTR）存储各种内部对象状态和元数据。底层文件系统必须给XATTR提供足够容量，btrfs没有限制随文件存储的xattr元数据；xfs的限制相对大(64KB)，多数部署都不会有瓶颈；ext4的则太小而不可用。要用这些文件系统，你得把下面这行写入ceph.conf的[osd]段里。
You should always add the following line to the [osd] section of your ceph.conf file for ext4 filesystems; you can optionally use it for btrfs and XFS.:
使用ext4文件系统时，应该一直把下面的配置放于[osd]段下；用btrfs和xfs时可以选填。
filestore xattr use omap = true
 3.1.1.3  文件系统背景知识
FS BACKGROUND INFO
The XFS and btrfs file systems provide numerous advantages in highly scaled data storage environments when compared to ext3 and ext4. Both XFS and btrfs are journaling file systems, which means that they are more robust when recovering from crashes, power outages, etc. These filesystems journal all of the changes they will make before performing writes.
xfs和btrfs相比较ext3/4而言，在高伸缩性数据存储方面有几个优势，xfs和btrfs都是日志文件系统，这使得在崩溃、断电后恢复时更健壮，因为文件系统在写入数据前会先记录所有变更。

XFS was developed for Silicon Graphics, and is a mature and stable filesystem. By contrast, btrfs is a relatively new file system that aims to address the long-standing wishes of system administrators working with large scale data storage environments. btrfs has some unique features and advantages compared to other Linux filesystems.
xfs由Silicon Graphics开发，是一个成熟、稳定的文件系统。相反，btrfs是相对年轻的文件系统，它致力于实现系统管理员梦寐以求的大规模数据存储基础，和其他Linux文件系统相比它有独一无二的功能和优势。

btrfs is a copy-on-write filesystem. It supports file creation timestamps and checksums that verify metadata integrity, so it can detect bad copies of data and fix them with the good copies. The copy-on-write capability means that btrfs can support snapshots that are writable. btrfs supports transparent compression and other features.
btrfs是写时复制（ copy-on-write, cow）文件系统，它支持文件创建时间戳和校验和（可校验元数据完整性）功能，所以它能探测到数据坏副本，并且用好副本修复。写时复制功能是说btrfs支持可写文件系统快照。btrfs也支持透明压缩和其他功能。

btrfs also incorporates multi-device management into the file system, which enables you to support heterogeneous disk storage infrastructure, data allocation policies. The community also aims to provide fsck, deduplication, and data encryption support in the future. This compelling list of features makes btrfs the ideal choice for Ceph clusters.
btrfs也集成了多设备管理功能，据此可以在底层支持异质硬盘存储，和数据分配策略。未来开发社区还会提供fsck、拆分、数据加密功能，这些诱人的功能正是ceph集群的理想选择。
 3.1.2  ceph的配置
When you start the Ceph service, the initialization process activates a series of daemons that run in the background. The hosts in a typical Ceph cluster run at least one of four daemons:
你启动ceph服务的时候，初始化进程会把一系列守护进程放到后台运行。典型的ceph集群中至少要运行下面四种守护进程中的一种。
Object Storage Device (ceph-osd)
Monitor (ceph-mon)
Metadata Server (ceph-mds)
Ceph Gateway (radosgw)
For your convenience, each daemon has a series of default values (i.e., many are set by ceph/src/common/config_opts.h). You may override these settings with a Ceph configuration file.
对象存储设备(ceph-osd)
监视器(ceph-mon)
元数据服务器(ceph-mds)
ceph网关(radosgw)
为方便起见，每个守护进程都有一系列默认值（很多在 ceph/src/common/config_opts.h里面设置），你可以用ceph配置文件覆盖这些设置。
 3.1.2.1  配置文件ceph.conf
When you start a Ceph cluster, each daemon looks for a ceph.conf file that provides its configuration settings. For manual deployments, you need to create a ceph.conf file to configure your cluster. For third party tools that create configuration files for you (e.g., Chef), you may use the information contained herein as a reference. The ceph.conf file defines:
启动ceph集群时，每个守护进程都从ceph.conf里查找它自己的配置。手动配置时，你需要创建ceph.conf来配置集群，使用第三方工具（如chef）创建配置文件时可以参考下面的信息。ceph.conf文件定义了：
Authentication settings
Cluster membership
Host names
Host addresses
Paths to keyrings
Paths to journals
Paths to data
Other runtime options
认证配置
集群成员
主机名
主机IP地址
密钥路径
日志路径
数据路径
其它运行时选项
The default ceph.conf locations in sequential order include:
默认的ceph.conf位置相继排列如下：
1. $CEPH_CONF (i.e., the path following the $CEPH_CONF environment variable)
2. -c path/path (i.e., the -c command line argument)
3. /etc/ceph/ceph.conf
4. ~/.ceph/config
5. ./ceph.conf (i.e., in the current working directory)

The ceph.conf file uses an ini style syntax. You can add comments to the ceph.conf file by preceding comments with a semi-colon (;) or a pound sign (#). For example:
ceph文件使用ini风格的语法，以分号(;)和井号(#)开始的行是注释，如下：
# <--A number (#) sign precedes a comment.
; A comment may be anything.
# Comments always follow a semi-colon (;) or a pound (#) on each line.
# The end of the line terminates a comment.
# We recommend that you provide comments in your configuration file(s).
 3.1.2.2  配置段落
Config Sections
The ceph.conf file can configure all daemons in a cluster, or all daemons of a particular type. To configure a series of daemons, the settings must be included under the processes that will receive the configuration as follows:
ceph.conf可配置集群里的所有守护进程，或者某一类型的所有守护进程。要配置一系列守护进程，这些配置必须位于能收到配置的段落之下，比如：
[global]
Description:
Settings under [global] affect all daemons in a Ceph cluster.
[global]下的配置影响ceph集群里的所有守护进程。
Example:
auth supported = cephx
[osd]
Description:
Settings under [osd] affect all ceph-osd daemons in the cluster.
[osd]下的配置影响集群里的所有ceph-osd进程。
Example:
osd journal size = 1000
[mon]
Description:
Settings under [mon] affect all ceph-mon daemons in the cluster.
[mon]下的配置影响集群里的所有ceph-mon进程。
Example:
mon addr = 10.0.0.101:6789
[mds]
Description:
Settings under [mds] affect all ceph-mds daemons in the cluster.
[mds]下的配置影响集群里的所有ceph-mds进程。
Example:
host = myserver01
[client]
Description:
Settings under [client] affect all clients (e.g., mounted CephFS filesystems, mounted block devices, etc.)
[client]下的配置影响所有客户端（如挂载的CephFS文件系统、挂载的块设备等等）。
Example:
log file = /var/log/ceph/radosgw.log

Global settings affect all instances of all daemon in the cluster. Use the [global] setting for values that are common for all daemons in the cluster. You can override each [global] setting by:
全局设置影响集群内所有守护进程的例程，但可以用下面的设置覆盖[global]设置：
1. Changing the setting in a particular process type (e.g., [osd], [mon], [mds] ).
在[osd]、[mon]、[mds]下设置某一类的进程。
2. Changing the setting in a particular process (e.g., [osd.1] )
修改特定进程的设置，如[osd.1]。
Overriding a global setting affects all child processes, except those that you specifically override.
覆盖全局设置会影响所有子进程，明确剔除的例外。
A typical global setting involves activating authentication. For example:
典型的全局设置包括激活认证，例如：

[global]
	# Enable authentication between hosts within the cluster.
	# 在集群内的主机间认证
	#v 0.54 and earlier
	auth supported = cephx

	#v 0.55 and after
	auth cluster required = cephx
	auth service required = cephx
	auth client required = cephx

You can specify settings that apply to a particular type of daemon. When you specify settings under [osd], [mon] or [mds] without specifying a particular instance, the setting will apply to all OSDs, monitors or metadata daemons respectively.
你可以在[osd]、[mon]、[mds]下设置特定类型的守护进程，而无须指定特定例程，这些设置会分别影响所有OSD、监视器、元数据进程。
You may specify settings for particular instances of a daemon. You may specify an instance by entering its type, delimited by a period (.) and by the instance ID. The instance ID for an OSD is always numeric, but it may be alphanumeric for monitors and metadata servers.
你也可以设置一个守护进程的特定例程，一个例程由类型和它的例程编号（ID）确定，OSD的例程ID只能是数字，监视器和元数据服务器的ID可包含字母和数字。

[osd.1]
        # settings affect osd.1 only.

[mon.a]
        # settings affect mon.a only.

[mds.b]
        # settings affect mds.b only.
 3.1.2.3  元变量
METAVARIABLES
Metavariables simplify cluster configuration dramatically. When a metavariable is set in a configuration value, Ceph expands the metavariable into a concrete value. Metavariables are very powerful when used within the [global], [osd], [mon] or [mds] sections of your configuration file. Ceph metavariables are similar to Bash shell expansion.
元变量大大简化了集群配置，ceph会把配置的元变量展开为具体值；元变量功能很强大，可以用在[global]、[osd]、[mon]、[mds]段里，类似于bash的shell扩展。

Ceph supports the following metavariables:
ceph支持下面的元变量：
$cluster
Description:
Expands to the cluster name. Useful when running multiple clusters on the same hardware.
展开为集群名字，在同一套硬件上运行多个集群时有用。
Example:
/etc/ceph/$cluster.keyring
Default:
ceph
$type
Description:
Expands to one of mds, osd, or mon, depending on the type of the current daemon.
展开为mds、osd、mon中的一个，有赖于当前守护进程的类型。
Example:
/var/lib/ceph/$type
$id
Description:
Expands to the daemon identifier. For osd.0, this would be 0; for mds.a, it would be a.
展开为守护进程标识；对osd.0来说是0，mds.a是a。
Example:
/var/lib/ceph/$type/$cluster-$id
$host
Description:
Expands to the host name of the current daemon.
展开为当前守护进程的主机名。
$name
Description:
Expands to $type.$id.
展开为$type.$id
Example:
/var/run/ceph/$cluster-$name.asok

 3.1.2.4  共有设置
COMMON SETTINGS
The Hardware Recommendations section provides some hardware guidelines for configuring the cluster. It is possible for a single host to run multiple daemons. For example, a single host with multiple disks or RAIDs may run one ceph-osd for each disk or RAID. Additionally, a host may run both a ceph-mon and an ceph-osd daemon on the same host. Ideally, you will have a host for a particular type of process. For example, one host may run ceph-osd daemons, another host may run a ceph-mds daemon, and other hosts may run ceph-mon daemons.
硬件推荐段提供了一些配置集群的硬件指导。一台机器可以运行多个进程，例如一台机器有多个硬盘或RAID，可以为每个硬盘和RAID配置一个守护进程。另外，在同一台主机上也可以同时运行ceph-mon和ceph-osd，理想情况下一台主机应该只运行一类进程，例如：一台主机运行着ceph-osd进程，另一台主机运行着ceph-mds进程，ceph-mon进程又在另外一台主机上。

Each host has a name identified by the host setting. Monitors also specify a network address and port (i.e., domain name or IP address) identified by the addr setting. A basic configuration file will typically specify only minimal settings for each instance of a daemon. For example:
每个主机都有一个标识名称（系统配置），监视器可用addr选项指定网络地址和端口（如域名或IP地址），基本配置可以只指定最小配置。例如：

[mon.a]
        host = hostName
        mon addr = 150.140.130.120:6789
[osd.0]
        host = hostName

Important: The host setting is the short name of the host (i.e., not an fqdn). It is NOT and IP address either. Enter hostname -s on the command line to retrieve the name of the host. Also, this setting is ONLY for mkcephfs and manual deployment. It MUST NOT be used with chef or ceph-deploy.
重要：host是主机的短名字，不是正式域名FQDN，也不是IP地址；在执行hostname -s就可以得到短名字。此设置只在mkcephfs和手动部署时用到，不能用于chef或ceph-deploy。
 3.1.2.5  网络
See the Network Configuration Reference for a detailed discussion about configuring a network for use with Ceph.
关于ceph网络配置的讨论见网络配置参考。
 3.1.2.6  认证
Authentication
New in version Bobtail: 0.56
bobtail v0.56新增。
For Bobtail (v 0.56) and beyond, you should expressly enable or disable authentication in the [global] section of your Ceph configuration file.
对于v0.56及后来版本，要在配置文件的[global]中明确启用或禁用认证。
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
Additionally, you should enable message signing. See Cephx Config Reference and Cephx Authentication for details.
另外，你应该启用消息签名，详情见认证配置（cephx配置）和cephx认证。
Important: When upgrading, we recommend expressly disabling authentication first, then perform the upgrade. Once the upgrade is complete, re-enable authentication.
重要：我们建议，升级时先明确地关闭认证，再进行升级。等升级完成后再重新启用认证。
 3.1.2.7  监视器集群
MONITORS
Ceph production clusters typically deploy with a minimum 3 monitors to ensure high availability should a monitor instance crash. An odd number of monitors (3) ensures that the Paxos algorithm can determine which version of the cluster map is the most recent from a quorum of monitors.
典型的ceph生产集群至少部署3个监视器来确保高可靠性，它允许一个监视器例程崩溃。奇数个监视器（3个）确保PAXOS算法能确定一批监视器里哪个版本的集群运行图是最新的。
Note: You may deploy Ceph with a single monitor, but if the instance fails, the lack of a monitor may interrupt data service availability.
注意：一个ceph集群可以只有一个监视器，但是如果它失败了，因没有监视器数据服务就会中断。
Ceph monitors typically listen on port 6789. For example:
ceph监视器默认监听6789端口，例如：
[mon.a]
        host = hostName
        mon addr = 150.140.130.120:6789

By default, Ceph expects that you will store a monitor’s data under the following path:
默认情况下，ceph会在下面的路径存储监视器数据：
/var/lib/ceph/mon/$cluster-$id

You must create the corresponding directory yourself. With metavariables fully expressed and a cluster named “ceph”, the foregoing directory would evaluate to:
你必须手动创建对应目录，前述的元变量必须先全部展开，名为ceph的集群将展开为：
/var/lib/ceph/mon/ceph-a
You may override this path using the mon data setting. We don’t recommend changing the default location. Create the default directory on your new monitor host.
你可以用mon data选项更改此路径，但我们不推荐修改。用下面的命令在新监视器主机上创建默认目录：
ssh {new-mon-host}
sudo mkdir /var/lib/ceph/mon/ceph-{mon-letter}
 3.1.2.8  OSDs
Ceph production clusters typically deploy OSDs where one host has one OSD daemon running a filestore on one data disk. A typical deployment specifies a journal size and whether the file store’s extended attributes (XATTRs) use an object map (i.e., when running on the ext4 filesystem). For example:
典型的生产集群里，一个数据盘上运行一个OSD进程；典型部署要指定日志尺寸、文件存储的扩展属性（XATTR）是否使用对象图（如运行在ext4之上），例如：
[osd]
        osd journal size = 10000
        filestore xattr use omap = true	#enables the object map. Only if running ext4.

[osd.0]
        hostname = {hostname}

By default, Ceph expects that you will store an OSD’s data with the following path:
默认ceph认为你把OSD数据存储在下面的路径下：
/var/lib/ceph/osd/$cluster-$id

You must create the corresponding directory yourself. With metavariables fully expressed and a cluster named “ceph”, the foregoing directory would evaluate to:
你必须创建对应目录，名字为ceph的集群其元变量完全展开后，前述的目录将是：
/var/lib/ceph/osd/ceph-0

You may override this path using the osd data setting. We don’t recommend changing the default location. Create the default directory on your new OSD host.
你可以用osd data选项更改默认值，但我们不建议修改。用下面的命令在新OSD主机上创建默认目录：
ssh {new-osd-host}
sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}

The osd data path ideally leads to a mount point with a hard disk that is separate from the hard disk storing and running the operating system and daemons. If the OSD is for a disk other than the OS disk, prepare it for use with Ceph, and mount it to the directory you just created:
osd data路径应该指向一个硬盘的挂载点，这个硬盘应该独立于操作系统和守护进程所在硬盘。按下列步骤准备好并挂载：
ssh {new-osd-host}
sudo mkfs -t {fstype} /dev/{disk}
sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}

We recommend using the xfs file system or the btrfs file system when running command:mkfs.
我们推荐用xfs或btrfs文件系统，命令是mkfs。

By default, Ceph expects that you will store an OSDs journal with the following path:
ceph默认把OSD日志存储在下面的路径：
/var/lib/ceph/osd/$cluster-$id/journal

Without performance optimization, Ceph stores the journal on the same disk as the OSDs data. An OSD optimized for performance may use a separate disk to store journal data (e.g., a solid state drive delivers high performance journaling).
没有性能优化时，ceph把日志和OSD数据存储相同的硬盘上；要优化OSD性能，可以把日志分离到单独的硬盘上（例如，固态硬盘能提供高日志性能）。

Ceph’s default osd journal size is 0, so you will need to set this in your ceph.conf file. A journal size should find the product of the filestore min sync interval and the expected throughput, and multiple the product by two (2):
ceph的osd journal size默认值是0，所以你得在ceph.conf里设置，日志尺寸应该至少2倍于 filestore min sync interval的值和预计吞吐量的乘积：
osd journal size = {2 * (expected throughput * filestore min sync interval)}

The expected throughput number should include the expected disk throughput (i.e., sustained data transfer rate), and network throughput. For example, a 7200 RPM disk will likely have approximately 100 MB/s. Taking the min() of the disk and network throughput should provide a reasonable expected throughput. Some users just start off with a 10GB journal size. For example:
预计吞吐量应该包括硬盘吞吐量（持续的数据传输速度）和网络吞吐量，例如，7200转的硬盘速度大概是100MB/s，取硬盘和网络吞吐量中较小的一个应该能提供合理的预计吞吐量。一些用户以10GB起步，例如：
osd journal size = 10000
 3.1.2.9  日志、调试
LOGS / DEBUGGING
Ceph is still on the leading edge, so you may encounter situations that require modifying logging output and using Ceph’s debugging. To activate Ceph’s debugging output (i.e., dout()), you may add debug settings to your configuration. Ceph’s logging levels operate on a scale of 1 to 20, where 1 is terse and 20 is verbose.
ceph仍在前沿，所以你可能碰到一些情况，需要修改日志和调试信息。为打开ceph的调试输出（例如，dout()），你可以在配置文件里添加调试选项。ceph的日志级别在1到20之间，1是简洁、20是详细。
Note: See Debugging and Logging for details on log rotation.
注意：关于日志滚动见调试和日志记录。
Subsystems common to each daemon may be set under [global] in your configuration file. Subsystems for particular daemons are set under the daemon section in your configuration file (e.g., [mon], [osd], [mds]). For example:
对每个进程都相同的子系统可以在[global]下配置，对特定守护进程的子系统要配置在进程段下，如[mon]、[osd]、[mds]下。例如：

[global]
        debug ms = 1

[mon]
        debug mon = 20
        debug paxos = 20
        debug auth = 20

[osd]
        debug osd = 20
        debug filestore = 20
        debug journal = 20
        debug monc = 20

[mds]
        debug mds = 20
        debug mds balancer = 20
        debug mds log = 20
        debug mds migrator = 20
When your system is running well, choose appropriate logging levels and remove unnecessary debugging settings to ensure your cluster runs optimally. Logging debug output messages is relatively slow, and a waste of resources when operating your cluster.
你的系统运行良好的时候，应该选择合适的日志级别、关闭不必要的调试设置来确保集群运行在最佳状态。记录调试输出相对慢，且浪费资源。
Each subsystem has a logging level for its output logs, and for its logs in-memory. You may set different values for each of these subsystems by setting a log file level and a memory level for debug logging. For example:
每个子系统的日志都有它自己的输出级别、和内存存留级别。你可以给每个子系统分别设置不同的日志文件和内存日志级别，例如：
debug {subsystem} {log-level}/{memory-level}
#for example
debug mds log 1/20

Subsystem
Log Level
Memory Level
default
0
5
lockdep
0
5
context
0
5
crush
1
5
mds
1
5
mds balancer
1
5
mds locker
1
5
mds log
1
5
mds log expire
1
5
mds migrator
1
5
buffer
0
0
timer
0
5
filer
0
5
objecter
0
0
rados
0
5
rbd
0
5
journaler
0
5
objectcacher
0
5
client
0
5
osd
0
5
optracker
0
5
objclass
0
5
filestore
1
5
journal
1
5
ms
0
5
mon
1
5
monc
0
5
paxos
0
5
tp
0
5
auth
1
5
finisher
1
5
heartbeatmap
1
5
perfcounter
1
5
rgw
1
5
hadoop
1
5
asok
1
5
throttle
1
5
 3.1.2.10  ceph.conf实例
EXAMPLE CEPH.CONF
[global]
        # For version 0.54 and earlier, you may enable
        # authentication with the following setting.
        # Specifying `cephx` enables authentication;
        # and specifying `none` disables authentication.

        #auth supported = cephx

        # For version 0.55 and beyond, you must explicitly enable
        # or disable authentication with "auth" entries in [global].

        auth cluster required = cephx
        auth service required = cephx
        auth client required = cephx


[osd]
        osd journal size = 1000
        # uncomment the following line if you are mounting with ext4
        # filestore xattr use omap = true


        # For Bobtail (v 0.56) and subsequent versions, you may
        # add settings for mkcephfs so that it will create and mount
        # the file system for you. Remove the comment `#` character for
        # the following settings and replace the values in parenthesis
        # with appropriate values, or leave the following settings commented
        # out to accept the default values. You must specify the --mkfs
        # option with mkcephfs in order for the deployment script to
        # utilize the following settings, and you must define the 'devs'
        # option for each osd instance; see below.

        #osd mkfs type = {fs-type}
        #osd mkfs options {fs-type} = {mkfs options}   # default for xfs is "-f"
        #osd mount options {fs-type} = {mount options} # default mount option is "rw, noatime"

[mon.a]
        host = myserver01
        mon addr = 10.0.0.101:6789

[mon.b]
        host = myserver02
        mon addr = 10.0.0.102:6789

[mon.c]
        host = myserver03
        mon addr = 10.0.0.103:6789

[osd.0]
        host = myserver01
        #devs = {path-to-device}

[osd.1]
        host = myserver02
        #devs = {path-to-device}

[osd.2]
        host = myserver03
        #devs = {path-to-device}

[mds.a]
        host = myserver01
        #devs = {path-to-device}

 3.1.2.11  运行时更改
RUNTIME CHANGES
Ceph allows you to make changes to the configuration of an ceph-osd, ceph-mon, or ceph-mds daemon at runtime. This capability is quite useful for increasing/decreasing logging output, enabling/disabling debug settings, and even for runtime optimization. The following reflects runtime configuration usage:
ceph可以在运行时更改ceph-osd、ceph-mon、ceph-mds守护进程的配置，这种功能在增加/降低日志输出、启用/禁用调试设置、甚至是运行时优化的时候非常有用，下面是运行时配置的用法：
ceph {daemon-type} tell {id or *} injectargs '--{name} {value} [--{name} {value}]'
Replace {daemon-type} with one of osd, mon or mds. You may apply the runtime setting to all daemons of a particular type with *, or specify a specific daemon’s ID (i.e., its number or letter). For example, to increase debug logging for a ceph-osd daemon named osd.0, execute the following:
用osd、mon、mds中的一个替代{daemon-type}，你可以用星号(*)或具体进程ID（其数字或字母）把运行时配置应用到一类进程的所有例程，例如增加名为osd.0的ceph-osd进程的调试级别的命令如下：
ceph osd tell 0 injectargs '--debug-osd 20 --debug-ms 1'
In your ceph.conf file, you may use spaces when specifying a setting name. When specifying a setting name on the command line, ensure that you use an underscore or hyphen (_ or -) between terms (e.g., debug osd becomes debug-osd).
在ceph.conf文件里配置时用空格分隔关键词，但在命令行使用的时候要用下划线或连字符(_或-)分隔，例如debug osd变成debug-osd。
 3.1.2.12  查看运行时配置
VIEWING A CONFIGURATION AT RUNTIME
If your Ceph cluster is running, and you would like to see the configuration settings from a running daemon, execute the following:
如果你的ceph集群在运行，而你想看一个在运行进程的配置，用下面的命令：
ceph --admin-daemon {/path/to/admin/socket} config show | less
The default path for the admin socket for each daemon is:
各守护进程的管理套接字默认路径如下：
/var/run/ceph/$cluster-$name.asok
At real time, the metavariables will evaluate to the actual cluster name and daemon name. For example, if the cluster name is ceph (it is by default) and you want to retrieve the configuration for osd.0, use the following:
同时，元变量将展开为实际的集群名和进程名，例如如果集群名是ceph（默认值），你可以用下面的命令检索osd.0的配置：
ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | less
 3.1.2.13  运行多个集群
Running Multiple Clusters
With Ceph, you can run multiple clusters on the same hardware. Running multiple clusters provides a higher level of isolation compared to using different pools on the same cluster with different CRUSH rulesets. A separate cluster will have separate monitor, OSD and metadata server processes. When running Ceph with default settings, the default cluster name is ceph, which means you would save your Ceph configuration file with the file name ceph.conf in the /etc/ceph default directory.
用ceph可以实现在同一套硬件上运行多个集群，运行多个集群和在同一个集群上使用CRUSH规则控制多个存储池相比提供了更高水平的隔离。独立的集群需要独立的监视器、OSD和元数据服务器进程。默认配置下集群名字是ceph，这意味着你得把配置文件保存为/etc/ceph下的ceph.conf。
When you run multiple clusters, you must name your cluster and save the Ceph configuration file with the name of the cluster. For example, a cluster named openstack will have a Ceph configuration file with the file name openstack.conf in the /etc/ceph default directory.
运行多个集群时，你必须为集群命名并用这个名字保存配置文件。例如，名为openstack的集群其配置文件将是/etc/ceph下的openstack.conf。
Important: Cluster names must consist of letters a-z and digits 0-9 only.
重要：集群名字里只能包含字母a-z和数字0-9。
Separate clusters imply separate data disks and journals, which are not shared between clusters. Referring to Metavariables, the $cluster metavariable evaluates to the cluster name (i.e., openstack in the foregoing example). Various settings use the $cluster metavariable, including:
独立的集群意味着独立数据盘和日志，它们不能在集群间共享。根据元变量，$cluster元变量对应集群名字（前例为openstack）。多处设置都用到$cluster元变量，包括：
keyring
admin socket
log file
pid file
mon data
mon cluster log file
osd data
osd journal
mds data
rgw data
See General Settings, OSD Settings, Monitor Settings, MDS Settings, RGW Settings and Log Settings for relevant path defaults that use the $cluster metavariable.
和$cluster元变量相关的默认路径见常规配置、OSD选项、监视器选项、mds配置参考、<6.3>、错误：引用源未找到。
When deploying the Ceph configuration file, ensure that you use the cluster name in your command line syntax. For example:
分发配置文件时，确保在命令行下用的是集群名，例如：
ssh myserver01 sudo tee /etc/ceph/openstack.conf < /etc/ceph/openstack.conf
When creating default directories or files, you should also use the cluster name at the appropriate places in the path. For example:
创建默认目录和文件时，也要代入集群名。例如：
sudo mkdir /var/lib/ceph/osd/openstack-0
sudo mkdir /var/lib/ceph/mon/openstack-a

Important: When running monitors on the same host, you should use different ports. By default, monitors use port 6789. If you already have monitors using port 6789, use a different port for your other cluster(s).
重要：在一台主机上运行多个监视器时，你得指定不同端口。监视器默认使用6789端口，如果它已经被占，其它集群得另外指定端口。
To invoke a cluster other than the default ceph cluster, use the --cluster=clustername option with the ceph command. For example:
要调动一个非默认ceph的集群，要给ceph命令加--cluster=clustername选项，例如：
ceph --cluster=openstack health
 3.1.3  网络配置
Network Configuration Reference
Network configuration is critical for building a high performance Ceph cluster. The Ceph cluster does not perform request routing or dispatching on behalf of the client. Instead, Ceph clients (i.e., block device, CephFS, REST gateway) make requests directly to OSDs. Ceph OSDs perform data replication on behalf of clients, which means replication and other factors impose additional loads on Ceph cluster networks.
网络配置对构建高性能集群来说相当重要。ceph集群不会代表客户端执行请求路由或调度，相反，ceph客户端（如块设备、CephFS、REST网关）直接向OSD请求，然后OSD为客户端执行数据复制，也就是说复制和其它因素会额外增加集群网的负载。
Our 5-minute Quick Start provides a trivial Ceph configuration file that sets monitor IP addresses and daemon host names only. The quick start configuration assumes a single “public” network. Ceph functions just fine with a public network only, but you may see significant performance improvement with a second “cluster” network in a large cluster.
我们的5分钟快速入门提供了一个简陋的ceph配置文件，其中只设置了监视器IP地址和守护进程所在的主机名，因为快速入门假设只有一个公共网。只用一个网可以运行ceph，但是在大型集群里用单独的集群网可显著地提升性能。
We recommend running a Ceph cluster with two networks: a public (front-side) network and a cluster (back-side) network. To support two networks, your hosts need to have more than one NIC. See Hardware Recommendations - Networks for additional details.
我们推荐用两个网络运行ceph：一个公共网（前端）和一个集群网（后端）。为此，主机得配备多个网卡，见网络。

There are several reasons to consider operating two separate networks:
运营两个独立网络的考量主要有：
1. Peformance: OSDs handle data replication for the clients. When OSDs replicate data more than once, the network load between OSDs easily dwarfs the network load between clients and the Ceph cluster. This can introduce latency and create a performance problem. Recovery and rebalancing can also introduce significant latency on the public network. See How Ceph Scales for additional details on how Ceph replicates data. See Monitor / OSD Interaction for details on heartbeat traffic.
性能：OSD为客户端处理数据复制，复制多份时OSD间的网络负载势必会影响到客户端和ceph集群的通讯，包括延时增加、产生性能问题；恢复和重均衡也会显著增加公共网延时。关于ceph如何复制参见ceph如何伸缩；关于心跳流量参见心跳配置 （监视器/OSD交互的配置）。
2. Security: While most people are generally civil, a very tiny segment of the population likes to engage in what’s known as a Denial of Service (DoS) attack. When traffic between OSDs gets disrupted, placement groups may no longer reflect an active + clean state, which may prevent users from reading and writing data. A great way to defeat this type of attack is to maintain a completely separate cluster network that doesn’t connect directly to the internet. Also, consider using Message Signatures to defeat spoofing attacks.
安全：大多数人都是良民，很少的一撮人喜欢折腾拒绝服务攻击（DoS）。当OSD间的流量瓦解时，归置组再也不能达到active+clean状态，这样用户就不能读写数据了。挫败此类攻击的一种好方法是维护一个完全独立的集群网，使之不能直连互联网；另外，请考虑用签名防止欺骗攻击。
 3.1.3.1  防火墙iptables
IP Tables
By default, daemons bind to ports within the 6800:7100 range. You may configure this range at your discretion. Before configuring your IP tables, check the default iptables configuration.
守护进程默认会绑定到6800:7100间的端口，你可以更改此范围。配置防火墙前先检查下iptables配置：
sudo iptables -L
Some Linux distributions include rules that reject all inbound requests except SSH from all network interfaces. For example:
一些Linux发行版的规则拒绝除SSH之外的所有入栈连接，例如：
REJECT all -- anywhere anywhere reject-with icmp-host-prohibited
You will need to delete these rules on both your public and cluster networks initially, and replace them with appropriate rules when you are ready to harden the ports on your cluster hosts.
你得先删掉公共网和集群网对应的这些规则，然后再增加安全保护规则。
 3.1.3.1.1  监视器防火墙
Monitor IP Tables
Monitors listen on port 6789 by default. Additionally, monitors always operate on the public network. When you add the rule using the example below, make sure you replace {iface} with the public network interface (e.g., eth0, eth1, etc.), {ip-address} with the IP address of the public network and {netmask} with the netmask for the public network.
监视器默认监听6789端口，而且监视器总是运行在公共网。按下例增加规则时，要把{iface}替换为公共网接口（如eth0、eth1等等）、{ip-address}替换为公共网IP、{netmask}替换为公共网掩码。
sudo iptables -A INPUT -i {iface} -p tcp -s {ip-address}/{netmask} --dport 6789 -j ACCEPT
 3.1.3.1.2  MDS防火墙
MDS IP Tables
Metadata servers listen on the first available port on the public network beginning at port 6800. Ensure that you open one port beginning at port 6800 for each metadata server that runs on the host. When you add the rule using the example below, make sure you replace {iface} with the public network interface (e.g., eth0, eth1, etc.), {ip-address} with the IP address of the public network and {netmask} with the netmask of the public network.
元数据服务器会监听公共网6800以上的第一个可用端口，确保打开元数据服务器上6800以上的端口。按下例增加规则时，要把{iface}替换为公共网接口（如eth0、eth1等等）、{ip-address}替换为公共网IP、{netmask}替换为公共网掩码。
For example:
例如：
sudo iptables -A INPUT -i {iface} -m multiport -p tcp -s {ip-address}/{netmask} --dports 6800:6810 -j ACCEPT
 3.1.3.1.3  OSD防火墙
OSD IP Tables
By default, OSDs bind to the first available ports on a host beginning at port 6800. Ensure that you open at least three ports beginning at port 6800 for each OSD that runs on the host. Each OSD on a host may use up to three ports:
OSD默认绑定6800以上的第一个可用端口，要确保各OSD至少打开了3个6800以上的端口。一主机上的各个OSD最多会用到3个端口：
1. One for talking to clients and monitors.
一个用于和客户端、监视器通讯；
2. One for sending data to other OSDs.
一个用于发送数据到其他OSD；
3. One for heartbeating.
一个用于心跳；

Ports are host-specific, so you don’t need to open any more ports than the number of ports needed by Ceph daemons running on that host. You may consider opening a few additional ports in case a daemon fails and restarts without letting go of the port such that the restarted daemon binds to a new port.
每台主机的端口不尽相同，所以你不需要开放多于ceph所需的端口。但是应该多开放一点，因为有时候某个守护进程失败了，但是重启前并未全部释放，以致于重启后的进程监听了新端口。
If you set up separate public and cluster networks, you must add rules for both the public network and the cluster network, because clients will connect using the public network and other OSDs will connect using the cluster network. When you add the rule using the example below, make sure you replace {iface} with the network interface (e.g., eth0, eth1, etc.), {ip-address} with the IP address and {netmask} with the netmask of the public or cluster network. For example:
如果你要分开公共网和集群网，必须分别为之设置防火墙，因为客户端会通过公共网连接，而其他OSD会通过集群网连接。按下例增加规则时，要把{iface}替换为公共网接口（如eth0、eth1等等）、{ip-address}替换为公共网IP、{netmask}替换为公共网掩码。例如：
sudo iptables -A INPUT -i {iface}  -m multiport -p tcp -s {ip-address}/{netmask} --dports 6800:6810 -j ACCEPT

Tip: If you run metadata servers on the same host as the OSDs, you can consolidate the public network configuration step. Ensure that you open the number of ports required for each daemon per host.
提示：如果你的元数据服务器和OSD在同一台主机上，可以合并公共网配置，要确保开放了足够用的端口。
 3.1.3.2  ceph网络
Ceph Networks
To configure Ceph networks, you must add a network configuration to the [global] section of the configuration file. Our 5-minute Quick Start provides a trivial Ceph configuration file that assumes one public network with client and server on the same network and subnet. Ceph functions just fine with a public network only. However, Ceph allows you to establish much more specific criteria, including multiple IP network and subnet masks for your public network. You can also establish a separate cluster network to handle OSD heartbeat, object replication and recovery traffic. Don’t confuse the IP addresses you set in your configuration with the public-facing IP addresses network clients may use to access your service. Typical internal IP networks are often 192.168.0.0 or 10.0.0.0.
ceph的网络配置要放到[global]段下。前述的5分钟快速入门提供了一个简陋的ceph配置文件，它假设服务器和客户端都位于同一个网段，ceph可以很好地适应这种情形。然而ceph允许配置更精细的公共网，包括多IP和多掩码；也能用单独的网络处理OSD心跳、对象复制、和恢复流量。不要混淆你配置的IP地址和客户端用来访问存储服务的公共网地址。典型的内网常常是192.168.0.0和10.0.0.0。
Tip: If you specify more than one IP address and subnet mask for either the public or the cluster network, the subnets within the network must be capable of routing to each other. Additionally, make sure you include each IP address/subnet in your IP tables and open ports for them as necessary.
提示：如果你给公共网或集群网配置了多个IP地址及子网掩码，这些子网必须能互通。另外要确保为各IP/子网开放了必要的端口。

Note: Ceph uses CIDR notation for subnets (e.g., 10.0.0.0/24).
注意：ceph用CIDR法表示子网，如10.0.0.0/24。
When you’ve configured your networks, you may restart your cluster or restart each daemon. Ceph daemons bind dynamically, so you do not have to restart the entire cluster at once if you change your network configuration.
配置好几个网络后，可以重启集群或挨个重启守护进程。ceph守护进程动态地绑定端口，所以更改网络配置后无需一次性重启整个集群。
 3.1.3.2.1  公共网
Public Network
To configure a public network, add the following option to the [global] section of your Ceph configuration file.
要配置一个公共网，把下列选项加到配置文件的[global]段下。
[global]
        ...
        public network = {public-network/netmask}
 3.1.3.2.2  集群网
Cluster Network
If you declare a cluster network, OSDs will route heartbeat, object replication and recovery traffic over the cluster network. This may improve performance compared to using a single network. To configure a cluster network, add the following option to the [global] section of your Ceph configuration file.
如果你声明了集群网，OSD将把心跳、对象复制和恢复流量路由到集群网，与单个网络相比这会提升性能。要配置集群网，把下列选项加进配置文件的[global]段。
[global]
        ...
        cluster network = {cluster-network/netmask}
We prefer that the cluster network is NOT reachable from the public network or the Internet for added security.
为安全起见，从公共网或互联网到集群网应该是不可达的。
 3.1.3.3  ceph守护进程
Ceph Daemons
Ceph has one network configuration requirement that applies to all daemons: the Ceph configuration file MUST specify the host for each daemon. Ceph also requires that a Ceph configuration file specify the monitor IP address and its port.
有一个网络配置是所有守护进程都要配的：各个守护进程都必须指定host，ceph也要求指定监视器IP地址及端口。
Important: Some deployment tools (e.g., ceph-deploy, Chef) may create a configuration file for you. DO NOT set these values if the deployment tool does it for you.
重要：一些部署工具（如ceph-deploy、chef）会给你创建配置文件，如果它能胜任那就别设置这些值。

Tip: The host setting is the short name of the host (i.e., not an fqdn). It is NOT an IP address either. Enter hostname -s on the command line to retrieve the name of the host.
提示：host选项是主机的短名，不是全资域名FQDN，也不是IP地址。在命令行下输入hostname -s获取主机名。

[mon.a]
        host = {hostname}
        mon addr = {ip-address}:6789

[osd.0]
        host = {hostname}
You do not have to set the host IP address for a daemon. If you have a static IP configuration and both public and cluster networks running, the Ceph configuration file may specify the IP address of the host for each daemon. To set a static IP address for a daemon, the following option(s) should appear in the daemon instance sections of your ceph.conf file.
并非一定要给守护进程设置IP地址。如果你有一个静态配置，且分离了公共网和集群网，ceph允许你在配置文件里指定主机的IP地址。要给守护进程设置静态IP，可把下列选项加到ceph.conf。
[osd.0]
        public addr = {host-public-ip-address}
        cluster addr = {host-cluster-ip-address}

One NIC OSD in a Two Network Cluster
单网卡OSD、双网络集群
Generally, we do not recommend deploying an OSD host with a single NIC in a cluster with two networks. However, you may accomplish this by forcing the OSD host to operate on the public network by adding a public addr entry to the [osd.n] section of the Ceph configuration file, where n refers to the number of the OSD with one NIC. Additionally, the public network and cluster network must be able to route traffic to each other, which we don’t recommend for security reasons.
一般来说，我们不建议用单网卡OSD主机部署两个网络。然而这事可以实现，把public addr选项配在[osd.n]段下即可强制OSD主机运行在公共网，其中n是其OSD号。另外，公共网和集群网必须互通，考虑到安全因素我们不建议这样做。
 3.1.3.4  网络配置选项
Network Config Settings
Network configuration settings are not required. Ceph assumes a public network with all hosts operating on it unless you specifically configure a cluster network.
网络配置选项不是必需的，ceph假设所有主机都运行于公共网，除非你特意配置了一个集群网。
 3.1.3.4.1  公共网
Public Network
The public network configuration allows you specifically define IP addresses and subnets for the public network. You may specifically assign static IP addresses or override public network settings using the public addr setting for a specific daemon.
公共网配置允许你明确地为公共网定义IP地址和子网。你可以明确分配静态IP或用public addr覆盖public network选项。
public network
Description:
The IP address and netmask of the public (front-side) network (e.g., 192.168.0.0/24). Set in [global]. You may specify comma-delimited subnets.
公共网（前端）的IP地址和掩码（如192.168.0.0/24）。用于[global]下。多个子网用逗号分隔。
Type:
{ip-address}/{netmask} [, {ip-address}/{netmask}]
Required:
No
Default:
N/A
public addr
Description:
The IP address for the public (front-side) network. Set for each daemon.
用于公共网（前端）的IP地址。适用于各守护进程。
Type:
IP Address
Required:
No
Default:
N/A
 3.1.3.4.2  集群网
Cluster Network
The cluster network configuration allows you to declare a cluster network, and specifically define IP addresses and subnets for the cluster network. You may specifically assign static IP addresses or override cluster network settings using the cluster addr setting for specific OSD daemons.
集群网配置允许你声明一个集群网，并明确地定义IP地址和子网。你可以配置静态IP或为某OSD守护进程配置cluster addr以覆盖cluster network选项。
cluster network
Description:
The IP address and netmask of the cluster (back-side) network (e.g., 10.0.0.0/24). Set in [global]. You may specify comma-delimited subnets.
集群网（后端）的IP地址及掩码（如10.0.0.0/24）。置于[global]下。多个子网用逗号分隔。
Type:
{ip-address}/{netmask} [, {ip-address}/{netmask}]
Required:
No
Default:
N/A
cluster addr
Description:
The IP address for the cluster (back-side) network. Set for each daemon.
集群网（后端）IP地址。置于各守护进程下。
Type:
Address
Required:
No
Default:
N/A
 3.1.3.4.3  端口绑定
Bind
Bind settings set the default port ranges Ceph OSD and MDS daemons use. The default range is 6800:7100. Ensure that your IP Tables configuration allows you to use the configured port range.
绑定选项设置OSD和MDS使用的默认端口范围，默认范围是6800:7100。确保防火墙开放了对应端口范围。
You may also enable Ceph daemons to bind to IPv6 addresses.
你也可以让ceph守护进程绑定到IPv6地址。
ms bind port min
Description:
The minimum port number to which an OSD or MDS daemon will bind.
OSD或MDS可绑定的最小端口号。
Type:
32-bit Integer
Default:
6800
Required:
No
ms bind port max
Description:
The maximum port number to which an OSD or MDS daemon will bind.
OSD或MDS可绑定的最大端口号。
Type:
32-bit Integer
Default:
7100
Required:
No.
ms bind ipv6
Description:
Enables Ceph daemons to bind to IPv6 addresses.
允许ceph绑定IPv6地址。
Type:
Boolean
Default:
false
Required:
No
 3.1.3.4.4  主机
Hosts
Ceph expects at least one monitor declared in the Ceph configuration file, with a mon addr setting under each declared monitor. Ceph expects a host setting under each declared monitor, metadata server and OSD in the Ceph configuration file.
ceph需要至少一个监视器，声明的每个监视器下都要加mon addr选项；每个监视器、元数据服务器和OSD下都要配host选项。
mon addr
Description:
A list of {hostname}:{port} entries that clients can use to connect to a Ceph monitor. If not set, Ceph searches [mon.*] sections.
{hostname}:{port}条目列表，用以让客户端连接ceph监视器。如果未设置，ceph查找[mon.*]段。
Type:
String
Required:
No
Default:
N/A
host
Description:
The hostname. Use this setting for specific daemon instances (e.g., [osd.0]).
主机名。此选项用于特定守护进程，如[osd.0]。
Type:
String
Required:
Yes, for daemon instances.
Default:
localhost

Tip: Do not use localhost. To get your host name, execute hostname -s on your command line and use the name of your host (to the first period, not the fully-qualified domain name).
提示：不要用localhost。在命令行下执行hostname -s获取主机名（到第一个点，不是全资域名），并用于配置文件。

Important: You should not specify any value for host when using a third party deployment system that retrieves the host name for you.
重要：用第三方部署工具时不要指定host选项，它会自行获取。
 3.1.3.4.5  TCP
Ceph disables TCP buffering by default.
ceph默认禁用TCP缓冲。
tcp nodelay
Description:
Ceph enables tcp nodelay so that each request is sent immediately (no buffering). Disabling Nagle’s algorithm increases network traffic, which can introduce latency. If you experience large numbers of small packets, you may try disabling tcp nodelay.
ceph用tcp nodelay使系统尽快（不缓冲）发送每个请求。禁用 Nagle算法可增加吞吐量，但会引进延时。如果你遇到大量小包，可以禁用tcp nodelay试试。
Type:
Boolean
Required:
No
Default:
true
tcp rcvbuf
Description:
The size of the socket buffer on the receiving end of a network connection. Disable by default.
网络套接字接收缓冲尺寸，默认禁用。
Type:
32-bit Integer
Required:
No
Default:
0
ms tcp read timeout
Description:
If a client or daemon makes a request to another Ceph daemon and does not drop an unused connection, the tcp read timeout defines the connection as idle after the specified number of seconds.
如果一客户端或守护进程请求连接到另一个ceph守护进程，且没有断开不再使用的连接，在tcp read timeout指定的秒数之后它将被标记为空闲。
Type:
Unsigned 64-bit Integer
Required:
No
Default:
900 15 minutes.
 3.1.4  认证配置（cephx配置）
Cephx Config Reference
To protect against man-in-the-middle attacks, Ceph provides its cephx authentication system to authenticate users and daemons. See Ceph Authentication & Authorization for an introduction to cephx authentication. See the Cephx Guide for details on enabling/disabling, creating users and setting user capabilities.
为防止中间人攻击，ceph用cephx认证系统来认证用户和守护进程。关于cephx认证的介绍见Ceph Authentication & Authorization；关于如何启用/禁用、创建用户、设置用户能力见cephx认证。
 3.1.4.1  启用/禁用认证
Enable/Disable Authentication
Depending on the version, Ceph either enables or disables authentication by default. Use the following settings to expressly enable or disable Ceph. See Ceph Authentication for additional details.
在不同版本中，ceph默认启用或禁用了认证。下列选项可明确启用或禁用认证，详情见ceph认证（cephx）。
Authentication Enablement Defaults
认证启用默认值
Ceph version 0.54 and earlier versions disable authentication by default. If you want to use Ceph authentication, you must specifically enable it for version 0.54 and earlier versions.
Ceph version 0.55 and later version enable authentication by default. If you do not want to use Ceph authentication, you must specifically disable it for versions 0.55 and later versions.
ceph 0.54及其之前版本默认禁用认证，如果你想用认证，必须明确启用。
ceph 0.55及其之后版本默认开启认证，如果你不想用认证，必须明确禁用。

Authentication Granularity
认证粒度
Ceph version 0.50 and earlier versions use auth supported to enable or disable authentication between the Ceph client and the cluster. Ceph authentication in earlier versions only authenticates users sending message traffic between the client and the cluster, so it does not have fine-grained control.
ceph 0.50及之前版本用auth supported来打开或关闭ceph客户端和集群间的认证，而且早期版本只认证了客户端和集群间的部分，所以它不能精细控制。
Ceph version 0.51 and later versions use fine-grained control, which allows you to require authentication of the client by the cluster (auth service required), authentication of the cluster by the client (auth client required), and authentication of a daemon within the cluster by another daemon within the cluster (auth cluster required).
ceph 0.51及其后续版本使用更精细的控制，它允许集群要求客户端提供认证信息（auth service reqiured）、客户端认证集群（auth client required）、集群内的守护进程相互认证（auth cluster required）。
auth supported
Deprecated since version 0.51.
从0.51开始废弃。
Description:
Indicates whether to use authentication. If not specified, it defaults to none, which means it is disabled.
指示是否使用认证。如果未配置，就是none，意思是禁用了。
Type:
String
Required:
No
Default:
none
auth cluster required
New in version 0.51.
0.51新增。
Description:
If enabled, the cluster daemons (i.e., ceph-mon, ceph-osd, and ceph-mds) must authenticate with each other. Valid setting is cephx or none.
如果启用了，集群守护进程（如ceph-mon、ceph-osd和ceph-mds）间必须相互认证。可用选项有cephx或none。
Type:
String
Required:
No
Default:
Version 0.54 and earlier none. Version 0.55 and later cephx.
0.54及之前为none，0.55及之后为cephx。
auth service required
New in version 0.51.
0.51新增。
Description:
If enabled, the cluster daemons require Ceph clients to authenticate with the cluster in order to access Ceph services. Valid setting is cephx or none.
如果启用，客户端要访问ceph服务的话，集群守护进程会要求它和集群认证。可用选项为cephx或none。
Type:
String
Required:
No
Default:
Version 0.54 and earlier none. Version 0.55 and later cephx.
0.54及之前为none，0.55及之后为cephx。
auth client required
New in version 0.51.
0.51新增。
Description:
If enabled, the client requires the Ceph cluster to authenticate with the client. Valid setting is cephx or none.
如果启用，客户端会要求ceph集群和它认证。可用选项为cephx或none。
Type:
String
Required:
No
Default:
Version 0.54 and earlier none. Version 0.55 and later cephx.
0.54及之前为none，0.55及之后为cephx。
 3.1.4.2  密钥
Keys
When you run Ceph with authentication enabled, ceph administrative commands and Ceph clients require authentication keys to access the cluster.
如果你的集群启用了认证，ceph管理命令和客户端得有密钥才能访问集群。
The most common way to provide these keys to the ceph administrative commands and clients is to include a Ceph keyring under the /etc/ceph directory. The filename is usually ceph.keyring (or $cluster.keyring) or simply keyring. If you include the keyring under the /etc/ceph directory, you don’t need to specify a keyring entry in your Ceph configuration file.
给ceph管理命令和客户端提供这些密钥的最常见方法就是把密钥环放到/etc/ceph下，文件名通常是ceph.keyring（或$cluster.keyring）或仅是keyring。如果你的密钥环位于/etc/ceph下，就不需要在ceph配置文件里指定keyring选项了。
We recommend copying the cluster’s keyring file to hosts where you’ll run administrative commands, because it contains the client.admin key.
我们建议把集群的密钥环拷贝到你执行管理命令的主机，它包含client.admin密钥：
sudo scp {user}@{ceph-cluster-host}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring

Tip: Ensure the ceph.keyring file has appropriate permissions set (e.g., chmod 644) on your client machine.
提示：确保给客户端上的ceph.keyring设置合理的权限位（如chmod 644）。
You may specify the key itself in the Ceph configuration file using the key setting (not recommended), or a path to a keyfile using the keyfile setting.
你可以用key选项把密钥写在配置文件里（别这样），或者用keyfile选项指定个路径。
keyring
Description:
The path to the keyring file.
到密钥环文件的路径。
Type:
String
Required:
No
Default:
/etc/ceph/$cluster.$name.keyring,/etc/ceph/$cluster.keyring, /etc/ceph/keyring,/etc/ceph/keyring.bin
keyfile
Description:
The path to a key file (i.e,. a file containing only the key).
到密钥文件的路径，如一个只包含密钥的文件。
Type:
String
Required:
No
Default:
None
key
Description:
The key (i.e., the text string of the key itself). Not recommended.
密钥（密钥文本），最好别这样做。
Type:
String
Required:
No
Default:
None
 3.1.4.3  签名
Signatures
In Ceph Bobtail and subsequent versions, we prefer that Ceph authenticate all ongoing messages between the entities using the session key set up for that initial authentication. However, Argonaut and earlier Ceph daemons do not know how to perform ongoing message authentication. To maintain backward compatibility (e.g., running both Botbail and Argonaut daemons in the same cluster), message signing is off by default. If you are running Bobtail or later daemons exclusively, configure Ceph to require signatures.
在bobtail及后续版本，ceph会用开始认证时生成的会话密钥认证所有在线实体。然而Argonaut及之前版本不知道如何认证在线消息，为保持向后兼容性（如在同一个集群里运行bobtail和argonaut），消息签名默认是关闭的。如果你只运行bobtail和后续版本，可以让ceph要求签名。
Like other parts of Ceph authentication, Ceph provides fine-grained control so you can enable/disable signatures for service messages between the client and Ceph, and you can enable/disable signatures for messages between Ceph daemons.
像ceph认证的其他部分一样，客户端和集群间的消息签名也能做到细粒度控制；而且能启用或禁用ceph守护进程间的签名。
ceph require signatures
Description:
If set to true, Ceph requires signatures on all message traffic between the client and the Ceph cluster, and between daemons within the cluster.
若设置为true，ceph集群会要求客户端签名所有消息，包括客户端和集群内其他守护进程间的。
Type:
Boolean
Required:
No
Default:
false
cepxh cluster require signatures
Description:
If set to true, Ceph requires signatures on all message traffic between Ceph daemons within the cluster.
若设置为true，ceph要求集群内所有守护进程签名相互之间的消息。
Type:
Boolean
Required:
No
Default:
false
cepxh service require signatures
Description:
If set to true, Ceph requires signatures on all message traffic between Ceph clients and the Ceph cluster.
若设置为true，ceph要求签名所有客户端和集群间的消息。
Type:
Boolean
Required:
No
Default:
false
cephx sign messages
Description:
If the Ceph version supports message signing, Ceph will sign all messages so they cannot be spoofed.
如果ceph版本支持消息签名，ceph会签名所有消息以防欺骗。
Type:
Boolean
Default:
true
 3.1.4.4  生存期
Time to Live
auth service ticket ttl
Description:
When Ceph sends a client a ticket for authentication, the Ceph cluster assigns the ticket a time to live.
ceph发给客户端一个用于认证的票据时分配给这个票据的生存期。
Type:
Double
Default:
60*60
 3.1.5  监视器配置
Monitor Config Reference
Understanding how to configure a Ceph monitor is an important part of building a reliable Ceph cluster. All Ceph clusters have at least one monitor. A monitor configuration usually remains fairly consistent, but you can add, remove or replace a monitor in a cluster. See Adding/Removing a Monitor for details.
理解如何配置ceph监视器是构建可靠集群的重要方面，任何ceph集群都需要至少一个监视器。一个监视器通常相当一致，但是你可以增加、删除、或替换集群中的监视器，详情见Adding/Removing a Monitor。
 3.1.5.1  背景
Background
Monitors maintain a “master copy” of the cluster map, which means a client can determine the location of all monitors, OSDs, and metadata servers just by connecting to one monitor and retrieving a current cluster map. Before Ceph clients can read from or write to OSDs or metadata servers, they must connect to a monitor first. With a current copy of the cluster map and the CRUSH algorithm, a client can compute the location for any object. The ability to compute object locations allows a client to talk directly to OSDs, which is a very important aspect of Ceph’s high scalability and performance.
监视器们维护着集群运行图的主副本，就是说客户端连到一个监视器并获取当前运行图就能确定所有监视器、OSD和元数据服务器的位置。ceph客户端读写OSD或元数据服务器前，必须先连到一个监视器，靠当前集群运行图的副本和CRUSH算法，客户端能计算出任何对象的位置，故此客户端有能力直接连到OSD，这对ceph的高伸缩性、高性能来说非常重要。
The primary role of the monitor is to maintain a master copy of the cluster map. Monitors also provide authentication and logging services. Ceph monitors write all changes in the monitor services to a single Paxos instance, and Paxos writes the changes to a key/value store for strong consistency. Ceph monitors can query the most recent version of the cluster map during sync operations. Ceph monitors leverage the key/value store’s snapshots and iterators (using leveldb) to perform store-wide synchronization.
监视器的主要角色是维护集群运行图的主副本，它也提供认证和日志记录服务。ceph监视器们把监视器服务的所有更改写入一个单独的Paxos例程，然后Paxos以键/值方式存储所有变更以实现高度一致性。同步期间，ceph监视器能查询集群运行图的最新版本，它们通过操作键/值存储快照和迭代器（用leveldb）来进行存储级同步。

Deprecated since version version: 0.58
从0.58开始废弃。
In Ceph versions 0.58 and earlier, Ceph monitors use a Paxos instance for each service and store the map as a file.
在0.58及更早版本中，ceph监视器每个服务用一个Paxos例程，并把运行图存储为文件。
 3.1.5.1.1  集群运行图
Cluster Maps
The cluster map is a composite of maps, including the monitor map, the OSD map, the placement group map and the metadata server map. The cluster map tracks a number of important things: which processes are in the cluster; which processes that are in the cluster are up and running or down; whether, the placement groups are active or inactive, and clean or in some other state; and, other details that reflect the current state of the cluster such as the total amount of storage space, and the amount of storage used.
集群运行图是多个图的组合，包括监视器图、OSD图、归置组图和元数据服务器图。集群运行图追踪几个重要事件：哪些进程在集群里（in）；哪些进程在集群里（in）是up且在运行、或down；归置组状态是active或inactive、clean或其他状态；和其他反映当前集群状态的信息，像总存储容量、和使用量。
When there is a significant change in the state of the cluster–e.g., an OSD goes down, a placement group falls into a degraded state, etc.–the cluster map gets updated to reflect the current state of the cluster. Additionally, the monitor also maintains a history of the prior states of the cluster. The monitor map, OSD map, placement group map and metadata server map each maintain a history of their map versions. We call each version an “epoch.”
当集群状态有明显变更时，如一OSD挂了、一归置组降级了等等，集群运行图会更新并反应集群当前状态。另外，监视器也维护着集群的主要状态历史。监视器图、OSD图、归置组图和元数据服务器图各自维护着它们的运行图版本。我们把各版本称为一个epoch。
When operating your cluster, keeping track of these states is an important part of your system administration duties. See Monitoring a Cluster and Monitoring OSDs and PGs for details.
运营集群时，跟踪这些状态是系统管理任务的重要部分。详情见Monitoring a Cluster and Monitoring OSDs and PGs。
 3.1.5.1.2  监视器法定人数
Monitor Quorum
Our 5-minute Quick Start provides a trivial Ceph configuration file that provides for one monitor in the test cluster. A cluster will run fine with a single monitor; however, a single monitor is a single-point-of-failure. To ensure high availability in a production cluster, you should run Ceph with multiple monitors so that the failure of a single monitor WILL NOT bring down your entire cluster.
前述的5分钟快速入门提供了一个简陋的ceph配置文件，它提供了一个监视器用于测试。只用一个监视器集群可以良好地运行，然而单监视器是一个单故障点，生产集群要实现高可用性的话得配置多个监视器，这样单个监视器的失效才不会影响整个集群。
When a cluster runs multiple monitors for high availability, Ceph monitors use Paxos to establish consensus about the master cluster map. A consensus requires a majority of monitors running to establish a quorum for consensus about the cluster map (e.g., 1; 2 out of 3; 3 out of 5; 4 out of 6; etc.).
集群用多个监视器实现高可用性时，多个监视器用Paxos算法对主集群运行图达成一致，这里的一致要求大多数监视器都在运行且够成法定人数（如1个、3之2在运行、5之3、6之4等等）。
 3.1.5.1.3  一致性
Consistency
When you add monitor settings to your Ceph configuration file, you need to be aware of some of the architectural aspects of Ceph monitors. Ceph imposes strict consistency requirements for a Ceph monitor when discovering another Ceph monitor within the cluster. Whereas, Ceph clients and other Ceph daemons use the Ceph configuration file to discover monitors, monitors discover each other using the monitor map (monmap), not the Ceph configuration file.
你把监视器加进ceph配置文件时，得注意一些架构问题，ceph发现集群内的其他监视器时对其有着严格的一致性要求。尽管如此，ceph客户端和其他ceph守护进程用配置文件发现监视器，监视器却用监视器图（monmap）相互发现而非配置文件。
A monitor always refers to the local copy of the monmap when discovering other monitors in the cluster. Using the monmap instead of the Ceph configuration file avoids errors that could break the cluster (e.g., typos in ceph.conf when specifying a monitor address or port). Since monitors use monmaps for discovery and they share monmaps with clients and other Ceph daemons, the monmap provides monitors with a strict guarantee that their consensus is valid.
一个监视器发现集群内的其他监视器时总是参考monmap的本地副本，用monmap而非ceph配置文件避免了可能损坏集群的错误（如ceph.conf中指定地址或端口的拼写错误）。正因为监视器把monmap用于发现、并共享于客户端和其他ceph守护进程间，monmap可严格地保证监视器的一致性是可靠的。
Strict consistency also applies to updates to the monmap. As with any other updates on the monitor, changes to the monmap always run through a distributed consensus algorithm called Paxos. The monitors must agree on each update to the monmap, such as adding or removing a monitor, to ensure that each monitor in the quorum has the same version of the monmap. Updates to the monmap are incremental so that monitors have the latest agreed upon version, and a set of previous versions. Maintaining a history enables a monitor that has an older version of the monmap to catch up with the current state of the cluster.
严格的一致性也适用于monmap的更新，因为关于监视器的任何更新、关于monmap的变更都是通过称为Paxos的分布式一致性算法传递的。监视器必须都同意monmap的每次更新，以确保法定人数里的每个监视器monmap版本相同，如增加、删除一个监视器。monmap的更新是增量的，所以监视器们都有最近同意的版本，以及一系列之前版本。历史版本的存在允许一个落后的监视器跟上集群当前状态。
If monitors discovered each other through the Ceph configuration file instead of through the monmap, it would introduce additional risks because the Ceph configuration files aren’t updated and distributed automatically. Monitors might inadvertently use an older Ceph configuration file, fail to recognize a monitor, fall out of a quorum, or develop a situation where Paxos isn’t able to determine the current state of the system accurately.
如果监视器通过配置文件而非monmap相互发现，这会引进其他风险，因为ceph配置文件不是自动更新并分发的，监视器有可能不小心用了较老的配置文件，以致于不认识某监视器、放弃法定人数、或者产生一种Paxos不能确定当前系统状态的情形。
 3.1.5.1.4  初始监视器
Bootstrapping Monitors
In most configuration and deployment cases, tools that deploy Ceph may help bootstrap the monitors by generating a monitor map for you (e.g., mkcephfs, ceph-deploy, etc). A monitor requires four explicit settings:
在大多数配置和部署案例中，部署ceph的工具可以帮你生成一个监视器图来初始化监视器（如mkcephfs、ceph-deploy等等），一个监视器需要4个选项：
Filesystem ID: The fsid is the unique identifier for your object store. Since you can run multiple clusters on the same hardware, you must specify the unique ID of the object store when bootstrapping a monitor. Deployment tools usually do this for you (e.g., mkcephfs or ceph-deploy can call a tool like uuidgen), but you may specify the fsid manually too.
文件系统ID：fsid是对象存储的唯一标识符。你可以在一套硬件上运行多个集群，所以在初始化监视器时必须指定对象存储的唯一ID。部署工具通常可替你完成（如mkcephfs或ceph-deploy会调用类似uuidgen的程序），但是你也可以手动指定fsid。
Monitor ID: A monitor ID is a unique ID assigned to each monitor within the cluster. It is an alphanumeric value, and by convention the identifier usually follows an alphabetical increment (e.g., a, b, etc.). This can be set in a Ceph configuration file (e.g., [mon.a], [mon.b]``, etc.), by a deployment tool, or using the ceph commandline.
监视器ID：监视器ID是分配给集群内各监视器的唯一ID，它是一个字母数字组合，为方便起见，标识符通常以字母顺序结尾（如a、b等等），可以设置于ceph配置文件（如[mon.a]、[mon.b]等等）、部署工具、或ceph命令行工具。
Keys: The monitor must have secret keys. A deployment tool such as mkcephfs or ceph-deploy usually does this for you, but you may perform this step manually too. See Monitor Keyrings for details.
密钥：监视器必须有密钥。像mkcephfs或ceph-deploy这样的部署工具通常会自动生成，也可以手动完成。见 Monitor Keyrings。
For additional details on bootstrapping, see Bootstrapping a Monitor.
关于初始化的其他信息见Bootstrapping a Monitor。
 3.1.5.2  监视器的配置
Configuring Monitors
To apply configuration settings to the entire cluster, enter the configuration settings under [global]. To apply configuration settings to all monitors in your cluster, enter the configuration settings under [mon]. To apply configuration settings to specific monitors, specify the monitor instance (e.g., [mon.a]). By convention, monitor instance names use alpha notation.
要把配置应用到整个集群，把它们放到[global]下；要用于所有监视器，置于[mon]下；要用于某监视器，指定监视器例程，如[mon.a]）。按惯例，监视器例程用字母命名。
[global]

[mon]

[mon.a]

[mon.b]

[mon.c]
 3.1.5.2.1  最小配置
Minimum Configuration
The bare minimum monitor settings for a Ceph monitor via the Ceph configuration file include a hostname and a monitor address for each monitor. You can configure these under [mon] or under the entry for a specific monitor.
ceph监视器的最简配置必须包括一个主机名和监视器地址，这些配置可置于[mon]下或某个监视器下。
[mon]
        mon host = hostname1,hostname2,hostname3
        mon addr = 10.0.0.10:6789,10.0.0.11:6789,10.0.0.12:6789
[mon.a]
        host = hostname1
        mon addr = 10.0.0.10:6789
See the Network Configuration Reference for details.
详情参见Network Configuration Reference。
Note: This minimum configuration for monitors assumes that a deployment tool generates the fsid and the mon. key for you.
注意：这里的监视器最简配置假设部署工具会自动给你生成fsid和mon.密钥。
Once you deploy a Ceph cluster, you SHOULD NOT change the IP address of the monitors. However, if you decide to change the monitor’s IP address, you must follow a specific procedure. See Changing a Monitor’s IP Address for details.
一旦部署了ceph集群，监视器IP不应该更改。然而，如果你决意要改，必须严格按照Changing a Monitor’s IP Address来改。
 3.1.5.2.2  集群ID
Cluster ID
Each Ceph cluster has a unique identifier (fsid). If specified, it usually appears under the [global] section of the configuration file. Deployment tools usually generate the fsid and store it in the monitor map, so the value may not appear in a configuration file. The fsid makes it possible to run daemons for multiple clusters on the same hardware.
每个ceph集群都有一个唯一标识符（fsid）。如果要指定，它应该出现在配置文件的[global]段下。部署工具通常生成fsid并存于监视器图，所以不一定会出现在配置文件里，fsid使得在一套硬件上运行多个集群成为可能。
fsid
Description:
The cluster ID. One per cluster.
集群ID，一集群一个。
Type:
UUID
Required:
Yes.
Default:
N/A. May be generated by a deployment tool if not specified.
无。若未指定，部署工具会生成。

Note: Do not set this value if you use a deployment tool that does it for you.
注意：如果你用部署工具就没必要设置。
 3.1.5.2.3  初始成员
Initial Members
We recommend running a production cluster with at least three monitors to ensure high availability. When you run multiple monitors, you may specify the initial monitors that must be members of the cluster in order to establish a quorum. This may reduce the time it takes for your cluster to come online.
我们建议在生产环境下最少部署3个监视器，以确保高可用性。运行多个监视器时，你可以指定为形成法定人数成员所需的初始监视器，这能减小集群上线时间。
[mon]
        mon initial members = a,b,c
mon initial members
Description:
描述：
The IDs of initial monitors in a cluster during startup. If specified, Ceph requires an odd number of monitors to form an initial quorum (e.g., 3).
集群启动时初始监视器的ID，若指定，ceph需要奇数个监视器来确定最初法定人数（如3）。
Type:
String
Default:
None

Note: A majority of monitors in your cluster must be able to reach each other in order to establish a quorum. You can decrease the initial number of monitors to establish a quorum with this setting.
注意：集群内的大多数监视器必须能互通以建立法定人数，你可以用此选项减小初始监视器数量来形成。
 3.1.5.2.4  数据
Data
Ceph provides a default path where monitors store data. For optimal performance in a production cluster, we recommend running monitors on separate hosts and drives from OSDs. Monitors do lots of fsync(), which can interfere with OSD workloads.
ceph监视器有存储数据的默认路径，生产集群为实现更高性能可把监视器部署到单独的主机和OSD，因为监视器会频繁fsync()，这可能影响OSD。
In Ceph versions 0.58 and earlier, monitors store their data in files. This approach allows users to inspect monitor data with common tools like ls and cat. However, it doesn’t provide strong consistency.
在ceph 0.58及更早版本中，监视器数据以文件保存，这样人们可以用ls和cat这些普通工具检查监视器数据，然而它不能提供健壮的一致性。
In Ceph versions 0.59 and later, monitors store their data as key/value pairs. Monitors require ACID transactions. Using a data store prevents recovering monitors from running corrupted versions through Paxos, and it enables multiple modification operations in one single atomic batch, among other advantages.
在ceph 0.59及后续版本中，监视器以键/值对存储数据。监视器需要ACID事务，数据存储的使用可防止监视器用损坏的版本进行恢复，除此之外，它允许在一个原子批量操作中进行多个修改操作。
Generally, we do not recommend changing the default data location. If you modify the default location, we recommend that you make it uniform across monitors by setting it in the [mon] section of the configuration file.
一般来说我们不建议更改默认数据位置，如果要改，我们建议所有监视器统一配置，加到配置文件的[mon]下。
mon data
Description:
The monitor’s data location.
监视器的数据位置。
Type:
String
Default:
/var/lib/ceph/mon/$cluster-$id
 3.1.5.2.5  存储容量
Storage Capacity
When a Ceph cluster gets close to its maximum capacity (i.e., mon osd full ratio), Ceph prevents you from writing to or reading from OSDs as a safety measure to prevent data loss. Therefore, letting a production cluster approach its full ratio is not a good practice, because it sacrifices high availability. The default full ratio is .95, or 95% of capacity. This a very aggressive setting for a test cluster with a small number of OSDs.
ceph集群利用率接近最大容量时（如mon osd full ratio），作为防止数据丢失的安全措施，它会阻止你读写OSD。因此，让生产集群用满可不是好事，因为牺牲了高可用性。full ratio默认值是.95或容量的95%。对小型测试集群来说这是非常激进的设置。
Tip: When monitoring your cluster, be alert to warnings related to the nearfull ratio. This means that a failure of some OSDs could result in a temporary service disruption if one or more OSDs fails. Consider adding more OSDs to increase storage capacity.
提示：监控集群时，要警惕和nearfull相关的警告。这意味着一些OSD的失败会导致临时服务中断，应该增加一些OSD来扩展存储容量。
A common scenario for test clusters involves a system administrator removing an OSD from the cluster to watch the cluster rebalance; then, removing another OSD, and so on until the cluster eventually reaches the full ratio and locks up. We recommend a bit of capacity planning even with a test cluster so that you can gauge how much spare capacity you will need to maintain for high availability. Ideally, you want to plan for a series of OSD failures where the cluster can recover to an active + clean state without replacing those OSDs immediately. You can run a cluster in an active + degraded state, but this is not ideal for normal operating conditions.
在测试集群的一个常见场景中，系统管理员从集群删除一个OSD接着观察重均衡；然后继续删除其他OSD，直到集群达到占满率并锁死。我们建议，即使在测试集群里也要规划一点空闲容量用于保证高可用性。理想情况下，要做好这样的预案：一系列OSD失败后，短时间内未更换它们仍能恢复到active+clean状态。你也可以在active+degraded状态运行集群，但对正常使用来说并不好。
The following diagram depicts a simplistic Ceph cluster containing 33 hosts with one OSD per host, each OSD having a 3TB capacity. So this exemplary cluster has a maximum actual capacity of 99TB. With a mon osd full ratio of 0.95, if the cluster falls to 5TB of remaining capacity, the cluster will not allow Ceph clients to read and write data. So its operating capacity is 95TB, not 99TB.
下图描述了一个简化的ceph集群，它包含33个主机、每主机一个OSD、每OSD 3TB容量，所以这个小白鼠集群有99TB的实际容量，其mon osd full ratio为.95。如果它只剩余5TB容量，集群就不允许客户端再读写数据，所以它的运行容量是95TB，而非99TB。

It is normal in such a cluster for one or two OSDs to fail. A less frequent but reasonable scenario involves a rack’s router or power supply failing, which brings down multiple OSDs simultaneously (e.g., OSDs 7-12). In such a scenario, you should still strive for a cluster that can remain operational and achieve an active + clean state–even if that means adding a few hosts with additional OSDs in short order. If your capacity utilization is too high, you may not lose data, but you could still sacrifice data availability while resolving an outage within a failure domain if capacity utilization of the cluster exceeds the full ratio. For this reason, we recommend at least some rough capacity planning.
在这样的集群里，坏一或两个OSD很平常；一种罕见但可能发生的情景是一个机架的路由器或电源挂了，这会导致多个OSD同时离线（如OSD 7-12），在这种情况下，你仍要力争保持集群可运行并达到active+clean状态，即使这意味着你得在短期内额外增加一些OSD及主机。如果集群利用率太高，在解决故障域期间也许不会丢数据，但很可能牺牲数据可用性，因为利用率超过了full ratio。故此，我们建议至少要粗略地规划下容量。
Identify two numbers for your cluster:
找出你集群的两个数字：
1. The number of OSDs.
OSD数量。
2. The total capacity of the cluster
集群总容量。
If you divide the total capacity of your cluster by the number of OSDs in your cluster, you will find the mean average capacity of an OSD within your cluster. Consider multiplying that number by the number of OSDs you expect will fail simultaneously during normal operations (a relatively small number). Finally multiply the capacity of the cluster by the full ratio to arrive at a maximum operating capacity; then, subtract the number of amount of data from the OSDs you expect to fail to arrive at a reasonable full ratio. Repeat the foregoing process with a higher number of OSD failures (e.g., a rack of OSDs) to arrive at a reasonable number for a near full ratio.
用集群里OSD总数除以集群总容量，就能得到OSD平均容量；如果按预计的OSD数乘以这个值所得的结果计算（偏小），实际应用时将出错；最后再用集群容量乘以占满率能得到最大运行容量，然后扣除预估的OSD失败率；用较高的失败率（如整机架的OSD）重复前述过程看是否接近占满率。
[global]
        mon osd full ratio = .80
        mon osd nearfull ratio = .70
mon osd full ratio
Description:
The percentage of disk space used before an OSD is considered full.
OSD硬盘使用率达到多少就认为它full。
Type:
Float
Default:
.95
mon osd nearfull ratio
Description:
The percentage of disk space used before an OSD is considered nearfull.
OSD硬盘使用率达到多少就认为它nearfull。
Type:
Float
Default:
.85

Tip: If some OSDs are nearfull, but others have plenty of capacity, you may have a problem with the CRUSH weight for the nearfull OSDs.
提示：如果一些OSD快满了，但其他的仍有足够空间，你可能配错CRUSH权重了。
 3.1.5.2.6  心跳
Heartbeat
Ceph monitors know about the cluster by requiring reports from each OSD, and by receiving reports from OSDs about the status of their neighboring OSDs. Ceph provides reasonable default settings for monitor/OSD interaction; however, you may modify them as needed. See Monitor/OSD Interaction for details.
ceph监视器要求各OSD向它报告、并接收OSD关于它们邻居的状态报告，以此来掌握集群。ceph提供了监视器和OSD交互的合理默认值，然而你可以按需修改，详情见监视器和OSD的交互。
 3.1.5.2.7  监视器存储同步
Monitor Store Synchronization
When you run a production cluster with multiple monitors (recommended), each monitor checks to see if a neighboring monitor has a more recent version of the cluster map (e.g., a map in a neighboring monitor with one or more epoch numbers higher than the most current epoch in the map of the instant monitor). Periodically, one monitor in the cluster may fall behind the other monitors to the point where it must leave the quorum, synchronize to retrieve the most current information about the cluster, and then rejoin the quorum. For the purposes of synchronization, monitors may assume one of three roles:
当你用多个监视器支撑一个生产集群时，各监视器都要检查邻居是否有集群运行图的最新版本（如，邻居监视器的图有一或多个epoch版本高于当前监视器的最高版epoch），过一段时间，集群里的某个监视器可能落后于其它监视器太多而不得不离开法定人数，然后同步到集群当前状态，并重回法定人数。为了同步，监视器可能承担三种中的一种角色：
1. Leader: The Leader is the first monitor to achieve the most recent Paxos version of the cluster map.
leader：它是实现最新Paxos版本的第一个监视器。
2. Provider: The Provider is a monitor that has the most recent version of the cluster map, but wasn’t the first to achieve the most recent version.
provider：有最新集群运行图的监视器，但不是第一个实现最新版。
3. Requester: A Requester is a monitor that has fallen behind the leader and must synchronize in order to retrieve the most recent information about the cluster before it can rejoin the quorum.
requester：落后于leader，重回法定人数前，必须同步以获取关于集群的最新信息。
These roles enable a leader to delegate synchronization duties to a provider, which prevents synchronization requests from overloading the leader–improving performance. In the following diagram, the requester has learned that it has fallen behind the other monitors. The requester asks the leader to synchronize, and the leader tells the requester to synchronize with a provider.
有了这些角色区分，leader就可以给provider委派同步任务，这会避免同步请求压垮leader、影响性能。在下面的图示中，requester已经知道它落后于其它监视器，然后向leader请求同步，leader让它去和provider同步。

Synchronization always occurs when a new monitor joins the cluster. During runtime operations, monitors may receive updates to the cluster map at different times. This means the leader and provider roles may migrate from one monitor to another. If this happens while synchronizing (e.g., a provider falls behind the leader), the provider can terminate synchronization with a requester.
新监视器加入集群时总要同步。在运行中，监视器会不定时收到集群运行图的更新，这就意味着leader和provider角色可能在监视器间漂移。如果这事发生在同步期间（如provider落后于leader），provider能终结和requester间的同步。
Once synchronization is complete, Ceph requires trimming across the cluster. Trimming requires that the placement groups are active + clean.
一旦同步完成，ceph需要修复整个集群，使归置组回到active+clean状态。
mon sync trim timeout
Description:

Type:
Double
Default:
30.0
mon sync heartbeat timeout
Description:

Type:
Double
Default:
30.0
mon sync heartbeat interval
Description:

Type:
Double
Default:
5.0
mon sync backoff timeout
Description:

Type:
Double
Default:
30.0
mon sync timeout
Description:

Type:
Double
Default:
30.0
mon sync max retries
Description:

Type:
Integer
Default:
5
mon sync max payload size
Description:
The maximum size for a sync payload.
同步载荷的最大尺寸。
Type:
32-bit Integer
Default:
1045676
mon accept timeout
Description:
Number of seconds the Leader will wait for the Requester(s) to accept a Paxos update. It is also used during the Paxos recovery phase for similar purposes.
leader等待peons接受PAXOS更新的时间，出于同样的目的此值也用于PAXOS恢复阶段。
Type:
Float
Default:
10.0
paxos propose interval
Description:
Gather updates for this time interval before proposing a map update.
提议更新之前收集本时间段的更新。
Type:
Double
Default:
1.0
paxos min wait
Description:
The minimum amount of time to gather updates after a period of inactivity.
经过一段不活跃时间后，收集更新的最小等待时间。
Type:
Double
Default:
0.05
paxos trim tolerance
Description:
The number of extra proposals tolerated before trimming.
修复前容忍的其他提议数量。
Type:
Integer
Default:
30
paxos trim disabled max versions
Description:
The maximimum number of version allowed to pass without trimming.
允许不修复就通过的最大版本数
Type:
Integer
Default:
100
mon lease
Description:
The length (in seconds) of the lease on the monitor’s versions.
监视器版本租期（秒）。
Type:
Float
Default:
5
mon lease renew interval
Description:
The interval (in seconds) for the Leader to renew the other monitor’s leases.
监视器leader（头领）刷新其他监视器租期的间隔。
Type:
Float
Default:
3
mon lease ack timeout
Description:
The number of seconds the Leader will wait for the Providers to acknowledge the lease extension.
leader在等到peons（随从）确认延长租期前等待的时间
Type:
Float
Default:
10.0
mon min osdmap epochs
Description:
Minimum number of OSD map epochs to keep at all times.
一直保存的OSD图元素最小数量。
Type:
32-bit Integer
Default:
500
mon max pgmap epochs
Description:
Maximum number of PG map epochs the monitor should keep.
监视器应该一直保存的PG图元素最大数量。
Type:
32-bit Integer
Default:
500
mon max log epochs
Description:
Maximum number of Log epochs the monitor should keep.
监视器应该保留的最大日志数量。
Type:
32-bit Integer
Default:
500
 3.1.5.2.8  Slurp（暴食？）
In Ceph version 0.58 and earlier, when a Paxos service drifts beyond a given number of versions, Ceph triggers the slurp mechanism, which establishes a connection with the quorum Leader and obtains every single version the Leader has for every service that has drifted. In Ceph versions 0.59 and later, slurp will not work, because there is a single Paxos instance for all services.
在ceph 0.58及之前版本中，当Paxos服务漂移出给定版本数时，就会触发slurp机制，它会和法定人数leader建立一个连接并获取leader拥有的每个版本，以同步每个漂移的的服务。在ceph 0.59及后续版本，slurp机制取消了，因为所有服务共享一个Paxos例程。
Deprecated since version 0.58.
从0.58过时。
paxos max join drift
Description:
The maximum Paxos iterations before we must first sync the monitor data stores.
在我们首次同步监视器数据存储前，Paxos迭代的最大数量。
Type:
Integer
Default:
10
mon slurp timeout
Description:
The number of seconds the monitor has to recover using slurp before the process is aborted and the monitor bootstraps.
监视器进程终止后、自举前，要等待多长时间才开始发出显式修复通告。
Type:
Double
Default:
10.0
mon slurp bytes
Description:
Limits the slurp messages to the specified number of bytes.
显式修复消息尺寸限制。
Type:
32-bit Integer
Default:
256 * 1024
 3.1.5.2.9  时钟
Clock
clock offset
Description:
How much to offset the system clock. See Clock.cc for details.
时钟可以漂移多少，详情见Clock.cc。
Type:
Double
Default:
0
Deprecated since version 0.58.
从0.58过时。
mon tick interval
Description:
A monitor’s tick interval in seconds.
监视器的心跳间隔，单位为秒。
Type:
32-bit Integer
Default:
5
mon clock drift allowed
Description:
The clock drift in seconds allowed between monitors.
监视器间允许的时钟漂移量
Type:
Float
Default:
.050
mon clock drift warn backoff
Description:
Exponential backoff for clock drift warnings
时钟偏移警告的退避指数
Type:
Float
Default:
5
mon timecheck interval
Description:
The time check interval (clock drift check) in seconds for the leader.
和leader的时间偏移检查（时钟漂移检查）。单位为秒。
Type:
Float
Default:
300.0
 3.1.5.2.10  客户端
Client
mon client hung interval
Description:
The client will try a new monitor every N seconds until it establishes a connection.
客户端每N秒尝试一个新监视器，直到它建立连接。
Type:
Double
Default:
3.0
mon client ping interval
Description:
The client will ping the monitor every N seconds.
客户端每N秒ping一次监视器。
Type:
Double
Default:
10.0
mon client max log entries per message
Description:
The maximum number of log entries a monitor will generate per client message.
某监视器为每客户端生成的最大日志条数。
Type:
Integer
Default:
1000
mon client bytes
Description:
The amount of client message data allowed in memory (in bytes).
内存中允许存留的客户端消息数量。
Type:
64-bit Integer Unsigned
Default:
100ul << 20
 3.1.5.3  杂项
Miscellaneous
mon max osd
Description:
The maximum number of OSDs allowed in the cluster.
集群允许的最大OSD数量。
Type:
32-bit Integer
Default:
10000
mon globalid prealloc
Description:
The number of global IDs to pre-allocate for clients and daemons in the cluster.
为集群预分配的全局ID数量。
Type:
32-bit Integer
Default:
100
mon sync fs threshold
Description:
Synchronize with the filesystem when writing the specified number of objects. Set it to 0 to disable it.
数量达到设定值时和文件系统同步，0为禁用。
Type:
32-bit Integer
Default:
5
mon subscribe interval
Description:
The refresh interval (in seconds) for subscriptions. The subscription mechanism enables obtaining the cluster maps and log information.
同步的刷新间隔（秒），同步机制允许获取集群运行图和日志信息。
Type:
Double
Default:
300
mon stat smooth intervals
Description:
Ceph will smooth statistics over the last N PG maps.
ceph将平滑最后N个归置组图的统计信息。
Type:
Integer
Default:
2
mon probe timeout
Description:
Number of seconds the monitor will wait to find peers before bootstrapping.
监视器自举无效，搜寻节点前等待的时间。
Type:
Double
Default:
2.0
mon daemon bytes
Description:
The message memory cap for metadata server and OSD messages (in bytes).
给元数据服务器和OSD的消息使用的内存空间。
Type:
64-bit Integer Unsigned
Default:
400ul << 20
mon max log entries per event
Description:
The maximum number of log entries per event.
每个事件允许的最大日志条数。
Type:
Integer
Default:
4096
 3.1.6  心跳配置 （监视器/OSD交互的配置）
Configuring Monitor/OSD Interaction
After you have completed your initial Ceph configuration, you may deploy and run Ceph. When you execute a command such as ceph health or ceph -s, the monitor reports on the current state of the cluster. The monitor knows about the cluster by requiring reports from each OSD, and by receiving reports from OSDs about the status of their neighboring OSDs. If the monitor doesn’t receive reports, or if it receives reports of changes in the cluster, the monitor updates the status of the cluster.
完成基本配置后就可以部署、运行ceph了。执行ceph health或ceph -s命令时，监视器会报告集群当前的状态。监视器通过让各OSD自己报告、并接收OSD关于邻居状态的报告来掌握集群动态。如果监视器没收到报告，或者它只收到集群的变更报告，那它就要更新集群状态。
Ceph provides reasonable default settings for monitor/OSD interaction. However, you may override the defaults. The following sections describe how Ceph monitors and OSDs interact for the purposes of monitoring.
关于监视器/OSD的交互ceph提供了合理的默认值，然而你可以覆盖它们。下面几段描述了ceph监视器如何与OSD交互。
 3.1.6.1  OSD验证心跳
OSDs Check Heartbeats
Each OSD checks the heartbeat of other OSDs every 6 seconds. You can change the heartbeat interval by adding an osd heartbeat interval setting under the [osd] section of your Ceph configuration file, or by setting the value at runtime. If an OSD doesn’t show a heartbeat within a 20 second grace period, the cluster may consider the OSD down. You may change this grace period by adding an osd heartbeat grace setting under the [osd] section of your Ceph configuration file, or by setting the value at runtime.
各OSD每6秒会与其他OSD进行心跳检查，用[osd]下的osd heartbeat interval可更改此间隔、或运行时更改。如果一个OSD 20秒都没有心跳，集群就认为它down了，用[osd]下的osd heartbeat grace可更改宽限期、或者运行时更改。

 3.1.6.2  OSD报告死亡OSD
OSDs Report Down OSDs
By default, an OSD must report to the monitors that another OSD is down three times before the monitors acknowledge that the reported OSD is down. You can change the minimum number of osd down reports by adding an mon osd min down reports setting (osd min down reports prior to v0.62) under the [mon] section of your Ceph configuration file, or by setting the value at runtime. By default, only one OSD is required to report another OSD down. You can change the number of OSDs required to report a monitor down by adding an mon osd min down reporters setting (osd min down reporters prior to v0.62) under the [mon] section of your Ceph configuration file, or by setting the value at runtime.
默认情况下，一个OSD必须向监视器报告三次另一个OSD down的消息，监视器才会认为那个被报告的OSD down了；配置文件里[mon]段下的mon osd min down reports选项（v0.62之前是osd min down reports）可更改这个最少osd down消息次数，或者运行时设置。默认情况下，只要有一个OSD报告另一个OSD挂的消息即可，配置文件里[mon]段下的mon osd min down reporters可用来更改必需OSD数（v0.62之前的osd min down reporters），或者运行时更改。

 3.1.6.3  OSD报告连接建立失败
OSDs Report Peering Failure
If an OSD cannot peer with any of the OSDs defined in its Ceph configuration file, it will ping the monitor for the most recent copy of the cluster map every 30 seconds. You can change the monitor heartbeat interval by adding an osd mon heartbeat interval setting under the [osd] section of your Ceph configuration file, or by setting the value at runtime.
如果一OSD不能和配置文件中定义的任何OSD建立连接，它会每30秒向监视器索要一次最新集群运行图，你可以在[osd]下设置osd mon heartbeat interval来更改这个心跳间隔，或者运行时更改。

 3.1.6.4  OSD报告自己的状态
OSDs Report Their Status
If an OSD doesn’t report to the monitor once at least every 120 seconds, the monitor will consider the OSD down. You can change the monitor report interval by adding an osd mon report interval max setting under the [osd] section of your Ceph configuration file, or by setting the value at runtime. The OSD attempts to report on its status every 30 seconds. You can change the OSD report interval by adding an osd mon report interval min setting under the [osd] section of your Ceph configuration file, or by setting the value at runtime.
如果一OSD至少120秒没向监视器报告过，监视器就认为它down了，你可以设置[osd]下的osd mon report interval max来更改此报告间隔，或者运行时更改。OSD每30秒会报告它自己的状态，在[osd]段下设置osd mon report interval min可更改OSD报告间隔，或运行时更改。

 3.1.6.5  配置选项
Configuration Settings
When modifying heartbeat settings, you should include them in the [global] section of your configuration file.
心跳选项应该置于配置文件的[global]段下。
 3.1.6.5.1  监视器选项
Monitor Settings
mon osd min up ratio
Description:
The minimum ratio of up OSDs before Ceph will mark OSDs down.
在把OSD标记为down前，保持处于up状态的OSD最小比例
Type:
Double
Default:
.3
mon osd min in ratio
Description:
The minimum ratio of in OSDs before Ceph will mark OSDs out.
在把OSD标记为out前，保持处于in状态的OSD最小比例
Type:
Double
Default:
.3
mon osd laggy halflife
Description:
The number of seconds laggy estimates will decay.
滞后量消退时间，秒。
Type:
Integer
Default:
60*60
mon osd laggy weight
Description:
The weight for new samples in laggy estimation decay.
滞后量消退时新样本的权重。
Type:
Double
Default:
0.3
mon osd adjust heartbeat grace
Description:
If set to true, Ceph will scale based on laggy estimations.
设置为true时，ceph将根据滞后量伸缩。
Type:
Boolean
Default:
true
mon osd adjust down out interval
Description:
If set to true, Ceph will scaled based on laggy estimations.
设置为true时，ceph将根据滞后量伸缩。
Type:
Boolean
Default:
true
mon osd auto mark in
Description:
Ceph will mark any booting OSDs as in the cluster.
ceph将把任何启动中的OSD标记为在集群中（in）。
Type:
Boolean
Default:
false
mon osd auto mark auto out in
Description:
Ceph will mark booting OSDs auto marked out of the cluster as in the cluster.
把在booting状态、且为auto marked out（不在对象存储集群内）状态的OSD标记为in。
Type:
Boolean
Default:
true
mon osd auto mark new in
Description:
Ceph will mark booting new OSDs as in the cluster.
把booting状态的新OSD标记为in。
Type:
Boolean
Default:
true
mon osd down out interval
Description:
The number of seconds Ceph waits before marking an OSD down and out if it doesn’t respond.
在OSD停止响应多少秒后把它标记为down。
Type:
32-bit Integer
Default:
300
mon osd downout subtree limit
Description:
The largest CRUSH unit type that Ceph will automatically mark out.
ceph可以自动标记为out的CRUSH单元。
Type:
String
Default:
rack
mon osd report timeout
Description:
The grace period in seconds before declaring unresponsive OSDs down.
宣布无响应OSD down前的宽限期，秒。
Type:
32-bit Integer
Default:
900
mon osd min down reporters
Description:
The minimum number of OSDs required to report a down OSD.
确定一OSD down的最少报告来源OSD数。
Type:
32-bit Integer
Default:
1
mon osd min down reports
Description:
The minimum number of times an OSD must report that another is down.
一OSD必须报告另一个down的次数。
Type:
32-bit Integer
Default:
3

 3.1.6.5.2  OSD选项
OSD Settings
osd heartbeat address
Description:
An OSD’s network address for heartbeats.
OSD用于心跳的网络地址。
Type:
Address
Default:
The host address.
本主机地址。
osd heartbeat interval
Description:
How often an OSD pings its peers (in seconds).
一OSD探测邻居的频率，秒。
Type:
32-bit Integer
Default:
6
osd heartbeat grace
Description:
The elapsed time when an OSD hasn’t shown a heartbeat that the cluster considers it down.
OSD多久没心跳就会被集群认为它挂（down）了。
Type:
32-bit Integer
Default:
20
osd mon heartbeat interval
Description:
How often the OSD pings a monitor if it has no OSD peers.
OSD没有邻居时多久探测一次监视器。
Type:
32-bit Integer
Default:
30
osd mon report interval max
Description:
The maximum time in seconds for an OSD to report to a monitor before the monitor considers the OSD down.
监视器允许OSD报告的最大间隔，超时将认为OSD挂了（down）。
Type:
32-bit Integer
Default:
120
osd mon report interval min
Description:
The minimum number of seconds for an OSD to report to a monitor to avoid the monitor considering the OSD down.
为免监视器把OSD标记为down，进行报告的最小间隔。
Type:
32-bit Integer
Default:
5
Valid Range:
Should be less than osd mon report interval max
要小于osd mon report interval max
osd mon ack timeout
Description:
The number of seconds to wait for a monitor to acknowledge a request for statistics.
OSD等待监视器提供统计信息的时间，秒。
Type:
32-bit Integer
Default:
30

 3.1.7  OSD配置
OSD Config Reference
You can configure OSDs in the Ceph configuration file, but OSDs can use the default values and a very minimal configuration. A minimal OSD configuration sets osd journal size and osd host, and uses default values for nearly everything else.
你可以在配置文件里配置OSD，但它可以用默认值和最小化配置。最简OSD配置需设置osd journal size和osd host，其他几乎都能用默认值。
OSDs are numerically identified in incremental fashion, beginning with 0 using the following convention.
OSD用递增数字标识，从0开始，按惯例这样表示：
osd.0
osd.1
osd.2
In a configuration file, you may specify settings for all OSDs in the cluster by adding configuration settings to the [osd] section of your configuration file. To add settings directly to a specific OSD (e.g., osd host), enter it in an OSD-specific section of your configuration file. For example:
在配置文件里，你可以在[osd]下配置所有OSD；要添加针对特定OSD的选项（如osd host），把它放到那个OSD段下即可，如：
[osd]
        osd journal size = 1024

[osd.0]
        osd host = osd-host-a

[osd.1]
        osd host = osd-host-b
 3.1.7.1  常规配置
General Settings
The following settings provide an OSD’s ID, and determine paths to data and journals. Ceph deployment scripts typically generate the UUID automatically. We DO NOT recommend changing the default paths for data or journals, as it makes it more problematic to troubleshoot Ceph later.
下列选项给出一OSD的ID，可以确定数据和日志的路径。ceph部署脚本通常会自动生成UUID。我们不建议更改数据和日志的默认路径，因为会使稍后的排障更麻烦。
The journal size should be at least twice the product of the expected drive speed multiplied by filestore max sync interval. However, the most common practice is to partition the journal drive (often an SSD), and mount it such that Ceph uses the entire partition for the journal.
日志尺寸最小应该是期望的驱动器速度和filestore max sync interval的乘积；最常见的方法是为日志驱动器（通常是SSD）分区并挂载好，使得ceph可以用整个分区做日志。
osd uuid
Description:
The universally unique identifier (UUID) for the OSD.
OSD的全局唯一标识符（UUID）。
Type:
UUID
Default:
The UUID.
Note:
The osd uuid applies to a single OSD. The fsid applies to the entire cluster.
osd uuid适用于单个OSD，fsid适用于整个集群。
osd data
Description:
The path to the OSDs data. You must create the directory when deploying Ceph. You should mount a drive for OSD data at this mount point. We do not recommend changing the default.
OSD数据存储位置，你得创建并把数据盘挂载到其下。我们不推荐更改默认值。
Type:
String
Default:
/var/lib/ceph/osd/$cluster-$id
osd max write size
Description:
The maximum size of a write in megabytes.
一次写入的最大尺寸，MB。
Type:
32-bit Integer
Default:
90
osd client message size cap
Description:
The largest client data message allowed in memory.
内存里允许的最大客户端数据消息。
Type:
64-bit Integer Unsigned
Default:
500MB default. 500*1024L*1024L
osd class dir
Description:
The class path for RADOS class plug-ins.
RADOS类插件的路径。
Type:
String
Default:
$libdir/rados-classes
 3.1.7.2  日志选项
Journal Settings
By default, Ceph expects that you will store an OSDs journal with the following path:
默认情况下，ceph觉得你会把OSD日志存储于下列路径：
/var/lib/ceph/osd/$cluster-$id/journal
Without performance optimization, Ceph stores the journal on the same disk as the OSDs data. An OSD optimized for performance may use a separate disk to store journal data (e.g., a solid state drive delivers high performance journaling).
没有性能优化时，ceph会把日志存储在和OSD数据相同的硬盘上。追求高性能的OSD可用单独的硬盘存储日志数据，如固态硬盘能提供高性能日志。
Ceph’s default osd journal size is 0, so you will need to set this in your ceph.conf file. A journal size should find the product of the filestore max sync interval and the expected throughput, and multiply the product by two (2):
osd journal size默认值是0，所以你得在ceph.conf里设置。此值应该是filestore max sync interval和期望吞吐量的乘积再乘以2。
osd journal size = {2 * (expected throughput * filestore max sync interval)}
The expected throughput number should include the expected disk throughput (i.e., sustained data transfer rate), and network throughput. For example, a 7200 RPM disk will likely have approximately 100 MB/s. Taking the min() of the disk and network throughput should provide a reasonable expected throughput. Some users just start off with a 10GB journal size. For example:
期望的吞吐量应包含期望的硬盘吞吐量（如持续数据传输速率）、和网络吞吐量，例如一个7200转硬盘的速度大致是100MB/s。硬盘和网络吞吐量中较小的一个是相对合理的吞吐量，有的用户则以10GB日志尺寸起步，例如：
osd journal size = 10000
osd journal
Description:
The path to the OSD’s journal. This may be a path to a file or a block device (such as a partition of an SSD). If it is a file, you must create the directory to contain it. We recommend using a drive separate from the osd data drive.
OSD日志路径。可以是一个文件或块设备（SSD的一个分区），如果是文件，要先创建相应目录。
Type:
String
Default:
/var/lib/ceph/osd/$cluster-$id/journal
osd journal size
Description:
The size of the journal in megabytes. If this is 0, and the journal is a block device, the entire block device is used. Since v0.54, this is ignored if the journal is a block device, and the entire block device is used.
日志尺寸（MB）。如果是0且日志文件是块设备，它会使用整个块设备，从v0.54起，如果日志文件是块设备，这个选项会被忽略，且会使用整个块设备。
Type:
32-bit Integer
Default:
5120
Recommended:
Begin with 1GB. Should be at least twice the product of the expected speed multiplied by filestore max sync interval.
最少1G，应该是期望的驱动器速度和filestore max sync interval的乘积。
See Journal Config Reference for additional details.
详情见 xxx
 3.1.7.3  监视器和OSD的交互
Monitor OSD Interaction
OSDs check each other’s heartbeats and report to monitors periodically. Ceph can use default values in many cases. However, if your network has latency issues, you may need to adopt longer intervals. See Configuring Monitor/OSD Interaction for a detailed discussion of heartbeats.
OSD周期性地相互检查心跳并报告给监视器。多数情况下，ceph都可以用默认值，但是如果你的网络延时大，就得用较长间隔。关于心跳的讨论参见Configuring Monitor/OSD Interaction。
 3.1.7.4  数据归置
Data Placement
See Pool & PG Config Reference for details.
详情见Pool & PG Config Reference。
 3.1.7.5  洗刷
Scrubbing
In addition to making multiple copies of objects, Ceph insures data integrity by scrubbing placement groups. Ceph scrubbing is analogous to fsck on the object storage layer. For each placement group, Ceph generates a catalog of all objects and compares each primary object and its replicas to ensure that no objects are missing or mismatched. Light scrubbing (daily) checks the object size and attributes. Deep scrubbing (weekly) reads the data and uses checksums to ensure data integrity.
除了为对象保存多个副本外，ceph还靠洗刷归置组来保证数据完整性。这种洗刷类似对象存储层的fsck，对每个归置组，ceph生成一个所有对象的目录，并比对每个主对象及其副本以确保没有对象丢失或错配。轻微洗刷（每天）检查对象尺寸和属性，深层洗刷（每周）会读出数据并用校验和保证数据完整性。
Scrubbing is important for maintaining data integrity, but it can reduce performance. You can adjust the following settings to increase or decrease scrubbing operations.
洗刷对维护数据完整性很重要，但会影响性能；你可以用下列选项来增加或减少洗刷操作。
osd max scrubs
Description:
The maximum number of scrub operations for an OSD.
一OSD的最大并发洗刷操作数。
Type:
32-bit Int
Default:
1
osd scrub thread timeout
Description:
The maximum time in seconds before timing out a scrub thread.
洗刷线程最大死亡时值。
Type:
32-bit Integer
Default:
60
osd scrub finalize thread timeout
Description:
The maximum time in seconds before timing out a scrub finalize thread.
洗刷终结线程最大超时值。
Type:
32-bit Integer
Default:
60*10
osd scrub load threshold
Description:
The maximum CPU load. Ceph will not scrub when the CPU load is higher than this number. Default is 50%.
最大CPU负载，当前CPU使用率高于此值时ceph不会洗刷。默认50%。
Type:
Float
Default:
0.5
osd scrub min interval
Description:
The maximum interval in seconds for scrubbing the OSD when the cluster load is low.
集群负载低的时候，洗刷的最大间隔时间，秒。
Type:
Float
Default:
5 minutes. 300
osd scrub max interval
Description:
The maximum interval in seconds for scrubbing the OSD irrespective of cluster load.
不论集群负载如何，都要进行洗刷的时间间隔。
Type:
Float
Default:
Once per day. 60*60*24
osd deep scrub interval
Description:
The interval for “deep” scrubbing (fully reading all data).
深层洗刷的间隔（完整地读所有数据）。
Type:
Float
Default:
Once per week. 60*60*24*7
osd deep scrub stride
Description:
Read size when doing a deep scrub.
深层洗刷时的读取尺寸。
Type:
32-bit Int
Default:
512 KB. 524288
 3.1.7.6  操作数
Operations
Operations settings allow you to configure the number of threads for servicing requests. If you set osd op threads to 0, it disables multi-threading. By default, Ceph uses two threads with a 30 second timeout and a 30 second complaint time if an operation doesn’t complete within those time parameters. You can set operations priority weights between client operations and recovery operations to ensure optimal performance during recovery.
操作数选项允许你设置用于服务的线程数，如果把osd op threads设置为0就禁用了多线程。默认情况下，ceph用两个参数30秒超时和30秒抗议时间来把握线程情况，如果一个操作在这些限定时间内没完成（ceph会采取措施）。你可以调整客户端操作和恢复操作的优先程度来保证恢复期间仍有良好的性能。
osd op threads
Description:
The number of threads to service OSD operations. Set to 0 to disable it. Increasing the number may increase the request processing rate.
OSD操作线程数，0为禁用。增大数量可以增加请求处理速度。
Type:
32-bit Integer
Default:
2
osd client op priority
Description:
The priority set for client operations. It is relative to osd recovery op priority.
设置客户端操作优先级，它相对于osd recovery op priority。
Type:
32-bit Integer
Default:
63
Valid Range:
1-63
osd recovery op priority
Description:
The priority set for recovery operations. It is relative to osd client op priority.
设置恢复优先级，其值相对于osd client op priority。
Type:
32-bit Integer
Default:
10
Valid Range:
1-63
osd op thread timeout
Description:
The OSD operation thread timeout in seconds.
OSD线程超时秒数。
Type:
32-bit Integer
Default:
30
osd op complaint time
Description:
An operation becomes complaint worthy after the specified number of seconds have elapsed.
一个操作进行多久后开始抱怨。
Type:
Float
Default:
30
osd disk threads
Description:
The number of disk threads, which are used to perform background disk intensive OSD operations such as scrubbing and snap trimming.
硬盘线程数，用于在后台执行IO密集操作，像数据洗刷和快照修复。
Type:
32-bit Integer
Default:
1
osd op history size
Description:
The maximum number of completed operations to track.
要跟踪的最大已完成操作数量。
Type:
32-bit Unsigned Integer
Default:
20
osd op history duration
Description:
The oldest completed operation to track.
要跟踪的最老已完成操作。
Type:
32-bit Unsigned Integer
Default:
600
osd op log threshold
Description:
How many operations logs to display at once.
一次显示多少操作日志。
Type:
32-bit Integer
Default:
5
 3.1.7.7  回填
Backfilling
When you add or remove OSDs to a cluster, the CRUSH algorithm will want to rebalance the cluster by moving placement groups to or from OSDs to restore the balance. The process of migrating placement groups and the objects they contain can reduce the cluster’s operational performance considerably. To maintain operational performance, Ceph performs this migration with ‘backfilling’, which allows Ceph to set backfill operations to a lower priority than requests to read or write data.
当你增加或移除OSD时，CRUSH算法将要求重新均衡集群，它会把一些归置组移出或移入多个OSD以回到均衡状态。归置组和对象的迁移过程会明显降低集群运营性能，为维持运营性能，ceph用backfilling来执行此迁移，它可以使得ceph的回填操作优先级低于用户读写请求。
osd max backfills
Description:
The maximum number of backfills allowed to or from a single OSD.
单个OSD允许的最大回填操作数。
Type:
64-bit Unsigned Integer
Default:
10
osd backfill scan min
Description:
The scan interval in seconds for backfill operations when cluster load is low.
集群负载低时，回填操作时扫描间隔。
Type:
32-bit Integer
Default:
64
osd backfill scan max
Description:
The maximum scan interval in seconds for backfill operations irrespective of cluster load.
回填操作时最大扫描间隔。
Type:
32-bit Integer
Default:
512
osd backfill full ratio
Description:
Refuse to accept backfill requests when the OSD’s full ratio is above this value.
OSD的占满率达到多少时拒绝接受回填请求。
Type:
Float
Default:
0.85
osd backfill retry interval
Description:
The number of seconds to wait before retrying backfill requests.
重试回填请求前等待秒数。
Type:
Double
Default:
10.0
 3.1.7.8  OSD运行图
OSD Map
OSD maps reflect the OSD daemons operating in the cluster. Over time, the number of map epochs increases. Ceph provides some settings to ensure that Ceph performs well as the OSD map grows larger.
OSD运行图反映集群中运行的OSD守护进程，斗转星移，图元增加。ceph用一些选项来确保OSD运行图增大时仍运行良好。
osd map dedup
Description:
Enable removing duplicates in the OSD map.
允许删除OSD图里的重复项。
Type:
Boolean
Default:
true
osd map cache size
Description:
The size of the OSD map cache in megabytes.
OSD图缓存尺寸，MB。
Type:
32-bit Integer
Default:
500
osd map cache bl size
Description:
The size of the in-memory OSD map cache in OSD daemons.
OSD进程中，驻留内存的OSD图缓存尺寸。
Type:
32-bit Integer
Default:
50
osd map cache bl inc size
Description:
The size of the in-memory OSD map cache incrementals in OSD daemons.
OSD进程中，驻留内存的OSD图缓存增量尺寸。
Type:
32-bit Integer
Default:
100
osd map message max
Description:
The maximum map entries allowed per MOSDMap message.
每个 MOSDMap图消息允许的最大条目数量。
Type:
32-bit Integer
Default:
100
 3.1.7.9  恢复
Recovery
When the cluster starts or when an OSD crashes and restarts, the OSD begins peering with other OSDs before writes can occur. See Monitoring OSDs and PGs for details.
当集群重启或某OSD崩溃后重启时，此OSD开始与其它OSD们建立连接，这样才能正常工作。详情见 Monitoring OSDs and PGs。
If an OSD crashed and comes back online, usually it will be out of sync with other OSDs containing more recent versions of objects in the placement groups. When this happens, the OSD goes into recovery mode and seeks to get the latest copy of the data and bring its map back up to date. Depending upon how long the OSD was down, the OSD’s objects and placement groups may be significantly out of date. Also, if a failure domain went down (e.g., a rack), more than one OSD may come back online at the same time. This can make the recovery process time consuming and resource intensive.
如果某OSD崩溃并重生，通常和其他OSD不同步，没有同归置组内最新版本的对象。这时，OSD进入恢复模式并且搜索最新数据副本，并更新运行图。根据OSD挂的时间长短，OSD的对象和归置组可能明显过期，另外，如果一个失效域挂了（如一个机柜），多个OSD会同时重生，这样恢复时间更长、更耗资源。
To maintain operational performance, Ceph performs recovery with limitations on the number recovery requests, threads and object chunk sizes which allows Ceph perform well in a degraded state.
为保持运营性能，ceph进行恢复时会限制恢复请求数、线程数、对象块尺寸，这样在降级状态下也能运行良好。
osd recovery delay start
Description:
After peering completes, Ceph will delay for the specified number of seconds before starting to recover objects.
对等关系建立完毕后，ceph开始对象恢复前等待的时间（秒）。
Type:
Float
Default:
15
osd recovery max active
Description:
The number of active recovery requests per OSD at one time. More requests will accelerate recovery, but the requests places an increased load on the cluster.
每个OSD一次处理的活跃恢复请求，增大此值能加速恢复，但增加了集群的负载。
Type:
32-bit Integer
Default:
5
osd recovery max chunk
Description:
The maximum size of a recovered chunk of data to push.
一次推送的数据块的最大尺寸。
Type:
64-bit Integer Unsigned
Default:
1 << 20
osd recovery threads
Description:
The number of threads for recovering data.
数据恢复时的线程数。
Type:
32-bit Integer
Default:
1
osd recovery thread timeout
Description:
The maximum time in seconds before timing out a recovery thread.
恢复线程最大死亡时值。
Type:
32-bit Integer
Default:
30
osd recover clone overlap
Description:
Preserves clone overlap during recovery. Should always be set to true.
数据恢复和迁移时保留的克隆重叠率。
Type:
Boolean
Default:
true
 3.1.7.10  杂项
Miscellaneous
osd snap trim thread timeout
Description:
The maximum time in seconds before timing out a snap trim thread.
快照修复线程最大死亡时值。
Type:
32-bit Integer
Default:
60*60*1
osd backlog thread timeout
Description:
The maximum time in seconds before timing out a backlog thread.
积压线程最大死亡时值。
Type:
32-bit Integer
Default:
60*60*1
osd default notify timeout
Description:
The OSD default notification timeout (in seconds).
OSD默认通告超时。
Type:
32-bit Integer Unsigned
Default:
30
osd check for log corruption
Description:
Check log files for corruption. Can be computationally expensive.
根据日志文件查找数据腐败，会耗费大量计算时间。
Type:
Boolean
Default:
false
osd remove thread timeout
Description:
The maximum time in seconds before timing out a remove OSD thread.
OSD删除线程的最大死亡时值。
Type:
32-bit Integer
Default:
60*60
osd command thread timeout
Description:
The maximum time in seconds before timing out a command thread.
命令线程最大超时值。
Type:
32-bit Integer
Default:
10*60
osd command max records
Description:
Limits the number of lost objects to return.
限制返回的丢失对象数量。
Type:
32-bit Integer
Default:
256
osd auto upgrade tmap
Description:
Uses tmap for omap on old objects.
在旧对象上给omap使用tmap。
Type:
Boolean
Default:
true
osd tmapput sets users tmap
Description:
Uses tmap for debugging only.
只在调试时使用tmap。
Type:
Boolean
Default:
false
osd preserve trimmed log
Description:
Preserves trimmed log files, but uses more disk space.
Type:
Boolean
Default:
false
 3.1.8  文件存储配置
Filestore config reference
filestore debug omap check
Description:
Debugging check on synchronization. Expensive. For debugging only.
同步检查。昂贵的调试选项。
Type:
Boolean
Required:
No
Default:
0
 3.1.8.1  扩展属性
EXTENDED ATTRIBUTES
Extended Attributes (XATTRs) are an imporant aspect in your configuration. Some file systems have limits on the number of bytes stored in XATTRS. Additionally, in some cases, the filesystem may not be as fast as an alternative method of storing XATTRs. The following settings may help improve performance by using a method of storing XATTRs that is extrinsic to the underlying filesystem.
扩展属性（XATTR）是配置里的重要方面。一些文件系统对XATTR字节数有限制，而且在一些情况下文件系统存储XATTR的速度不如其他方法，下面的设置通过使用独立于文件系统的存储方法能帮你提升性能。
Ceph XATTRs are stored as inline xattr, using the XATTRs provided by the underlying file system, if it does not impose a size limit. If there is a size limit ( 4KB total on ext4, for instance ), some Ceph XATTRs will be stored in an key/value database ( aka omap ) when the filestore max inline xattr size or filestore max inline xattrs threshold are reached.
ceph扩展属性用底层文件系统（如果没有尺寸限制）的XATTR存储为inline xattr。如果有限制，如ext4限制为4KB，达到filestore max inline xattr size或filestore max inline xattrs 阀值时一些XATTR将存储为键/值数据库（也叫omap）。
filestore xattr use omap
Description:
Use object map for XATTRS. Set to true for ext4 file systems.
为XATTR使用对象图，采用ext4文件系统时要设置为true。
Type:
Boolean
Required:
No
Default:
false
filestore max inline xattr size
Description:
The maximimum size of an XATTR stored in the filesystem (i.e., XFS, btrfs, ext4, etc.) per object. Should not be larger than the filesytem can handle.
每个对象在文件系统里存储的XATTR最大尺寸，应该小于文件系统支持的尺寸。
Type:
Unsigned 32-bit Integer
Required:
No
Default:
512
filestore max inline xattrs
Description:
The maximum number of XATTRs stored in the fileystem per object.
每个对象存储在文件系统里的XATTR数量。
Type:
32-bit Integer
Required:
No
Default:
2
 3.1.8.2  同步间隔
SYNCHRONIZATION INTERVALS
Periodically, the filestore needs to quiesce writes and synchronize the filesystem, which creates a consistent commit point. It can then free journal entries up to the commit point. Synchronizing more frequently tends to reduce the time required perform synchronization, and reduces the amount of data that needs to remain in the journal. Less frequent synchronization allows the backing filesystem to coalesce small writes and metadata updates more optimally–potentially resulting in more efficient synchronization.
filestore需要周期性地静默写入、同步文件系统，这创建了一个提交点，然后就能释放相应的日志条目了。较大的同步频率可减小执行同步的时间及保存在日志里的数据量；较小的频率使得后端的文件系统能优化归并较小的数据和元数据写入，因此可能使同步更有效。

filestore max sync interval
Description:
The maximum interval in seconds for synchronizing the filestore.
同步filestore的最大间隔秒数。
Type:
Double
Required:
No
Default:
5
filestore min sync interval
Description:
The minimum interval in seconds for synchronizing the filestore.
同步filestore的最小间隔秒数。
Type:
Double
Required:
No
Default:
.01
 3.1.8.3  flusher（回冲器？）
FLUSHER
The filestore flusher forces data from large writes to be written out using sync file range before the sync in order to (hopefully) reduce the cost of the eventual sync. In practice, disabling ‘filestore flusher’ seems to improve performance in some cases.
文件存储回写器强制使用sync file range来写出大块数据，这样处理有望减小最终同步的代价。实践中，禁用"filestore flusher"有时候能提升性能。
filestore flusher
Description:
Enables the filestore flusher.
启用filestore flusher功能。
Type:
Boolean
Required:
No
Default:
false
filestore flusher max fds
Description:
Sets the maximum number of file descriptors for the flusher.
设置回写器的最大文件描述符数量。
Type:
Integer
Required:
No
Default:
512
filestore sync flush
Description:
Enables the synchronization flusher.
启用同步回写器。
Type:
Boolean
Required:
No
Default:
false
filestore fsync flushes journal data
Description:
Flush journal data during filesystem synchronization.
文件系统同步时也回写日志数据。
Type:
Boolean
Required:
No
Default:
false
 3.1.8.4  队列
QUEUE
The following settings provide limits on the size of filestore queue.
下面的配置能限制文件存储队列的尺寸。
filestore queue max ops
Description:
Defines the maximum number of in progress operations the file store accepts before blocking on queuing new operations.
文件存储操作接受的最大并发数，超过此设置的请求会被拒绝。
Type:
Integer
Required:
No. Minimal impact on performance.
无。对性能影响最小。
Default:
500
filestore queue max bytes
Description:
The maximum number of bytes for an operation.
一个操作的最大字节数。
Type:
Integer
Required:
No
Default:
100 << 20
filestore queue committing max ops
Description:
The maximum number of operations the filestore can commit.
文件存储能提交的最大操作数。
Type:
Integer
Required:
No
Default:
500
filestore queue committing max bytes
Description:
The maximum number of bytes the filestore can commit.
文件存储器能提交的最大字节数。
Type:
Integer
Required:
No
Default:
100 << 20
 3.1.8.5  超时选项
TIMEOUTS
filestore op threads
Description:
The number of filesystem operation threads that execute in parallel.
最大并行文件系统操作线程数。
Type:
Integer
Required:
No
Default:
2
filestore op thread timeout
Description:
The timeout for a filesystem operation thread (in seconds).
文件系统操作线程超时值，单位为秒。
Type:
Integer
Required:
No
Default:
60
filestore op thread suicide timeout
Description:
The timeout for a commit operation before cancelling the commit (in seconds).
提交操作超时值（秒），超时后会取消。
Type:
Integer
Required:
No
Default:
180
 3.1.8.6  B-tree文件系统
B-TREE FILESYSTEM
filestore btrfs snap
Description:
Enable snapshots for a btrfs filestore.
对btrfs文件存储器启用快照功能。
Type:
Boolean
Required:
No. Only used for btrfs.
无。仅适用于btrfs。
Default:
true
filestore btrfs clone range
Description:
Enable cloning ranges for a btrfs filestore.
允许btrfs文件存储克隆动作排队。
Type:
Boolean
Required:
No. Only used for btrfs.
无。仅适用于btrfs。
Default:
true
 3.1.8.7  日志（文件系统）
JOURNAL
filestore journal parallel
Description:
Enables parallel journaling, default for btrfs.
允许并发记日志，对btrfs默认开。
Type:
Boolean
Required:
No
Default:
false
filestore journal writeahead
Description:
Enables writeahead journaling, default for xfs.
允许预写日志，对xfs默认开。
Type:
Boolean
Required:
No
Default:
false
filestore journal trailing
Description:
Deprecated, never use.
过时了，从没用过。
Type:
Boolean
Required:
No
Default:
false
 3.1.8.8  杂项
MISC
filestore merge threshold
Description:
Min number of files in a subdir before merging into parent
并入父目录前，子目录内的最小文件数。
Type:
Integer
Required:
No
Default:
10
filestore split multiple
Description:
filestore_split_multiple*filestore_merge_threshold*16 is the max files in a subdir before splitting into child directories.
filestore_split_multiple*filestore_merge_threshold*16是分割为子目录前某目录内的最大文件数。
Type:
Integer
Required:
No
Default:
2
filestore update to
Description:
Limits filestore auto upgrade to specified version.
限制文件存储自动更新到某个指定版本。
Type:
Integer
Required:
No
Default:
1000
filestore blackhole
Description:
Drop any new transactions on the floor.
丢弃任何讨论中的事务。
Type:
Boolean
Required:
No
Default:
false
filestore dump file
Description:
File onto which store transaction dumps?
存储事务转储目的文件。
Type:
Boolean
Required:
No
Default:
false
filestore kill at
Description:
inject a failure at the n’th opportunity
在第N次机会后注入一个失效。
Type:
String
Required:
No
Default:
false
filestore fail eio
Description:
Fail/Crash on eio.
在IO错误的时候失败或崩溃。
Type:
Boolean
Required:
No
Default:
true
 3.1.9  日志配置
Journal config reference
Ceph OSDs use a journal for two reasons: speed and consistency.
ceph的OSD使用日志的原因有二：速度和一致性。
Speed: The journal enables the OSD to commit small writes quickly. Ceph writes small, random i/o to the journal sequentially, which tends to speed up bursty workloads by allowing the backing filesystem more time to coalesce writes. The OSD journal, however, can lead to spiky performance with short spurts of high-speed writes followed by periods without any write progress as the filesystem catches up to the journal.
速度：日志允许OSD快速地提交小块数据的写入，ceph把小片、随机IO依次写入日志，这样，后端文件系统就有机会归并写入动作，并最终提升并发承载力。因此，使用OSD日志能展现出优秀的瞬间写性能，实际上却没有任何写动作，因为文件系统把它们捕捉到了日志。
Consistency: Ceph OSDs requires a filesystem interface that guarantees atomic compound operations. Ceph OSDs write a description of the operation to the journal and apply the operation to the filesystem. This enables atomic updates to an object (for example, placement group metadata). Every few seconds–between filestore max sync interval and filestore min sync interval–the OSD stops writes and synchronizes the journal with the filesystem, allowing OSDs to trim operations from the journal and reuse the space. On failure, OSDs replay the journal starting after the last synchronization operation.
一致性：ceph的OSD需要一个能保证原子操作的文件系统接口。OSD把一个操作的描述写入日志，然后把操作应用到文件系统，这需要原子更新一个对象（例如归置组元数据）。每隔一段 filestore max sync interval和 filestore min sync interval之间的时间，OSD停止写入、把日志同步到文件系统，这样允许OSD修整日志里的操作并重用空间。失败后，OSD从上次的同步点开始重放日志。
Ceph OSDs support the following journal settings:
OSD支持下面的日志设置：
journal dio
Description:
Enables direct i/o to the journal. Requires journal block align set to true.
对日志启用径直IO，需要 journal block align设置为true。
Type:
Boolean
Required:
Yes when using aio.
用aio时自动启用。
Default:
true
journal aio
Description:
Enables using libaio for asynchronous writes to the journal. Requires journal dio set to true.
异步写入日志时用libaio库，需要 journal dio设为true。
Type:
Boolean
Required:
No.
Default:
false
journal block align
Description:
Block aligns writes. Required for dio and aio.
块对齐写，dio和aio需要。
Type:
Boolean
Required:
Yes when using dio and aio.
用dio和aio时自动启用。
Default:
true
journal max write bytes
Description:
The maximum number of bytes the journal will write at any one time.
一次写入日志的最大尺寸。
Type:
Integer
Required:
No
Default:
10 << 20
journal max write entries
Description:
The maximum number of entries the journal will write at any one time.
一次写入日志的最大数量。
Type:
Integer
Required:
No
Default:
100
journal queue max ops
Description:
The maximum number of operations allowed in the queue at any one time.
队列里一次允许的最大操作数量。
Type:
Integer
Required:
No
Default:
500
journal queue max bytes
Description:
The maximum number of bytes allowed in the queue at any one time.
队列里一次允许的最大字节数。
Type:
Integer
Required:
No
Default:
10 << 20
journal align min size
Description:
Align data payloads greater than the specified minimum.
对齐大于指定最小值的数据有效载荷。
Type:
Integer
Required:
No
Default:
64 << 10
journal zero on create
Description:
Causes the file store to overwrite the entire journal with 0‘s during mkfs.
在创建文件系统期间用0填充整个日志。
Type:
Boolean
Required:
No
Default:
false
 3.1.10  存储池、归置组和CRUSH配置
Pool, PG and CRUSH Config Reference
When you create pools and set the number of placement groups for the pool, Ceph uses default values when you don’t specifically override the defaults. We recommend overridding some of the defaults. Specifically, we recommend setting a pool’s replica size and overriding the default number of placement groups. You can specifically set these values when running pool commands. You can also override the defaults by adding new ones in the [global] section of your Ceph configuration file.
当你创建存储池并给它分配归置组数量时，如果你没指定ceph就用默认值。我们建议更改某些默认值，特别是存储池的副本数和默认归置组数量，可以在运行 pool命令的时候设置这些值。你也可以把新默认值写入ceph配置文件的[global]段。
[global]

        # By default, Ceph makes 2 replicas of objects. If you want to make three
        # copies of an object the default value--a primary copy and two replica
        # copies--reset the default values as shown in 'osd pool default size'.
        # If you want to allow Ceph to write a lesser number of copies in a degraded
        # state, set 'osd pool default min size' to a number less than the
        # 'osd pool default size' value.

        osd pool default size = 3  # Write an object 3 times.
        osd pool default min size = 1 # Allow writing one copy in a degraded state.

        # Ensure you have a realistic number of placement groups. We recommend
        # approximately 100 per OSD. E.g., total number of OSDs multiplied by 100
        # divided by the number of replicas (i.e., osd pool default size). So for
        # 10 OSDs and osd pool default size = 3, we'd recommend approximately
        # (100 * 10) / 3 = 333.

        osd pool default pg num = 333
        osd pool default pgp num = 333
mon max pool pg num
Description:
The maximium number of placement groups per pool.
每个存储的最大归置组数量。
Type:
Integer
Default:
65536
mon pg create interval
Description:
Number of seconds between PG creation in the same OSD.
在同一个OSD里创建PG的间隔秒数。
Type:
Float
Default:
30.0
mon pg stuck threshold
Description:
Number of seconds after which PGs can be considered as being stuck.
多长时间无响应的PG才认为它卡住了。
Type:
32-bit Integer
Default:
300
osd pg bits
Description:
Placement group bits per OSD.
每个OSD的归置组位数。
Type:
32-bit Integer
Default:
6
osd pgp bits
Description:
The number of bits per OSD for PGPs.
每个OSD为PGP留的位数。
Type:
32-bit Integer
Default:
6
osd crush chooseleaf type
Description:
The bucket type to use for chooseleaf in a CRUSH rule. Uses ordinal rank rather than name.
在一个CRUSH规则内用于chooseleaf的桶类型。用序列号而不是名字。
Type:
32-bit Integer
Default:
1. Typically a host containing one or more OSDs.
1，通常一台主机包含一或多个OSD。
osd min rep
Description:
The minimum number of replicas for a ruleset.
一规则集的最小副本数。
Type:
32-bit Integer
Default:
1
osd max rep
Description:
The maximum number of replicas for a ruleset.
一规则集的最大副本数。
Type:
32-bit Integer
Default:
10
osd pool default crush rule
Description:
The default CRUSH ruleset to use when creating a pool.
创建一存储池时使用的默认CRUSH规则。
Type:
32-bit Integer
Default:
0
osd pool default size
Description:
Sets the number of replicas for objects in the pool. The default value is the same as ceph osd pool set {pool-name} size {size}.
设置一存储池的对象副本数，默认值等同于ceph osd pool set {pool-name} size {size}
Type:
32-bit Integer
Default:
2
osd pool default min size
Descrption:
Sets the minimum number of written replicas for objects in the pool in order to acknowledge a write operation to the client. If minimum is not met, Ceph will not acknowledge the write to the client. This setting ensures a minimum number of replicas when operating in degraded mode.
设置存储池中已写副本的最小数量，以向客户端确认写操作。如果未达到最小值，ceph就不会向客户端发送写确认。此选项确保了降级（degraded）模式下的最小副本数。
Type:
32-bit Integer
Default:
0, which means no particular minimum. If 0, minimum is size - (size / 2).
0，意思是没有最小值。如果为0，最小值是size - (size / 2)
osd pool default pg num
Description:
The default number of placement groups for a pool. The default value is the same as pg_num with mkpool.
一个存储池的默认归置组数量，默认值即是mkpool的pg_num参数。
Type:
32-bit Integer
Default:
8
osd pool default pgp num
Description:
The default number of placement groups for placement for a pool. The default value is the same as pgp_num with mkpool. PG and PGP should be equal (for now).
一个存储池里，为归置使用的归置组数量，默认值等同于mkpool的pgp_num参数。当前PG和PGP应该相同。
Type:
32-bit Integer
Default:
8
osd pool default flags
Description:
The default flags for new pools.
新存储池的默认标志。
Type:
32-bit Integer
Default:
0
osd max pgls
Description:
The maximum number of placement groups to list. A client requesting a large number can tie up the OSD.
将列出的最大归置组数量，一客户端请求量大时会影响OSD。
Type:
Unsigned 64-bit Integer
Default:
1024
Note:
Default should be fine.
默认值应该没问题。
osd min pg log entries
Description:
The minimum number of placement group logs to maintain when trimming log files.
清理日志文件的时候保留的归置组日志量。
Type:
32-bit Int Unsigned
Default:
1000
osd default data pool replay window
Description:
The time (in seconds) for an OSD to wait for a client to replay a request.
一OSD等待客户端重播请求的时间，秒。
Type:
32-bit Integer
Default:
45
 3.1.11  消息
Messaging
ms tcp nodelay
Description:
Disables nagle’s algorithm on messenger tcp sessions.
禁用nagle算法，是关于TCP会话传递的。
Type:
Boolean
Required:
No
Default:
true
ms initial backoff
Description:
The initial time to wait before reconnecting on a fault.
出错时重连的初始等待时间。
Type:
Double
Required:
No
Default:
.2
ms max backoff
Description:
The maximum time to wait before reconnecting on a fault.
出错重连时等待的最大时间。
Type:
Double
Required:
No
Default:
15.0
ms nocrc
Description:
Disables crc on network messages. May increase performance if cpu limited.
禁用网络消息的crc校验，CPU不足时可提升性能。
Type:
Boolean
Required:
No
Default:
false
ms die on bad msg
Description:
Debug option; do not configure.
调试选项，不要配置。
Type:
Boolean
Required:
No
Default:
false
ms dispatch throttle bytes
Description:
Throttles total size of messages waiting to be dispatched.
等着传送的消息尺寸阀值。
Type:
64-bit Unsigned Integer
Required:
No
Default:
100 << 20
ms bind ipv6
Description:
Enable if you want your daemons to bind to IPv6 address instead of IPv4 ones. (Not required if you specify a daemon or cluster IP.)
如果想让守护进程绑定到IPv6地址而非IPv4就得启用（如果你指定了守护进程或集群IP就不必要了）
Type:
Boolean
Required:
No
Default:
false
ms rwthread stack bytes
Description:
Debug option for stack size; do not configure.
堆栈尺寸调试选项，不要配置。
Type:
64-bit Unsigned Integer
Required:
No
Default:
1024 << 10
ms tcp read timeout
Description:
Controls how long (in seconds) the messenger will wait before closing an idle connection.
控制信差关闭空闲连接前的等待秒数。
Type:
64-bit Unsigned Integer
Required:
No
Default:
900
ms inject socket failures
Description:
Debug option; do not configure.
调试选项，别配置。
Type:
64-bit Unsigned Integer
Required:
No
Default:
0
 3.1.12  常规配置
General config reference
fsid
Description:
The filesystem ID. One per cluster.
文件系统ID，每集群一个。
Type:
UUID
Required:
No.
Default:
N/A. Generated if not specified.
admin socket
Description:
The socket for executing administrative commands irrespective of whether Ceph monitors have established a quorum.
执行管理命令的套接字，不管ceph监视器团体是否已经建立。
Type:
String
Required:
No
Default:
/var/run/ceph/$cluster-$name.asok
pid file
Description:
Each running Ceph daemon has a running process identifier (PID) file.
每个运行的ceph进程都有一个PID文件。
Type:
String
Required:
No
Default:
N/A. The default path is /var/run/$cluster/$name.pid. The PID file is generated upon start-up.
chdir
Description:
The directory Ceph daemons change to once they are up and running. Default / directory recommended.
ceph进程一旦启动、运行就进入这个目录，默认推荐/。
Type:
String
Required:
No
Default:
/
max open files
Description:
If set, when the Ceph service starts, Ceph sets the max open fds at the OS level (i.e., the max # of file descriptors). It helps prevents OSDs from running out of file descriptors.
如果设置了，ceph服务启动的时候会在操作系统级设置max open fds（如最大数量的文件描述符），这有助于防止耗尽文件描述符。
Type:
64-bit Integer
Required:
No
Default:
0
fatal signal handlers
Description:
If set, we will install signal handlers for SEGV, ABRT, BUS, ILL, FPE, XCPU, XFSZ, SYS signals to generate a useful log message
如果设置了，将安装SEGV、ABRT、BUS、ILL、FPE、XCPU、XFSZ、SYS信号处理器，用于产生有用的日志信息。
Type:
Boolean
Default:
true

 3.2  ceph部署
Ceph deployment
The ceph-deploy tool is a way to deploy Ceph relying only upon SSH access to the servers, sudo, and some Python. It runs on your workstation, and does not require servers, databases, or any other tools. If you set up and tear down Ceph clusters a lot, and want minimal extra bureaucracy, ceph-deploy is an ideal tool. The ceph-deploy tool is not a generic deployment system. It was designed exclusively for Ceph users who want to get Ceph up and running quickly with sensible initial configuration settings without the overhead of installing Chef, Puppet or Juju. Users who want fine-control over security settings, partitions or directory locations should use a tool such as Juju, Puppet, Chef or Crowbar.
ceph-deploy工具是一种部署ceph的方法，它只依赖到服务器的SSH访问、sudo、和python。它可在你的工作站上运行，不需要服务器、数据库、或其它工具。如果你安装、拆卸过很多ceph集群，不想要额外的工具，那ceph-deploy是理想之选。它不是个通用部署系统，只为ceph用户设计，用它可以快速地设置并运行一个默认值较合理的集群，而无需头疼chef、puppet或juju的安装。如果您想良好地控制安全设置、分区、或目录位置，可以试试类似juju、chef、crowbar的工具。
ceph部署
Ceph Deploy
With ceph-deploy, you can install Ceph packages on remote hosts, create a cluster, add monitors, gather (or forget) keys, add metadata servers and OSDs, configure admin hosts, and tear down the clusters. With a single tool, you can develop scripts to create, deploy and tear down clusters quickly and easily.
用ceph-deploy可以远程安装ceph软件包、创建集群、增加监视器、收集（或丢弃）密钥、增加元数据服务器和OSD、配置管理主机、拆除集群。只用这个工具你就可以快速、轻易地开发脚本来创建、部署和拆除集群。
mkcephfs（已过时）
The mkcephfs utility generates an fsid and keys for your cluster, as defined by the Ceph configuration file. It does not create directories for you and relies on use of the root password. As of Ceph v0.60, it is deprecated in favor of ceph-deploy.
mkcephfs工具可为你生成一个fsid和密钥，正如配置文件里所定义。它没给你创建目录，且依赖root密码，从v0.60起它被ceph-deploy取代了。
 3.2.1  转移到ceph-deploy
Transitioning to ceph-deploy
If you have an existing cluster that you deployed with mkcephfs, you will need to make a few changes to your configuration to ensure that your cluster will work with ceph-deploy.
如果你在用的集群是用mkcephfs部署的，那得稍微改动下配置文件才能和ceph-deploy配合。
 3.2.1.1  监视器密钥环
Monitor Keyring
You will need to add caps mon = "allow *" to your monitor keyring if it is not already in the keyring. By default, the monitor keyring is located under /var/lib/ceph/mon/ceph-$id/keyring. When you have added the caps setting, your monitor keyring should look something like this:
如果你的监视器密钥环里没有caps mon = "allow *"，那就把它加上。监视器的密钥环默认位于 /var/lib/ceph/mon/ceph-$id/keyring，加上caps选项后你的监视器密钥环看起来大致如此：
[mon.]
        key = AQBJIHhRuHCwDRAAZjBTSJcIBIoGpdOR9ToiyQ==
        caps mon = "allow *"
Adding caps mon = "allow *" will ease the transition from mkcephfs to ceph-deploy by allowing ceph-create-keys to use the mon. keyring file in $mon_data and get the caps it needs.
增加caps mon = "allow *"允许了ceph-create-keys使用位于$mon_data下的mon.密钥环文件、并获取需要的能力，因此简化了mkcephfs到ceph-deploy的转移。
 3.2.1.2  用默认路径
Use Default Paths
Under the /var/lib/ceph directory, the mon and osd directories need to use the default paths.
在/var/lib/ceph下，mon和osd目录要使用默认路径。
OSDs: The path should be /var/lib/ceph/osd/ceph-$id
OSDs: 路径应该是/var/lib/ceph/osd/ceph-$id
MON: The path should be /var/lib/ceph/mon/ceph-$id
MON: 路径应该是/var/lib/ceph/mon/ceph-$id
Under those directories, the keyring should be in a file named keyring.
在这些目录下，密钥环应该位于名为keyring的文件内。
 3.2.2  飞前检查
Preflight Checklist
New in version 0.60.
0.60版新增。
This Preflight Checklist will help you prepare an admin node for use with ceph-deploy, and server nodes for use with passwordless ssh and sudo.
Before you can deploy Ceph using ceph-deploy, you need to ensure that you have a few things set up first on your admin node and on nodes running Ceph daemons.
这篇飞前检查帮你准备一个运行ceph-deploy的管理节点，以及多个无密码ssh和sudo登录的服务器节点。
在能用ceph-deploy部署ceph前，你得先确认位于管理节点和ceph守护进程运行节点上的一些配置。
 3.2.2.1  选装一个操作系统
Install an Operating System
Install a recent release of Debian or Ubuntu (e.g., 12.04, 12.10) on your nodes. For additional details on operating systems or to use other operating systems other than Debian or Ubuntu, see OS Recommendations.
安装一个最新版的Debian或Ubuntu（如12.04、12.10），关于Debian或Ubuntu以外的操作系统详见 OS Recommendations。
 3.2.2.2  安装SSH服务器
Install an SSH Server
The ceph-deploy utility requires ssh, so your server node(s) require an SSH server.
ceph-deploy工具需要ssh，所以你的服务器节点需要ssh服务器。
sudo apt-get install openssh-server
 3.2.2.3  创建一用户
Create a User
Create a user on nodes running Ceph daemons.
在运行ceph守护进程的节点上创建一个普通用户。
Tip: We recommend a username that brute force attackers won’t guess easily (e.g., something other than root, ceph, etc).
提示：我们建议选一个暴力攻击者不可能轻易猜到的用户名，如root、ceph之外的名字。

ssh user@ceph-server
sudo useradd -d /home/ceph -m ceph
sudo passwd ceph
ceph-deploy installs packages onto your nodes. This means that the user you create requires passwordless sudo privileges.
ceph-deploy会在节点安装软件包，所以你创建的用户需要无密码sudo权限。
Note: We DO NOT recommend enabling the root password for security reasons.
注意：安全起见，我们不推荐启用root密码。
To provide full privileges to the user, add the following to /etc/sudoers.d/ceph.
为赋予用户所有权限，把下列加入/etc/sudoers.d/ceph。
echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
sudo chmod 0440 /etc/sudoers.d/ceph
 3.2.2.4  配置SSH
Configure SSH
Configure your admin machine with password-less SSH access to each node running Ceph daemons (leave the passphrase empty).
配置你的管理主机，使之可通过SSH无密码访问各节点，口令留空。
ssh-keygen
Generating public/private key pair.
Enter file in which to save the key (/ceph-client/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-client/.ssh/id_rsa.
Your public key has been saved in /ceph-client/.ssh/id_rsa.pub.
Copy the key to each node running Ceph daemons:
把公钥拷贝到各节点：
ssh-copy-id ceph@ceph-server
Modify your ~/.ssh/config file of your admin node so that it defaults to logging in as the user you created when no username is specified.
修改你管理节点上的~/.ssh/config，以使未指定用户名时默认用你刚刚创建的用户名。
Host ceph-server
        Hostname ceph-server.fqdn-or-ip-address.com
        User ceph
 3.2.2.5  安装ceph-deploy
Install ceph-deploy
To install ceph-deploy, execute the following:
执行下列命令安装ceph-deploy：
wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
echo deb http://ceph.com/debian-cuttlefish/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update
sudo apt-get install ceph-deploy
 3.2.2.6  确认连通性
Ensure Connectivity
Ensure that your Admin node has connectivity to the network and to your Server node (e.g., ensure iptables, ufw or other tools that may prevent connections, traffic forwarding, etc. to allow what you need).
确保你的管理节点网络连接正常、且能连接服务器节点（如确认iptables、ufw或其它可阻断连接的工具、流量转发等等这些组件配置正常）。
Tip: The ceph-deploy tool is new and you may encounter some issues without effective error messages.
提示：ceph-deploy工具是全新的，所以你可能碰到一些问题，却没有用的提示。
Once you have completed this pre-flight checklist, you are ready to begin using ceph-deploy.
完成飞前检查后，你就可以开始用ceph-deploy部署了。
 3.2.3  软件包管理
Package Management
 3.2.3.1  安装
Install
To install Ceph packages on your cluster hosts, open a command line on your client machine and type the following:
要在集群主机上安装ceph软件包，在管理主机上打开命令行并执行下列命令：
ceph-deploy install {hostname [hostname] ...}
Without additional arguments, ceph-deploy will install the most recent stable Ceph package to the cluster host(s). To specify a particular package, you may select from the following:
没提供额外选项的话ceph-deploy默认会把最新稳定版安装到集群主机，要指定某个软件包可以用下列参数：
--stable <code-name>
--testing
--dev <branch-or-tag>
For example:
例如：
ceph-deploy install --stable cuttlefish hostname1
ceph-deploy install --testing hostname2
ceph-deploy install --dev wip-some-branch hostname{1,2,3,4,5}
For additional details, see Installing Debian/Ubuntu Packages. For additional usage, execute:
其他详情见 Installing Debian/Ubuntu Packages。其他用法见：
ceph-deploy install -h
 3.2.3.2  卸载
Uninstall
To uninstall Ceph packages from your cluster hosts, open a terminal on your admin host and type the following:
要从集群主机卸载ceph软件包，在管理主机的终端下执行：
ceph-deploy uninstall {hostname [hostname] ...}
On a Debian or Ubuntu system, you may also:
在Debian或Ubuntu系统上你也可以：
ceph-deploy purge {hostname [hostname] ...}
The tool will unininstall ceph packages from the specified hosts. Purge additionally removes configuration files.
此工具会从指定主机上卸载ceph软件包，另外purge会删除配置文件。
 3.2.4  创建一集群
Create a Cluster
The first step in using Ceph with ceph-deploy is to create a new Ceph cluster. A new Ceph cluster has:
使用ceph-deploy的第一步就是新建一个集群，新集群具备：
A Ceph configuration file, and
一个ceph配置文件，以及
A monitor keyring.
一个监视器密钥环。
The Ceph configuration file consists of at least:
ceph配置文件至少要包含：
Its own filesystem ID (fsid)
它自己的文件系统ID（fsid）
The initial monitor(s) hostname(s), and
最初的监视器（们）及其主机名（们），以及
The initial monitor(s) and IP address(es).
最初的监视器及其IP地址。
For additional details, see the Monitor Configuration Reference.
The ceph-deploy tool also creates a monitor keyring and populates it with a [mon.] key. For additional details, see the Cephx Guide.
详情见 Monitor Configuration Reference。
ceph-deploy工具也创建了一个监视器密钥环并置于[mon.]内，详情见 Cephx Guide。
 3.2.4.1  用法
Usage
To create a cluster with ceph-deploy, use the new command and specify the host(s) that will be initial members of the monitor quorum.
要用ceph-deploy创建集群，用new命令、并指定几个主机作为初始监视器法定人数。
ceph-deploy new {host [host], ...}
For example:
例如：
ceph-deploy new mon1.foo.com
ceph-deploy new mon{1,2,3}
The ceph-deploy utility will use DNS to resolve hostnames to IP addresses. The monitors will be named using the first component of the name (e.g., mon1 above). It will add the specified host names to the Ceph configuration file. For additional details, execute:
ceph-deploy工具会用DNS把主机名解析为IP地址。监视器将被命名为域名的第一段（如前述的mon1），它会把指定主机名加入ceph配置文件。其他用法见：
ceph-deploy new -h
 3.2.4.2  命名集群
Naming a Cluster
By default, Ceph clusters have a cluster name of ceph. You can specify a cluster name if you want to run multiple clusters on the same hardware. For example, if you want to optimize a cluster for use with block devies, and another for use with the gateway, you can run two different clusters on the same hardware if they have a different fsid and cluster name.
默认，ceph集群的名字为ceph，如果你想在同一套硬件上运行多套集群可以指定其他名字。比如，如果想优化一个集群用于块设备，另一个用作网关，你可以在同一套硬件上运行两个不同的集群，但是它们要配置不同的fsid和集群名。
ceph-deploy --cluster {cluster-name} new {host [host], ...}
For example:
例如：
ceph-deploy --cluster rbd-cluster new ceph-mon1
ceph-deploy --cluster rbd-cluster new ceph-mon{1,2,3}

Note: If you run multiple clusters, ensure you adjust the default port settings and open ports for your additional cluster(s) so that the networks of the two different clusters don’t conflict with each other.
注意：如果你运行多个集群，必须修改默认端口选项并为其打开端口，这样两个不同的集群网才不会相互冲突。
 3.2.5  增加/删除监视器
Add/Remove Monitors
With ceph-deploy, adding and removing monitors is a simple task. You just add or remove one or more monitors on the command line with one command. Before ceph-deploy, the process of adding and removing monitors involved numerous manual steps. Using ceph-deploy imposes a restriction: you may only install one monitor per host.
用ceph-deploy增加和删除监视器很简单，只要一个命令就可以增加或删除一或多个监视器。以前，增加和删除监视器涉及数个手动步骤。用ceph-deploy也预示着一个局限性，你只能在一主机上安装一个监视器。
Note: We do not recommend comingling monitors and OSDs on the same host.
注意：我们不建议把监视器和OSD置于同一主机上。
For high availability, you should run a production Ceph cluster with AT LEAST three monitors. Ceph uses the Paxos algorithm, which requires a consensus among the majority of monitors in a quorum. You can establish a monitor quorum with only one monitor; however, you can not determine a majority with two monitors. A majority of monitors must be counted as such: 1:1, 2:3, 3:4, 3:5, 4:6, etc.
考虑到高可用性，生产集群应该至少有3个监视器。ceph用Paxos算法，要求法定人数里的大多数达成一致。你可以只用一个监视器形成法定人数，然而你不能用两个监视器确定大多数。大多数监视器的比例必须像：1:1、2:3、3:4、3:5、4:6等等。
See Monitor Config Reference for details on configuring monitors.
关于监视器的配置见Monitor Config Reference。
 3.2.5.1  增加一监视器
Add a Monitor
Once you create a cluster and install Ceph packages to the monitor host(s), you may deploy the monitor(s) to the monitor host(s). When using ceph-deploy, the tool enforces a single monitor per host.
创建集群并在监视器主机上安装ceph软件包后，接着部署监视器。用ceph-deploy时，它限制一主机只能装一个监视器。
ceph-deploy mon create {host-name [host-name]...}

Note: Ensure that you add monitors such that they may arrive at a consensus among a majority of monitors.
注意：确保你增加的监视器能在大多数成员中达成一致。
 3.2.5.2  删除一监视器
Remove a Monitor
If you have a monitor in your cluster that you’d like to remove, you may use the destroy option.
如果你想删除集群中的某个监视器，可以用destroy选项。
ceph-deploy mon destroy {host-name [host-name]...}

Note: Ensure that if you remove a monitor, the remaining monitors will be able to establish a consensus. If that is not possible, consider adding a monitor before removing the monitor you would like to take offline.
注意：确保你删除一监视器后，其余监视器仍能达成一致。如果不可能，删除它之前先加一个。
 3.2.6  密钥管理
Keys Management
 3.2.6.1  收集密钥
Gather Keys
Before you can provision a host to run OSDs or metadata servers, you must gather monitor keys and the OSD and MDS bootstrap keyrings. To gather keys, enter the following:
在准备一台主机作为OSD或元数据服务器时，你得收集监视器、OSD、和MDS的初始密钥环，可用下列命令：
ceph-deploy gatherkeys {monitor-host}

Note: To retreive the keys, you specify a host that has a Ceph monitor.
注意：检索密钥时，指定一个包含ceph监视器的主机。
 3.2.6.2  忘记密钥
Forget Keys
When you are no longer using ceph-deploy (or if you are recreating a cluster), you should delete the keys in the local directory of your admin host. To delete keys, enter the following:
不再使用ceph-deploy（或另建一集群）时，你应该删除管理主机上、本地目录中的密钥。可用下列命令：
ceph-deploy forgetkeys
 3.2.7  增加/删除OSD
Add/Remove OSDs
Adding and removing Ceph OSD Daemons to your cluster may involve a few more steps when compared to adding and removing other Ceph daemons. Ceph OSD Daemons write data to the disk and to journals. So you need to provide a disk for the OSD and a path to the journal partition (i.e., this is the most common configuration, but you may configure your system to your own needs).
新增和拆除ceph的OSD进程相比另二种要多几步。OSD守护进程要向磁盘写数据和日志，所以你得相应地提供一硬盘和日志分区（这是最常见的配置，但你可以按需配置）。
By default, ceph-deploy will create an OSD with the XFS filesystem. You may override the filesystem type by providing a --fs-type FS_TYPE argument, where FS_TYPE is an alternate filesystem such as ext4 or btrfs.
默认情况下，ceph-deploy会创建基于XFS文件系统的OSD，但你可以用--fs-type FS_TYPE覆盖默认值，其中FS_TYPE是别种文件系统，如ext4或btrfs。
In Ceph v0.60 and later releases, Ceph supports dm-crypt on disk encryption. You may specify the --dm-crypt argument when preparing an OSD to tell ceph-deploy that you want to use encryption. You may also specify the --dmcrypt-key-dir argument to specify the location of dm-crypt encryption keys.
在ceph-v0.60及其后续发布，ceph可用dm-crypt加密，在准备OSD时你可以用--dm-crypt参数告诉ceph-deploy你想用加密功能。也可以用--dmcrypt-key-dir参数指定dm-crypt加密密钥的位置。
You should test various drive configurations to gauge their throughput before before building out a large cluster. See Data Storage for additional details.
在构建一个大型集群前，你应该测试各种驱动器配置来衡量其吞吐量。详情见Data Storage。
 3.2.7.1  列举磁盘
List Disks
To list the disks on a node, execute the following command:
执行下列命令列举一节点上的磁盘：
ceph-deploy disk list {node-name [node-name]...}
 3.2.7.2  擦净磁盘
Zap Disks
To zap a disk (delete its partition table) in preparation for use with Ceph, execute the following:
用下列命令擦净（删除分区表）磁盘，以用于ceph：
ceph-deploy disk zap {osd-server-name}:{disk-name}
ceph-deploy disk zap osdserver1:sdb

Important: This will delete all data.
重要：这会删除所有数据。
 3.2.7.3  准备OSD
Prepare OSDs
Once you create a cluster, install Ceph packages, and gather keys, you may prepare the OSDs and deploy them to the OSD node(s). If you need to identify a disk or zap it prior to preparing it for use as an OSD, see List Disks and Zap Disks.
创建集群、安装ceph软件包、收集密钥完成后你就可以准备OSD并把它们部署到OSD节点了。如果你想确认某磁盘或擦净它，参见 List Disks和Zap Disks。
ceph-deploy osd prepare {node-name}:{disk}[:{path/to/journal}]
ceph-deploy osd prepare osdserver1:sdb:/dev/ssd1
The prepare command only prepares the OSD. It does not activate it. To activate a prepared OSD, use the activate command. See Activate OSDs for details.
prepare命令只准备OSD，不会激活它。要激活一个已准备好的OSD，用activate命令，见Activate OSDs。
The foregoing example assumes a disk dedicated to one Ceph OSD Daemon, and a path to an SSD journal partition. We recommend storing the journal on a separate drive to maximize throughput. You may dedicate a single drive for the journal too (which may be expensive) or place the journal on the same disk as the OSD (not recommended as it impairs performance). In the foregoing example we store the journal on a partioned solid state drive.
前例假定一个硬盘只会用于一个OSD守护进程，以及一个到SSD日志分区的路径。我们建议把日志存储于另外的驱动器以最优化性能；你也可以指定一单独的驱动器用于日志（也许比较昂贵）、或者把日志放到OSD数据盘（不建议，因为它损伤性能）。前例中我们把日志存储于已分区的固态硬盘。
Note: When running multiple Ceph OSD daemons on a single node, and sharing a partioned journal with each OSD daemon, you should consider the entire node the minimum failure domain for CRUSH purposes, because if the SSD drive fails, all of the Ceph OSD daemons that journal to it will fail too.
注意：在一个节点运行多个OSD守护进程、且多个OSD守护进程共享一个日志分区时，你应该考虑整个节点的最小CRUSH故障域，因为如果这个SSD坏了，所有用其做日志的OSD守护进程也会失效。
 3.2.7.4  激活OSD
Activate OSDs
Once you prepare an OSD you may activate it with the following command.
准备好OSD后，可以用下列命令激活它。
ceph-deploy osd activate {node-name}:{path/to/disk}[:{path/to/journal}]
ceph-deploy osd activate osdserver1:/dev/sdb1:/dev/ssd1
The activate command will cause your OSD to come up and be placed in the cluster. The activate command uses the path to the partition created when running the prepare command.
activate命令会让OSD进入up且in状态，此命令所用路径和prepare相同。
 3.2.7.5  创建OSD
Create OSDs
You may prepare OSDs, deploy them to the OSD node(s) and activate them in one step with the create command. The create command is a convenience method for executing the prepare and activate command sequentially.
你可以用create命令一次完成准备OSD、部署到OSD节点、并激活它。create命令是依次执行prepare和activate命令的捷径。
ceph-deploy osd create {node-name}:{disk}[:{path/to/journal}]
ceph-deploy osd create osdserver1:sdb:/dev/ssd1
 3.2.7.6  拆除OSD
Destroy OSDs
Note: Coming soon. See Remove OSDs for manual procedures.
注意：稍后完成。手动过程见Remove OSDs。
 3.2.8  增加/拆除元数据服务器
Add/Remove Metadata Server
With ceph-deploy, adding and removing metadata servers is a simple task. You just add or remove one or more metadata servers on the command line with one command.
用ceph-deploy增加和拆除元数据服务器很简单，只要一个命令就可以增加或拆除一或多个元数据服务器。
Note: CephFS is in production using 1 metadata server per cluster. You MUST deploy at least one metadata server to use CephFS.
注意：生产环境下，每个CephFS集群只用一个元数据服务器，但你必须至少有一个才能用CephFS。
See MDS Config Reference for details on configuring metadata servers.
元数据服务器配置见 MDS Config Reference。
 3.2.8.1  增加一元数据服务器
Add a Metadata Server
Once you deploy monitors and OSDs you may deploy the metadata server(s).
部署完监视器和OSD后，还可以部署元数据服务器。
ceph-deploy mds create {host-name}[:{daemon-name}] [{host-name}[:{daemon-name}] ...]
You may specify a daemon instance a name (optional) if you would like to run multiple daemons on a single server.
如果你想在同一主机上运行多个守护进程，可以为每个进程指定名字（可选）。
 3.2.8.2  删除一元数据服务器
Remove a Metadata Server
Coming soon...
 3.2.9  清除一主机
Purge a Host
When you remove Ceph daemons and uninstall Ceph, there may still be extraneous data from the cluster on your server. The purge and purgedata commands provide a convenient means of cleaning up a host.
拆除ceph守护进程并卸载软件包后，此主机上可能还有集群中的数据，purge和purgedata命令提供了一种清理主机的简便方法。
 3.2.9.1  清除数据
Purge Data
To remove all data from /var/lib/ceph (but leave Ceph packages intact), execute the purgedata command.
要清理掉/var/lib/ceph下的所有数据（但保留ceph软件包），执行purgedata命令：
ceph-deploy purgedata {hostname} [{hostname} ...]
 3.2.9.2  Purge
To remove all data from /var/lib/ceph and uninstall Ceph packages, execute the purge command.
要清理掉/var/lib/ceph下的所有数据、并卸载ceph软件包，用purge命令。
ceph-deploy purge {hostname} [{hostname} ...]
 3.2.10  管理任务
Admin Tasks
Once you have set up a cluster with ceph-deploy, you may provide the client admin key and the Ceph configuration file to another host so that a user on the host may use the ceph command line as an administrative user.
用ceph-deploy建立一个集群后，你可以把客户端管理密钥和ceph配置文件发给其他管理员，以便让他用ceph命令管理集群。
 3.2.10.1  创建一管理主机
Create an Admin Host
To enable a host to execute ceph commands with administrator priveleges, use the admin command.
要允许一主机以管理员权限执行ceph命令，用admin命令：
ceph-deploy admin {host-name [host-name]...}
 3.2.10.2  分发配置文件
Deploy Config File
To send an updated copy of the Ceph configuration file to hosts in your cluster, use the config push command.
要把改过的配置文件分发给集群内各主机，可用config push命令。
ceph-deploy config push {host-name [host-name]...}

Tip: With a base name and increment host-naming convention, it is easy to deploy configuration files via simple scripts (e.g., ceph-deploy config hostname{1,2,3,4,5}).
提示：用相同前缀和递增数字命名的话，部署可以通过一个命令轻易完成，如ceph-deploy config hostname{1,2,3,4,5}
 3.2.10.3  检索配置文件
Retrieve Config File
To retrieve a copy of the Ceph configuration file from a host in your cluster, use the config pull command.
要从集群内的一主机检索配置文件，用config pull命令。
ceph-deploy config pull {host-name [host-name]...}
 3.2.11  用mkcephfs部署（已过时）
Deploying with mkcephfs
The mkcephfs tool is the old method of deploying new Ceph clusters. It is now deprecated in favor of ceph-deploy, which has better support for modifying an existing cluster.
mkcephfs是以前的ceph集群部署方法，现在淘汰了，转移到了ceph-deploy，它能更好地修改现有集群。
 3.2.11.1  允许root登录集群主机
ENABLE LOGIN TO CLUSTER HOSTS AS ROOT
To deploy with mkcephfs, you will need to be able to login as root on each host without a password. For each host, perform the following:
用mkcephfs配置，需要以root身份登录且不需要密码，在每台主机上执行下面的命令：
sudo passwd root
Enter a password for the root user.
给root设置一个密码。
On the admin host, generate an ssh key without specifying a passphrase and use the default locations.
在管理主机上生成一个ssh密钥，不要指定通行码，密钥文件使用默认位置。
sudo -i
ssh-keygen
Generating public/private key pair.
Enter file in which to save the key (/ceph-admin/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-admin/.ssh/id_rsa.
Your public key has been saved in /ceph-admin/.ssh/id_rsa.pub.
Modify your ~/.ssh/config file to login as root, as follows:
更改你的~/.ssh/config文件来登录为root，如下：
Host myserver01
        Hostname myserver01.fully-qualified-domain.com
        User root
Host myserver02
        Hostname myserver02.fully-qualified-domain.com
        User root
You may use RSA or DSA keys. Once you generate your keys, copy them to each OSD host. For example:
你可以用RSA或DSA密钥，生成后把公钥拷贝到每个OSD主机。例如：
ssh-copy-id root@myserver01
ssh-copy-id root@myserver02
 3.2.11.2  把配置文件复制到所有节点
COPY CONFIGURATION FILE TO ALL HOSTS
Ceph’s mkcephfs deployment script does not copy the configuration file you created from the Administration host to the OSD Cluster hosts. Copy the configuration file you created (i.e., mycluster.conf in the example below) from the Administration host to etc/ceph/ceph.conf on each OSD Cluster host if you are using mkcephfs to deploy Ceph.
ceph的mkcephfs脚本不会把管理主机上创建的配置文件拷贝到OSD主机，所以你得手动把配置文件（如下例中的mycluster.conf）复制到OSD主机的/etc/ceph/ceph.conf。
ssh myserver01 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
ssh myserver02 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
ssh myserver03 sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
 3.2.11.3  创建默认目录
CREATE THE DEFAULT DIRECTORIES
The mkcephfs deployment script does not create the default server directories. Create server directories for each instance of a Ceph daemon (if you haven’t done so already). The host variables in the ceph.conf file determine which host runs each instance of a Ceph daemon. Using the exemplary ceph.conf file, you would perform the following:
mkcephfs配置脚本不会创建默认服务器目录（貌似现在的版本可以），要为每个ceph进程手动创建目录。ceph.conf配置文件里的host变量指定哪个主机运行哪个进程。用ceph.conf样板文件的话你要执行下面的命令：

On myserver01:
sudo mkdir /var/lib/ceph/osd/ceph-0
sudo mkdir /var/lib/ceph/mon/ceph-a

On myserver02:
sudo mkdir /var/lib/ceph/osd/ceph-1
sudo mkdir /var/lib/ceph/mon/ceph-b

On myserver03:
sudo mkdir /var/lib/ceph/osd/ceph-2
sudo mkdir /var/lib/ceph/mon/ceph-c
sudo mkdir /var/lib/ceph/mds/ceph-a
 3.2.11.4  把硬盘挂载到数据目录
MOUNT DISKS TO THE DATA DIRECTORIES
If you are running multiple OSDs per host and one hard disk per OSD, you should mount the disk under the OSD data directory (if you haven’t done so already). When mounting disks in this manner, there is no need for an entry in /etc/fstab.
如果你在每台主机上都要运行多个OSD进程，应该先把这些硬盘分别挂载到其数据目录下。用这种方式挂载硬盘时，/etc/fstab里的条目无所谓。
New in version 0.56.
For Bobtail (v 0.56) and beyond, you may specify the file system type, filesystem options, and mount options. Add the following to the [global] section of your Ceph configuration file, and replace the values in braces with appropriate values:
bobtail (0.56)版之后，你可以指定文件系统类型、创建选项、和挂载选项。把下列这段加到配置文件的[global]段下，并替换掉花括号里的内容：
osd mkfs type = {fs-type}
osd mkfs options {fs-type} = {mkfs options}   # default for xfs is "-f"
osd mount options {fs-type} = {mount options} # default mount option is "rw, noatime"
For example:
例如：
osd mkfs type = btrfs
osd mkfs options btrfs = -m raid0
osd mount options btrfs = rw, noatime
For each [osd.n] section of your configuration file, specify the storage device. For example:
配置文件里的每个[osd.n]段下给指定一个存储设备，例如：
[osd.1]
        devs = /dev/sda
[osd.2]
        devs = /dev/sdb
 3.2.11.5  运行mkcephfs
RUN MKCEPHFS
Once you have copied your Ceph Configuration to the OSD Cluster hosts and created the default directories, you may deploy Ceph with the mkcephfs script.
配置文件拷贝完毕、默认目录创建完毕后就可以用mkcephfs配置ceph集群了。
Note: mkcephfs is a quick bootstrapping tool. It does not handle more complex operations, such as upgrades.
注意：mkcephfs是快速起步工具，它不能处理复杂的操作，如升级。

To run mkcephfs, execute the following:
mkcephfs命令的用法如下：
cd /etc/ceph
sudo mkcephfs -a -c /etc/ceph/ceph.conf -k ceph.keyring

Note: For mkcephfs to use the mkfs configuration options, you MUST specify a devs entry for each OSD.
注意：要让mkcephfs使用mkfs配置选项，必须给每个OSD指定devs。
The script adds an admin key to the ceph.keyring, which is analogous to a root password. See Authentication when running with cephx enabled. To start the cluster, execute the following:
这个脚本把一个管理密钥添加到了ceph.keyring里，它和root密码的功能类似。启用了cephx时参见认证部分。执行下列命令启动集群：
service ceph -a start

See Operating a Cluster for details. Also see man mkcephfs.
详情参见错误：引用源未找到。
 3.3  运维
Cluster Operations
高级操作
HIGH-LEVEL OPERATIONS
High-level cluster operations consist primarily of starting, stopping, and restarting a cluster with the ceph service; checking the cluster’s health; and, monitoring an operating cluster.
高级集群操作主要包括用ceph服务管理脚本启动、停止、重启集群，和集群健康状态检查、监控和操作集群。
Operating a Cluster
Monitoring a Cluster
Monitoring OSDs and PGs
Authentication Overview
Cephx Authentication
数据归置
Data Placement
Once you have your cluster up and running, you may begin working with data placement. Ceph supports petabyte-scale data storage clusters, with storage pools and placement groups that distribute data across the cluster using Ceph’s CRUSH algorithm.
你的集群开始运行后，就可以尝试数据归置了。Ceph是PB级数据存储集群，它用CRUSH算法、靠存储池和归置组在集群内分布数据。
Data Placement Overview
Pools
Placement Groups
CRUSH Maps
低级运维
Low-level Operations
Low-level cluster operations consist of starting, stopping, and restarting a particular daemon within a cluster; changing the settings of a particular daemon or subsystem; and, adding a daemon to the cluster or removing a daemon from the cluster. The most common use cases for low-level operations include growing or shrinking the Ceph cluster and replacing legacy or failed hardware with new hardware.
低级集群运维包括启动、停止、重启集群内的某个具体守护进程；更改某守护进程或子系统配置；增加或拆除守护进程。低级运维的一个常见情形包括扩展、缩减ceph集群，以及更换损坏硬件。
Adding/Removing OSDs
Adding/Removing Monitors
Command Reference
故障排除
Troubleshooting
Ceph is still on the leading edge, so you may encounter situations that require you to evaluate your Ceph configuration and modify your logging and debugging settings to identify and remedy issues you are encountering with your cluster.
ceph仍在积极开发中，所以你可能碰到一些问题，需要评估ceph配置文件、并修改日志和调试选项来纠正它。
The Ceph Community
Recovering from Monitor Failures
Troubleshooting OSDs
Troubleshooting PGs
Logging and Debugging
CPU Profiling
Memory Profiling
 3.3.1  操纵集群
Operating a cluster
 3.3.1.1  用upstart控制ceph
Running Ceph with Upstart
When deploying Ceph Cuttlefish and beyond with ceph-deploy, you may start and stop Ceph daemons or the entire cluster using the event-based Upstart. Upstart does not require you to define daemon instances in the Ceph configuration file (although, they are still required for sysvinit should you choose to use it).
用ceph-deploy部署完ceph Cuttlefish及更高版本后，你可以用基于事件的Upstart来启动、关闭整个集群。upstart不要求你在配置文件里定义守护进程例程，然而sysvinit仍需要。
To list the Ceph Upstart jobs and instances, execute:
用下列命令列出ceph作业和例程：
sudo initctl list | grep ceph
See initctl for additional details.
详情参见initctl。
 3.3.1.1.1  启动集群
Starting a Cluster
To start the cluster, execute the following:
要启动集群，用下列命令：
sudo start ceph-all
 3.3.1.1.2  关闭集群
Stopping a Cluster
To stop the cluster, execute the following:
要停止集群，用下列命令：
sudo stop ceph-all
 3.3.1.1.3  启动一类进程
Starting all Daemons by Type
To start all daemons of a particular type, execute one of the following:
要启动某一类守护进程，用下列命令：
sudo start ceph-osd-all
sudo start ceph-mon-all
sudo start ceph-mds-all
 3.3.1.1.4  停止一类进程
Stopping all Daemons by Type
To stop all daemons of a particular type, execute one of the following:
要停止某一类守护进程，用下列命令：
sudo stop ceph-osd-all
sudo stop ceph-mon-all
sudo stop ceph-mds-all
 3.3.1.1.5  启动一个进程
Starting a Daemon
To start a specific daemon instance, execute one of the following:
要启动一指定守护进程例程，用下列命令之一：
sudo start ceph-osd id={id}
sudo start ceph-mon id={hostname}
sudo start ceph-mds id={hostname}
For example:
例如：
sudo start ceph-osd id=1
sudo start ceph-mon id=ceph-server
sudo start ceph-mds id=ceph-server
 3.3.1.1.6  停止一个进程
Stopping a Daemon
To stop a specific daemon instance, execute one of the following:
要停止一指定守护进程例程，用下列命令之一：
sudo stop ceph-osd id={id}
sudo stop ceph-mon id={hostname}
sudo stop ceph-mds id={hostname}
For example:
例如：
sudo stop ceph-osd id=1
sudo start ceph-mon id=ceph-server
sudo start ceph-mds id=ceph-server
 3.3.1.2  以服务运行ceph
Running Ceph as a Service
When you deploy Ceph Argonaut or Bobtail with mkcephfs, use the service or traditional sysvinit.
你用mkcephfs部署了ceph的Argonaut或Bobtail版时，用服务或传统的sysvinit（控制ceph）。
The ceph service provides functionality to start, restart, and stop your Ceph cluster. Each time you execute ceph processes, you must specify at least one option and one command. You may also specify a daemon type or a daemon instance. For most newer Debian/Ubuntu distributions, you may use the following syntax:
ceph服务提供了启动、重启、停止ceph集群的功能，每次执行的时候必须指定至少一个选项和一个命令，还必须指定一个守护进程类型或例程名称。对最新的Debian/Ubuntu发行版，你可以用下面的语法：
sudo service ceph [options] [commands] [daemons]
For older distributions, you may wish to use the /etc/init.d/ceph path:
对较老的发行版，你也许想用/etc/init.d/ceph：
sudo /etc/init.d/ceph [options] [commands] [daemons]
The ceph service options include:
ceph服务的选项包括：

Option
Shortcut
Description
--verbose
-v
Use verbose logging.
详细的日志。
--valgrind
N/A
(Dev and QA only) Use Valgrind debugging.
（只适用于开发者和品质保证人员）使用Valgrind调试。
--allhosts
-a
Execute on all hosts in ceph.conf. Otherwise, it only executes on localhost.
在ceph.conf里配置的所有主机上执行，否则它只在本机执行。
--restart
N/A
Automatically restart daemon if it core dumps.
核心转储后自动重启。
--norestart
N/A
Don’t restart a daemon if it core dumps.
核心转储后不自动重启。
--conf
-c
Use an alternate configuration file.
使用另外一个配置文件。

The ceph service commands include:
ceph服务的命令包括：
Command
Description
start
Start the daemon(s).
stop
Stop the daemon(s).
forcestop
Force the daemon(s) to stop. Same as kill -9
killall
Kill all daemons of a particular type.
cleanlogs
Cleans out the log directory.
cleanalllogs
Cleans out everything in the log directory.
For subsystem operations, the ceph service can target specific daemon types by adding a particular daemon type for the [daemons] option. Daemon types include:
至于子系统操作，ceph服务能指定守护进程类型，在[daemons]处指定守护进程类型就行了，守护进程类型包括：
mon
osd
mds
The ceph service’s [daemons] setting may also target a specific instance:
ceph服务的[daemons]设置也可以指定一个具体例程：
To start a Ceph daemon on the local Ceph Node, use the following syntax:
要启动本地ceph节点上的守护进程，用下列语法：
sudo /etc/init.d/ceph start osd.0
To start a Ceph daemon on another node, use the following syntax:
要启动另外一节点上的ceph守护进程，依下列语法：
sudo /etc/init.d/ceph -a start osd.0
Where osd.0 is the first OSD in the cluster.
这里osd.0是集群里的第一个OSD。
 3.3.1.2.1  启动集群
STARTING A CLUSTER
To start your Ceph cluster, execute ceph with the start command. The usage may differ based upon your Linux distribution. For example, for most newer Debian/Ubuntu distributions, you may use the following syntax:
要启动ceph集群，执行ceph的时候加上start命令，用法可能因你的Linux发行版而有所不同，例如对大多数较新的Debian/Ubuntu你可以用下面的语法：
sudo service ceph start [options] [start|restart] [daemonType|daemonID]
For older distributions, you may wish to use the /etc/init.d/ceph path:
对较老的发行版可能要用/etc/init.d/ceph：
sudo /etc/init.d/ceph [options] [start|restart] [daemonType|daemonID]
The following examples illustrates a typical use case:
下面的命令展示了典型用法：
sudo service ceph -a start
sudo /etc/init.d/ceph -a start
Once you execute with -a (i.e., execute on all nodes), Ceph should begin operating. You may also specify a particular daemon instance to constrain the command to a single instance. To start a Ceph daemon on the local Ceph Node, use the following syntax:
使用-a（在所有节点上执行）选项可操作整个集群，你也可以指定一个具体例程把操作限定到某一个例程，例如要启动本地节点的一个例程，按下列语法：
sudo /etc/init.d/ceph start osd.0
To start a Ceph daemon on another node, use the following syntax:
要启动其他节点上的一个例程，按下列语法：
sudo /etc/init.d/ceph -a start osd.0
 3.3.1.2.2  停止集群
Stopping a Cluster
To stop your Ceph cluster, execute ceph with the stop command. The usage may differ based upon your Linux distribution. For example, for most newer Debian/Ubuntu distributions, you may use the following syntax:
要停止ceph集群，可以在执行ceph时加上stop命令，用法因Linux发行版不同而有所差异。例如，在最新的Debian/Ubuntu上可以用下面的语法：
sudo service ceph [options] stop [daemonType|daemonID]
For example:
例如：
sudo service ceph -a stop
For older distributions, you may wish to use the /etc/init.d/ceph path:
在较老的发行版上可以用/etc/init.d/ceph：
sudo /etc/init.d/ceph -a stop
Once you execute with -a (i.e., execute on all nodes), Ceph should shut down. You may also specify a particular daemon instance to constrain the command to a single instance. To stop a Ceph daemon on the local Ceph Node, use the following syntax:
执行命令时一旦加了-a（意为在所有节点执行），整个ceph集群都会关闭。你也可以加选项把操作限定到某个具体例程。要停止本地节点上的一个守护进程，按下列语法：
sudo /etc/init.d/ceph stop osd.0
To stop a Ceph daemon on another node, use the following syntax:
要停止别处守护进程，按下列语法：
sudo /etc/init.d/ceph -a stop osd.0
 3.3.2  监控集群
Monitoring a cluster
Once you have a running cluster, you may use the ceph tool to monitor your cluster. Monitoring a cluster typically involves checking OSD status, monitor status, placement group status and metadata server status.
集群运行起来后，你可以用ceph工具来监控，典型的监控包括检查OSD状态、监视器状态、归置组状态和元数据服务器状态。
 3.3.2.1  交互模式
Interactive Mode
To run the ceph tool in interactive mode, type ceph at the command line with no arguments. For example:
要在交互模式下运行ceph，不要带参数运行ceph，例如：
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status
 3.3.2.2  检查集群健康状况
CHECKING CLUSTER HEALTH
After you start your cluster, and before you start reading and/or writing data, check your cluster’s health first. You can check on the health of your Ceph cluster with the following:
启动集群后、读写数据前，先检查下集群的健康状态。你可以用下面的命令检查：
ceph health
If you specified non-default locations for your configuration or keyring, you may specify their locations:
如果你的ceph.conf或密钥环不在默认路径下，你得指定：
ceph -c /path/to/conf -k /path/to/keyring health
Upon starting the Ceph cluster, you will likely encounter a health warning such as HEALTH_WARN XXX num placement groups stale. Wait a few moments and check it again. When your cluster is ready, ceph health should return a message such as HEALTH_OK. At that point, it is okay to begin using the cluster.
集群起来的时候，你也许会碰到像 HEALTH_WARN XXX num placement groups stale这样的健康告警，等一会再检查下。集群准备好的话ceph health会给出像 HEALTH_OK一样的消息，这时候就可以开始使用集群了。
 3.3.2.3  观察集群
Watching a Cluster
To watch the cluster’s ongoing events, open a new terminal. Then, enter:
要观察集群内正发生的事件，打开一个新终端，然后输入：
ceph -w
Ceph will print each version of the placement group map and their status. For example, a tiny Ceph cluster consisting of one monitor, one metadata server and two OSDs may print the following:
ceph会打印每个归置组版本和它们的状态，例如一个包括1个监视器、1个元数据服务器和2个OSD的小型ceph集群可能会打印下面的：
health HEALTH_OK
monmap e1: 1 mons at {a=192.168.0.1:6789/0}, election epoch 0, quorum 0 a
osdmap e13: 2 osds: 2 up, 2 in
 placement groupmap v9713: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
mdsmap e4: 1/1/1 up {0=a=up:active}

     2012-08-01 11:33:53.831268 mon.0 [INF] placement groupmap v9712: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
     2012-08-01 11:35:31.904650 mon.0 [INF] placement groupmap v9713: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
     2012-08-01 11:35:53.903189 mon.0 [INF] placement groupmap v9714: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
     2012-08-01 11:37:31.865809 mon.0 [INF] placement groupmap v9715: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
 3.3.2.4  检查集群状态
CHECKING A CLUSTER’S STATUS
To check a cluster’s status, execute the following:
要检查集群的状态，执行下面的命令：
ceph status
或者：
ceph -s
In interactive mode, type status and press Enter.
在交互模式下，输入status然后按回车：
ceph> status
Ceph will print the cluster status. For example, a tiny Ceph cluster consisting of one monitor, one metadata server and two OSDs may print the following:
ceph将打印集群状态，例如一个包括1个监视器、1个元数据服务器和2个OSD的小型ceph集群可能打印：
health HEALTH_OK
monmap e1: 1 mons at {a=192.168.0.1:6789/0}, election epoch 0, quorum 0 a
osdmap e13: 2 osds: 2 up, 2 in
 placement groupmap v9754: 384 placement groups: 384 active+clean; 8730 bytes data, 22948 MB used, 264 GB / 302 GB avail
mdsmap e4: 1/1/1 up {0=a=up:active}
 3.3.2.5  检查OSD状态
CHECKING OSD STATUS
You can check OSDs to ensure they are up and in by executing:
你可以执行下列命令来确定OSD状态为up且in：
ceph osd stat
或者：
ceph osd dump
You can also check view OSDs according to their position in the CRUSH map.
你也可以根据OSD在CRUSH图里的位置来查看：
ceph osd tree
Ceph will print out a CRUSH tree with a host, its OSDs, whether they are up and their weight.
ceph会打印CRUSH的树状态、它的OSD例程、状态、权重：
# id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1
For a detailed discussion, refer to Monitoring OSDs and Placement Groups.
 3.3.2.6  检查监视器状态
CHECKING MONITOR STATUS
If your cluster has multiple monitors (likely), you should check the monitor quorum status after you start the cluster before reading and/or writing data. A quorum must be present when multiple monitors are running. You should also check monitor status periodically to ensure that they are running.
如果你有多个监视器（很可能），你启动集群后、读写数据前应该检查监视器法定人数状态。多个监视器必须活着、且在运行，要周期性检查监视器状态来确定它们在运行：
To see display the monitor map, execute the following:
要查看监视器图，执行下面的命令：
ceph mon stat
或者：
ceph mon dump
To check the quorum status for the monitor cluster, execute the following:
要检查监视器的法定人数状态，执行下面的命令：
ceph quorum_status
Ceph will return the quorum status. For example, a Ceph cluster consisting of three monitors may return the following:
ceph会返回法定人数状态，例如，包含3个监视器的ceph集群可能返回下面的：
{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}
 3.3.2.7  检查MDS状态
CHECKING MDS STATUS
Metadata servers provide metadata services for Ceph FS. Metadata servers have two sets of states: up | down and active | inactive. To ensure your metadata servers are up and active, execute the following:
元数据服务器为ceph文件系统提供元数据服务，元数据服务器有两种状态：up|down和active|inactive，执行下面的命令来确保元数据服务器up且active：
ceph mds stat
To display details of the metadata cluster, execute the following:
要展示元数据集群的详细状态，执行下面的命令：
ceph mds dump
 3.3.2.8  检查归置组状态
CHECKING PLACEMENT GROUP STATES
Placement groups map objects to OSDs. When you monitor your placement groups, you will want them to be active and clean. For other PG states, see Monitoring OSDs and Placement Groups.
归置组把对象映射到OSD，归置组的状态应该是active且clean，其它的PG状态请参见Monitoring OSDs and Placement Groups.。
 3.3.2.9  使用管理套接字
Using the Admin Socket
The Ceph admin socket allows you to query a daemon via a socket interface. By default, Ceph sockets reside under /var/run/ceph. To access a socket, use the following command:
ceph管理套接字允许你通过socket接口查询守护进程。它们默认存在于/var/run/ceph下，用下列命令访问套接字：
ceph --admin-daemon /var/run/ceph/{socket-name}
To view the available admin socket commands, execute the following command:
用下列命令查看可用的管理套接字命令：
ceph --admin-daemon /var/run/ceph/{socket-name} help
The admin socket command enables you to show and set your configuration at runtime. See Viewing a Configuration at Runtime for details.
管理套接字命令允许你在运行时查看和修改配置，见Viewing a Configuration at Runtime。
Additionally, you can set configuration values at runtime directly (i.e., the admin socket bypasses the monitor, unlike ceph {daemon-type} tell {id} injectargs, which relies on the monitor but doesn’t require you to login directly to the host in question ).
另外，你可以在运行时直接修改配置选项。管理套接字会绕过监视器，不要求你直接登录主机，不像ceph {daemon-type} tell {id} injectargs依赖监视器。
 3.3.3  监控OSD和归置组
Monitoring OSDs and PGs
High availability and high reliability require a fault-tolerant approach to managing hardware and software issues. Ceph has no single point-of-failure, and can service requests for data in a “degraded” mode. Ceph’s data placement introduces a layer of indirection to ensure that data doesn’t bind directly to particular OSD addresses. This means that tracking down system faults requires finding the placement group and the underlying OSDs at root of the problem.
高可用性和高可靠性要求容错方法来管理软硬件。ceph没有单故障点，并且能在“降级”模式下继续提供服务。其数据归置引进了一个间接层，它可保证数据不会直接绑死到某一个特定OSD地址，这也意味着追踪系统错误的根源得深入归置组及底层的OSD。
Tip: A fault in one part of the cluster may prevent you from accessing a particular object, but that doesn’t mean that you can’t access other objects. When you run into a fault, don’t panic. Just follow the steps for monitoring your OSDs and placement groups. Then, begin troubleshooting.
提示：集群某一部分失效可能导致不能访问某个对象，但不会牵连其他对象。碰到这种问题时无需恐慌，只需按步骤检查OSD和归置组，然后排除故障。
Ceph is generally self-repairing. However, when problems persist, monitoring OSDs and placement groups will help you identify the problem.
ceph通常能自己康复，然而如果故障持续存在，监控OSD和归置组有助于找出问题所在。
 3.3.3.1  监控OSD
Monitoring OSDs
An OSD’s status is either in the cluster (in) or out of the cluster (out); and, it is either up and running (up), or it is down and not running (down). If an OSD is up, it may be either in the cluster (you can read and write data) or it is out of the cluster. If it was in the cluster and recently moved out of the cluster, Ceph will migrate placement groups to other OSDs. If an OSD is out of the cluster, CRUSH will not assign placement groups to the OSD. If an OSD is down, it should also be out.
某OSD的状态可以是在集群内(in)或集群外(out)、也可以是活着且在运行(up)或挂了且不在运行(down)。如果一个OSD活着，它也可以是in（你可以读写数据）或者out集群。如果它以前是in但最近out了，ceph会把其归置组迁移到其他OSD。如果一OSD out了，CRUSH就不会再分配归置组给它。如果它挂了（down）其状态也应该是out。
Note: If an OSD is down and in, there is a problem and the cluster will not be in a healthy state.
注意：如果一OSD状态为down且in，必定有问题，而且集群处于非健康状态。


If you execute a command such as ceph health, ceph -s or ceph -w, you may notice that the cluster does not always echo back HEALTH OK. Don’t panic. With respect to OSDs, you should expect that the cluster will NOT echo HEALTH OK in a few expected circumstances:
如果你执行过这些命令，如ceph health、ceph -s、或ceph -w，也许注意到了，集群并非一直返回HEALTH OK，别紧张。就OSD而言你应该明确，在一些情况下集群不会返回HEALTH OK。
1. You haven’t started the cluster yet (it won’t respond).
你还没启动集群（它不会响应的）。
2. You have just started or restarted the cluster and it’s not ready yet, because the placement groups are getting created and the OSDs are in the process of peering.
你刚刚启动或重启完集群，而且它还没准备好，因为归置组正被创建、OSD们正在相互建立连接。
3. You just added or removed an OSD.
你刚刚增加或拆除一个OSD。
4. You just have modified your cluster map.
你刚刚修改完集群运行图。
An important aspect of monitoring OSDs is to ensure that when the cluster is up and running that all OSDs that are in the cluster are up and running, too. To see if all OSDs are running, execute:
OSD监控的一个重要事情是，当集群启动并运行时，所有OSD也应该是启动(up)并在集群内(in)运行。用下列命令查看：
ceph osd stat
The result should tell you the map epoch (eNNNN), the total number of OSDs (x), how many are up (y) and how many are in (z).
其结果会告诉你运行图版本(eNNNN)、OSD总数(x)、y个是up的、z个是in的。
eNNNN: x osds: y up, z in
If the number of OSDs that are in the cluster is more than the number of OSDs that are up, execute the following command to identify the ceph-osd daemons that aren’t running:
如果处于in状态的OSD多于up的，用下列命令看看哪些ceph-osd守护进程没在运行：
ceph osd tree

dumped osdmap tree epoch 1
# id    weight  type name       up/down reweight
-1      2       pool openstack
-3      2               rack dell-2950-rack-A
-2      2                       host dell-2950-A1
0       1                               osd.0   up      1
1       1                               osd.1   down    1

Tip: The ability to search through a well-designed CRUSH hierarchy may help you troubleshoot your cluster by identifying the physcial locations faster.
提示：精心设计的CRUSH分级结构可以帮你更快的定位到物理位置、加快故障排除。
If an OSD is down, start it:
若一个OSD是down的，启动它：
sudo /etc/init.d/ceph -a start osd.1
See OSD Not Running for problems associated with OSDs that stopped, or won’t restart.
和OSD没运行或不启动相关的问题请看OSD Not Running。
 3.3.3.2  归置组集
PG Sets
When CRUSH assigns placement groups to OSDs, it looks at the number of replicas for the pool and assigns the placement group to OSDs such that each replica of the placement group gets assigned to a different OSD. For example, if the pool requires three replicas of a placement group, CRUSH may assign them to osd.1, osd.2 and osd.3 respectively. CRUSH actually seeks a pseudo-random placement that will take into account failure domains you set in your CRUSH map, so you will rarely see placement groups assigned to nearest neighbor OSDs in a large cluster. We refer to the set of OSDs that should contain the replicas of a particular placement group as the Acting Set. In some cases, an OSD in the Acting Set is down or otherwise not able to service requests for objects in the placement group. When these situations arise, don’t panic. Common examples include:
CRUSH要把归置组分配到OSD时，它先查询这个存储池的副本数设置，再把归置组分配到OSD，这样就把各副本分配到了不同OSD。比如，如果存储池要求归置组有3个副本，CRUSH可能把它们分别分配到osd.1、osd.2、osd.3。考虑到你设置于CRUSH运行图中的失败域，实际上CRUSH找出的是伪随机位置，所以在大型集群中，你很少能看到归置组被分配到了相邻的OSD。我们把涉及某个特定归置组副本的一组OSD称为acting set。在一些情况下，位于acting set中的一个OSD down了或者不能为归置组内的对象提供服务，这些情形发生时无需惊慌，常见原因如下：
You added or removed an OSD. Then, CRUSH reassigned the placement group to other OSDs–thereby changing the composition of the Acting Set and spawning the migration of data with a “backfill” process.
你增加或拆除了一OSD。然后CRUSH把那个归置组分配到了其他OSD，因此改变了Acting Set的构成、并且用backfill进程启动了数据迁移。
An OSD was down, was restared, and is now recovering.
一OSD down了、重启了、而现在正恢复（recovering）。
An OSD in the Acting Set is down or unable to service requests, and another OSD has temporarily assumed its duties.
acting set中的一个OSD挂了，不能提供服务，另一个OSD临时接替其工作。
Ceph processes a client request using the Up Set, which is the set of OSDs that will actually handle the requests. In most cases, the Up Set and the Acting Set are virtually identical. When they are not, it may indicate that Ceph is migrating data, an OSD is recovering, or that there is a problem (i.e., Ceph usually echoes a “HEALTH WARN” state with a “stuck stale” message in such scenarios).
ceph靠up set处理客户端请求，它们是最终处理请求的一系列OSD。大多数情况下up set和acting set本质上相同，如果不同，说明可能ceph在迁移数据、某OSD在恢复、或者哪里有问题。这种情况下，ceph通常表现为HEALTH WARN状态，还有"stuck stale"消息。
To retrieve a list of placement groups, execute:
用下列命令获取归置组列表：
ceph pg dump
To view which OSDs are within the Acting Set or the Up Set for a given placement group, execute:
要根据指定归置组号查看哪些OSD位于Acting Set或Up Set里，执行：
ceph pg map {pg-num}
The result should tell you the osdmap epoch (eNNN), the placement group number ({pg-num}), the OSDs in the Up Set (up[]), and the OSDs in the acting set (acting[]).
其结果会告诉你osdmap版本（eNNN）、归置组号（{pg-num}）、Up Set内的OSD（up[]）、和Acting Set内的OSD（acting[]）。
osdmap eNNN pg {pg-num} -> up [0,1,2] acting [0,1,2]

Note: If the Up Set and Acting Set do not match, this may be an indicator that the cluster rebalancing itself or of a potential problem with the cluster.
注意：如果Up Set和Acting Set不一致，这可能表明集群内部在重均衡或者有潜在问题。
 3.3.3.3  节点互联
Peering
Before you can write data to a placement group, it must be in an active state, and it should be in a clean state. For Ceph to determine the current state of a placement group, the primary OSD of the placement group (i.e., the first OSD in the acting set), peers with the secondary and tertiary OSDs to establish agreement on the current state of the placement group (assuming a pool with 3 replicas of the PG).
写入数据前，归置组必须处于active、而且应该是clean状态。假设一存储池的归置组有3个副本，为让ceph确定归置组的当前状态，一归置组的主OSD（如acting set内的第一个OSD）会与第二和第三OSD建立连接、并就归置组的当前状态达成一致意见。

The OSDs also report their status to the monitor. See Configuring Monitor/OSD Interaction for details. To troubleshoot peering issues, see Peering Failure.
OSD们也向监视器报告自己的状态，详情见Configuring Monitor/OSD Interaction。要排除连接建立问题，参见Peering Failure。
 3.3.3.4  归置组状态的监控
Monitoring Placement Group States
If you execute a command such as ceph health, ceph -s or ceph -w, you may notice that the cluster does not always echo back HEALTH OK. After you check to see if the OSDs are running, you should also check placement group states. You should expect that the cluster will NOT echo HEALTH OK in a number of placement group peering-related circumstances:
如果你执行过ceph health、ceph -s、或ceph -w命令，你也许注意到了集群并非总返回HEALTH OK。检查完OSD是否在运行后，你还应该检查归置组状态。你应该明白，在归置组建立连接时集群不会返回HEALTH OK：
1. You have just created a pool and placement groups haven’t peered yet.
2. The placement groups are recovering.
3. You have just added an OSD to or removed an OSD from the cluster.
4. You have just modified your CRUSH map and your placement groups are migrating.
5. There is inconsistent data in different replicas of a placement group.
6. Ceph is scrubbing a placement group’s replicas.

1. 刚刚创建了一个存储池，归置组还没建立连接；
2. 归置组正在恢复；
3. 刚刚增加或删除了一个OSD；
4. 刚刚修改了CRUSH图，并且归置组正在迁移；
5. 某一归置组的副本间的数据不一致；
6. ceph正在洗刷一个归置组的副本；
If one of the foregoing circumstances causes Ceph to echo HEALTH WARN, don’t panic. In many cases, the cluster will recover on its own. In some cases, you may need to take action. An important aspect of monitoring placement groups is to ensure that when the cluster is up and running that all placement groups are active, and preferably in the clean state. To see the status of all placement groups, execute:
如果是前述原因之一导致了ceph返回HEALTH WARN，无需紧张。很多情况下，集群会自行恢复；有些时候你得采取些措施。归置组监控的一件重要事情是保证集群起来并运行着，所有归置组都处于active状态、并且最好是clean状态。用下列命令查看所有归置组状态：
ceph pg stat
The result should tell you the placement group map version (vNNNNNN), the total number of placement groups (x), and how many placement groups are in a particular state such as active+clean (y).
其结果会告诉你归置组运行图的版本号（vNNNNNN）、归置组总数x、有多少归置组处于某种特定状态，如active+clean (y)。
vNNNNNN: x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail

Note: It is common for Ceph to report multiple states for placement groups.
注意：ceph报告多种状态是常见的。
In addition to the placement group states, Ceph will also echo back the amount of data used (aa), the amount of storage capacity remaining (bb), and the total storage capacity for the placement group. These numbers can be important in a few cases:
除了归置组状态之外，ceph也会报告数据占据的空间（aa）、剩余空间（bb）和归置组总容量。这些数字在某些情况下是很重要的：
You are reaching your near full ratio or full ratio.
快达到near full ratio或full ratio时。
Your data isn’t getting distributed across the cluster due to an error in your CRUSH configuration.
由于CRUSH配置错误致使你的数据没能在集群内分布。
归置组ID
Placement Group IDs
Placement group IDs consist of the pool number (not pool name) followed by a period (.) and the placement group ID–a hexadecimal number. You can view pool numbers and their names from the output of ceph osd lspools. The default pool names data, metadata and rbd correspond to pool numbers 0, 1 and 2 respectively. A fully qualified placement group ID has the following form:
归置组ID包含存储池号（不是存储池名字），后面跟一个点（.），然后是归置组ID一个十六进制数字。用ceph osd lspools可查看存储池号及其名字，默认存储池名字data、metadata、和rbd对应的存储池号分别是0、1、2。完整的归置组ID格式如下：
	{pool-num}.{pg-id}
And it typically looks like this:
典型长相：
	0.1f
To retrieve a list of placement groups, execute the following:
用下列命令获取归置组列表：
ceph pg dump
You can also format the output in JSON format and save it to a file:
你也可以让它输出到JSON格式，并保存到文件：
ceph pg dump -o {filename} --format=json
To query a particular placement group, execute the following:
要查询某个归置组，用下列命令：
ceph pg {poolnum}.{pg-id} query
Ceph will output the query in JSON format.
ceph会输出成JSON格式。
{
  "state": "active+clean",
  "up": [
    1,
    0
  ],
  "acting": [
    1,
    0
  ],
  "info": {
    "pgid": "1.e",
    "last_update": "4'1",
    "last_complete": "4'1",
    "log_tail": "0'0",
    "last_backfill": "MAX",
    "purged_snaps": "[]",
    "history": {
      "epoch_created": 1,
      "last_epoch_started": 537,
      "last_epoch_clean": 537,
      "last_epoch_split": 534,
      "same_up_since": 536,
      "same_interval_since": 536,
      "same_primary_since": 536,
      "last_scrub": "4'1",
      "last_scrub_stamp": "2013-01-25 10:12:23.828174"
    },
    "stats": {
      "version": "4'1",
      "reported": "536'782",
      "state": "active+clean",
      "last_fresh": "2013-01-25 10:12:23.828271",
      "last_change": "2013-01-25 10:12:23.828271",
      "last_active": "2013-01-25 10:12:23.828271",
      "last_clean": "2013-01-25 10:12:23.828271",
      "last_unstale": "2013-01-25 10:12:23.828271",
      "mapping_epoch": 535,
      "log_start": "0'0",
      "ondisk_log_start": "0'0",
      "created": 1,
      "last_epoch_clean": 1,
      "parent": "0.0",
      "parent_split_bits": 0,
      "last_scrub": "4'1",
      "last_scrub_stamp": "2013-01-25 10:12:23.828174",
      "log_size": 128,
      "ondisk_log_size": 128,
      "stat_sum": {
        "num_bytes": 205,
        "num_objects": 1,
        "num_object_clones": 0,
        "num_object_copies": 0,
        "num_objects_missing_on_primary": 0,
        "num_objects_degraded": 0,
        "num_objects_unfound": 0,
        "num_read": 1,
        "num_read_kb": 0,
        "num_write": 3,
        "num_write_kb": 1
      },
      "stat_cat_sum": {

      },
      "up": [
        1,
        0
      ],
      "acting": [
        1,
        0
      ]
    },
    "empty": 0,
    "dne": 0,
    "incomplete": 0
  },
  "recovery_state": [
    {
      "name": "Started\/Primary\/Active",
      "enter_time": "2013-01-23 09:35:37.594691",
      "might_have_unfound": [

      ],
      "scrub": {
        "scrub_epoch_start": "536",
        "scrub_active": 0,
        "scrub_block_writes": 0,
        "finalizing_scrub": 0,
        "scrub_waiting_on": 0,
        "scrub_waiting_on_whom": [

        ]
      }
    },
    {
      "name": "Started",
      "enter_time": "2013-01-23 09:35:31.581160"
    }
  ]
}
The following subsections describe common states in greater detail.
后续子章节详述了常见状态。
 3.3.3.4.1  存储池在建中
Creating
When you create a pool, it will create the number of placement groups you specified. Ceph will echo creating when it is creating one or more placement groups. Once they are created, the OSDs that are part of a placement group’s Acting Set will peer. Once peering is complete, the placement group status should be active+clean, which means a Ceph client can begin writing to the placement group.
创建存储池时，它会创建指定数量的归置组。ceph在创建一或多个归置组时会显示creating；创建完后，在其归置组的Acting Set里的OSD将建立互联；一旦互联完成，归置组状态应该变为active+clean，意思是ceph客户端可以向归置组写入数据了。

 3.3.3.4.2  互联建立中
Peering
When Ceph is Peering a placement group, Ceph is bringing the OSDs that store the replicas of the placement group into agreement about the state of the objects and metadata in the placement group. When Ceph completes peering, this means that the OSDs that store the placement group agree about the current state of the placement group. However, completion of the peering process does NOT mean that each replica has the latest contents.
ceph为归置组建立互联时，会让存储归置组副本的OSD之间就其中的对象和元数据状态达成一致。ceph完成了互联，也就意味着存储着归置组的OSD就其当前状态达成了一致。然而，互联过程的完成并不能表明各副本都有了数据的最新版本。
权威历史
Authoratative History
Ceph will NOT acknowledge a write operation to a client, until all OSDs of the acting set persist the write operation. This practice ensures that at least one member of the acting set will have a record of every acknowledged write operation since the last successful peering operation.
With an accurate record of each acknowledged write operation, Ceph can construct and disseminate a new authoritative history of the placement group–a complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.
ceph不会向客户端确认写操作，直到acting set里的所有OSD都完成了写操作。这样处理保证了从上次成功互联起，acting set中至少有一个成员确认了每个写操作。
有了各个已确认写操作的精确记录，ceph就可以构建和散布一个新的归置组权威历史——一个完整、完全有序的操作集，如果被采用，就能把一个OSD的归置组副本更新到最新。
 3.3.3.4.3  活跃
Active
Once Ceph completes the peering process, a placement group may become active. The active state means that the data in the placement group is generally available in the primary placement group and the replicas for read and write operations.
ceph完成互联进程后，一归置组就可变为active。active状态通常意味着在主归置组和副本中的数据都可以读写。
 3.3.3.4.4  整洁
Clean
When a placement group is in the clean state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the placement group. Ceph replicated all objects in the placement group the correct number of times.
某一归置组处于clean状态时，主OSD和副本OSD已成功互联，并且没有偏离的归置组。ceph已把归置组中的对象复制了规定次数。
 3.3.3.4.5  已降级
Degraded
When a client writes an object to the primary OSD, the primary OSD is responsible for writing the replicas to the replica OSDs. After the primary OSD writes the object to storage, the placement group will remain in a degraded state until the primary OSD has received an acknowledgement from the replica OSDs that Ceph created the replica objects successfully.
当客户端向主OSD写入数据时，由主OSD负责把副本写入其余复制OSD。主OSD把对象写入复制OSD后，在没收到成功完成的确认前，主OSD会一直停留在degraded状态。
The reason a placement group can be active+degraded is that an OSD may be active even though it doesn’t hold all of the objects yet. If an OSD goes down, Ceph marks each placement group assigned to the OSD as degraded. The OSDs must peer again when the OSD comes back online. However, a client can still write a new object to a degraded placement group if it is active.
归置组状态可以是active+degraded状态，原因在于一OSD即使没所有对象也可以处于active状态。如果一OSD挂了，ceph会把相关的归置组都标记为degraded；那个OSD重生后，它们必须重新互联。然而，如果归置组仍处于active状态，即便它处于degraded状态，客户端还可以向其写入新对象。
If an OSD is down and the degraded condition persists, Ceph may mark the down OSD as out of the cluster and remap the data from the down OSD to another OSD. The time between being marked down and being marked out is controlled by mon osd down out interval, which is set to 300 seconds by default.
如果一OSD挂了，且degraded状态持续，ceph会把down的OSD标记为在集群外（out）、并把那些down掉的OSD上的数据重映射到其它OSD。从标记为down到out的时间间隔由mon osd down out interval控制，默认是300秒。
A placement group can also be degraded, because Ceph cannot find one or more objects that Ceph thinks should be in the placement group. While you cannot read or write to unfound objects, you can still access all of the other objects in the degraded placement group.
归置组也会被降级（degraded），因为归置组找不到本应存在于归置组中的一或多个对象，这时，你不能读或写找不到的对象，但仍能访问其它位于降级归置组中的对象。
 3.3.3.4.6  恢复中
Recovering
Ceph was designed for fault-tolerance at a scale where hardware and software problems are ongoing. When an OSD goes down, its contents may fall behind the current state of other replicas in the placement groups. When the OSD is back up, the contents of the placement groups must be updated to reflect the current state. During that time period, the OSD may reflect a recovering state.
ceph被设计为可容错，可抵御一定规模的软、硬件问题。当某OSD挂了（down）时，其内容版本会落后于归置组内的其它副本；它重生（up）时，归置组内容必须更新，以反映当前状态；在此期间，OSD在recovering状态。
Recovery isn’t always trivial, because a hardware failure might cause a cascading failure of multiple OSDs. For example, a network switch for a rack or cabinet may fail, which can cause the OSDs of a number of host machines to fall behind the current state of the cluster. Each one of the OSDs must recover once the fault is resolved.
恢复并非总是这些小事，因为一次硬件失败可能牵连多个OSD。比如一个机柜的网络交换机失败了，这会导致多个主机落后于集群的当前状态，问题解决后每一个OSD都必须恢复。
Ceph provides a number of settings to balance the resource contention between new service requests and the need to recover data objects and restore the placement groups to the current state. The osd recovery delay start setting allows an OSD to restart, re-peer and even process some replay requests before starting the recovery process. The osd recovery threads setting limits the number of threads for the recovery process (1 thread by default). The osd recovery thread timeout sets a thread timeout, because multiple OSDs may fail, restart and re-peer at staggered rates. The osd recovery max active setting limits the number of recovery requests an OSD will entertain simultaneously to prevent the OSD from failing to serve . The osd recovery max chunk setting limits the size of the recovered data chunks to prevent network congestion.
ceph提供了很多选项来均衡资源竞争，如新服务请求、恢复数据对象和恢复归置组到当前状态。osd recovery delay start选项允许一OSD在开始恢复进程前，先重启、重建互联、甚至处理一些重放请求；osd recovery threads选项限制恢复进程的线程数，默认为1线程；osd recovery thread timeout设置线程超时，因为多个OSD可能交替失败、重启和重建互联；osd recovery max active选项限制一OSD最多同时接受多少请求，以防它压力过大而不能正常服务；osd recovery max chunk选项限制恢复数据块尺寸，以防网络拥塞。
 3.3.3.4.7  回填中
Back Filling
When a new OSD joins the cluster, CRUSH will reassign placement groups from OSDs in the cluster to the newly added OSD. Forcing the new OSD to accept the reassigned placement groups immediately can put excessive load on the new OSD. Back filling the OSD with the placement groups allows this process to begin in the background. Once backfilling is complete, the new OSD will begin serving requests when it is ready.
有新OSD加入集群时，CRUSH会把现有集群内的归置组重分配给它。强制新OSD立即接受重分配的归置组会使之过载，用归置组回填可使这个过程在后台开始。回填完成后，新OSD准备好时就可以对外服务了。
During the backfill operations, you may see one of several states: backfill_wait indicates that a backfill operation is pending, but isn’t underway yet; backfill indicates that a backfill operation is underway; and, backfill_too_full indicates that a backfill operation was requested, but couldn’t be completed due to insufficient storage capacity.
在回填操作期间，你可能见到几种状态之一：backfill_wait表明一回填操作挂起了，还没开始；backfill表明一回填操作正在进行；backfill_too_full表明请求回填了，但是因存储空间不足而不能完成。
Ceph provides a number of settings to manage the load spike associated with reassigning placement groups to an OSD (especially a new OSD). By default, osd_max_backfills sets the maximum number of concurrent backfills to or from an OSD to 10. The osd backfill full ratio enables an OSD to refuse a backfill request if the OSD is approaching its its full ratio (85%, by default). If an OSD refuses a backfill request, the osd backfill retry interval enables an OSD to retry the request (after 10 seconds, by default). OSDs can also set osd backfill scan min and osd backfill scan max to manage scan intervals (64 and 512, by default).
ceph提供了多个选项来解决重分配归置组时相关的负载问题。默认，osd_max_backfill把双向的回填并发量都设置为10；osd backfill full ratio可让一OSD在接近占满率（默认85%）时拒绝回填请求，如果一OSD拒绝了回填请求，在osd backfill retry interval间隔之后将重试（默认10秒）；OSD也能用osd backfill scan min和osd backfill scan max来管理扫描间隔（默认64和512）。
 3.3.3.4.8  被重映射
Remapped
When the Acting Set that services a placement group changes, the data migrates from the old acting set to the new acting set. It may take some time for a new primary OSD to service requests. So it may ask the old primary to continue to service requests until the placement group migration is complete. Once data migration completes, the mapping uses the primary OSD of the new acting set.
某一归置组的Acting Set变更时，数据要从旧集合迁移到新的。主OSD要花费一些时间才能提供服务，所以它可以让老的主OSD持续服务、直到归置组迁移完。数据迁移完后，主OSD会映射到新acting set。
 3.3.3.4.9  发蔫（stale）
Stale
While Ceph uses heartbeats to ensure that hosts and daemons are running, the ceph-osd daemons may also get into a stuck state where they aren’t reporting statistics in a timely manner (e.g., a temporary network fault). By default, OSD daemons report their placement group, up thru, boot and failure statistics every half second (i.e., 0.5), which is more frequent than the heartbeat thresholds. If the Primary OSD of a placement group’s acting set fails to report to the monitor or if other OSDs have reported the primary OSD down, the monitors will mark the placement group stale.
虽然ceph用心跳来保证主机和守护进程在运行，但是ceph-osd仍有可能进入stuck状态，它们没有按时报告其状态（如网络瞬断）。默认，OSD守护进程每半秒（0.5）会一次报告其归置组、出流量、引导和失败统计状态，此频率高于心跳阀值。如果一归置组的主OSD所在的acting set没能向监视器报告、或者其它监视器已经报告了那个主OSD已down，监视器们就会把此归置组标记为stale。
When you start your cluster, it is common to see the stale state until the peering process completes. After your cluster has been running for awhile, seeing placement groups in the stale state indicates that the primary OSD for those placement groups is down or not reporting placement group statistics to the monitor.
启动集群时，会经常看到stale状态，直到互联完成。集群运行一阵后，如果还能看到有归置组位于stale状态，就说明那些归置组的主OSD挂了（down）、或没在向监视器报告统计信息。
 3.3.3.5  找出故障归置组
Identifying Troubled PGs
As previously noted, a placement group isn’t necessarily problematic just because its state isn’t active+clean. Generally, Ceph’s ability to self repair may not be working when placement groups get stuck. The stuck states include:
如前所述，一个归置组状态是active+clean时未必有问题。一般来说，归置组卡住时ceph的自修复功能往往无能为力，卡住的状态细分为：
Unclean: Placement groups contain objects that are not replicated the desired number of times. They should be recovering.
不干净：归置组里有些对象的复制数未达到期望次数，它们应该在恢复中。
Inactive: Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back up.
不活跃：归置组不能处理读写，因为它们在等着一个持有最新数据的OSD再次进入up状态。
Stale: Placement groups are in an unknown state, because the OSDs that host them have not reported to the monitor cluster in a while (configured by mon osd report timeout).
发蔫：归置组们处于一种未知状态，因为存储它们的OSD有一阵子没向监视器报告了（由mon osd report timeout配置）。
To identify stuck placement groups, execute the following:
为找出卡住的归置组，执行：
ceph pg dump_stuck [unclean|inactive|stale]
See Placement Group Subsystem for additional details. To troubleshoot stuck placement groups, see Troubleshooting PG Errors.
详情见Placement Group Subsystem，排除卡住的归置组见Troubleshooting PG Errors。
 3.3.3.6  定位对象
Finding an Object Location
To store object data in the Ceph Object Store, a Ceph client must:
要把对象数据存入Ceph对象存储，一ceph客户端必须：
1. Set an object name
设置对象名
2. Specify a pool
指定一存储池
The Ceph client retrieves the latest cluster map and the CRUSH algorithm calculates how to map the object to a placement group, and then calculates how to assign the placement group to an OSD dynamically. To find the object location, all you need is the object name and the pool name. For example:
ceph客户端索取最新集群运行图、并用CRUSH算法计算对象到归置组的映射，然后计算如何动态地把归置组映射到OSD。要定位对象，只需要知道对象名和存储池名字，例如：
ceph osd map {poolname} {object-name}

练习：定位一个对象
Excercise: Locate an Object
As an exercise, lets create an object. Specify an object name, a path to a test file containing some object data and a pool name using the rados put command on the command line. For example:
反正是练习，我们先创建一个对象。给rados put命令指定一对象名、一个包含数据的测试文件路径、和一个存储池名字，例如：
rados put {object-name} {file-path} --pool=data
rados put test-object-1 testfile.txt --pool=data
To verify that the Ceph Object Store stored the object, execute the following:
用下列命令确认ceph对象存储已经包含此对象：
rados -p data ls
Now, identify the object location:
现在可以定位对象了：
ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1
Ceph should output the object’s location. For example:
ceph应该输出对象的位置，例如：
osdmap e537 pool 'data' (0) object 'test-object-1' -> pg 0.d1743484 (0.4) -> up [1,0] acting [1,0]
To remove the test object, simply delete it using the rados rm command. For example:
要删除测试对象，用rados rm即可，如：
rados rm test-object-1 --pool=data

As the cluster evolves, the object location may change dynamically. One benefit of Ceph’s dynamic rebalancing is that Ceph relieves you from having to perform the migration manually. See the Architecture section for details.
随着集群的运转，对象位置会动态改变。ceph动态重均衡的优点之一，就是把你从手动迁移中解救了，详情见Architecture。
 3.3.4  认证概览
ceph认证及授权
Ceph authentication & authorization
Ceph is a distributed storage system where a typical deployment involves a relatively small quorum of monitors, scores of metadata servers (MDSs) and many thousands of OSD daemons operating across many hosts/nodes–representing the server portion of the Ceph object store. Ceph clients such as CephFS, Ceph block device and Ceph Gateway interact with the Ceph object store. All Ceph object store clients use the librados library to interact with the Ceph object store. The following diagram illustrates an abstract client/server technology stack.
ceph是一个分布式存储系统，其典型部署包含相对少量的监视器、许多元数据服务器（MDS）和数千OSD守护进程，它们运行于很多主机或节点，共同构成了ceph对象存储的服务器部分；ceph客户端如CephFS、Ceph块设备和Ceph网关与ceph对象存储交互，所有客户端都用librados库与ceph对象存储交互，下面的图抽象地展示了客户端/服务器技术。

Users are either individuals or system actors such as applications, which use Ceph clients to interact with Ceph server daemons.
用户可以是个人或系统角色，像应用程序，它们用ceph客户端和ceph服务器守护进程交互。

For additional information, see our Cephx Guide and ceph-authtool manpage.
更多信息参见Cephx Guide和ceph-authtool手册。
 3.3.4.1  ceph认证（cephx）
CEPH AUTHENTICATION (CEPHX)
Cryptographic authentication has some computational costs, though they should generally be quite low. If the network environment connecting your client and server hosts is very safe and you cannot afford authentication, you can use a Ceph option to turn it off. This is not generally recommended, but should you need to do so, details can be found in the Disable Cephx section.
加密认证要耗费一定计算资源，但通常很低。如果您的客户端和服务器网络环境相当安全，而且认证的负面效应更大，你可以关闭它，通常不推荐您这么做，但必要时可以。详情参见Disable Cephx。
Important Remember, if you disable authentication, you are at risk of a man-in-the-middle attack altering your client/server messages, which could lead to disastrous security effects.
重要：记住，如果禁用了认证，就会有篡改客户端/服务器消息这样的中间人攻击风险，这会导致灾难性后果。
A key scalability feature of Ceph is to avoid a centralized interface to the Ceph object store, which means that Ceph clients must be able to interact with OSDs directly. To protect data, Ceph provides its cephx authentication system, which authenticates users operating Ceph clients. The cephx protocol operates in a manner with behavior similar to Kerberos.
ceph一个主要伸缩功能就是避免了对象存储的中央接口，这就要求ceph客户端能直接和OSD交互。Ceph通过cephx认证系统保护数据，它也认证运行ceph客户端的用户，cephx协议运行机制类似Kerberos。
A user/actor invokes a Ceph client to contact a monitor. Unlike Kerberos, each monitor can authenticate users and distribute keys, so there is no single point of failure or bottleneck when using cephx. The monitor returns an authentication data structure similar to a Kerberos ticket that contains a session key for use in obtaining Ceph services. This session key is itself encrypted with the user’s permanent secret key, so that only the user can request services from the Ceph monitor(s). The client then uses the session key to request its desired services from the monitor, and the monitor provides the client with a ticket that will authenticate the client to the OSDs that actually handle data. Ceph monitors and OSDs share a secret, so the client can use the ticket provided by the monitor with any OSD or metadata server in the cluster. Like Kerberos, cephx tickets expire, so an attacker cannot use an expired ticket or session key obtained surreptitiously. This form of authentication will prevent attackers with access to the communications medium from either creating bogus messages under another user’s identity or altering another user’s legitimate messages, as long as the user’s secret key is not divulged before it expires.
用户/参与者通过调用ceph客户端来联系监视器，不像Kerberos，每个监视器都能认证用户、发布密钥，所以使用cephx时不会有单点故障或瓶颈。监视器返回一个类似Kerberos票据的认证数据结构，它包含一个可用于获取ceph服务的会话密钥，会话密钥是用户的永久私钥自加密过的，只有此用户能从ceph监视器请求服务。客户端用会话密钥向监视器请求需要的服务，然后监视器给客户端一个凭证用以向实际持有数据的OSD认证。ceph的监视器和OSD共享相同的密钥，所以集群内任何OSD或元数据服务器都认可客户端从监视器获取的凭证，像Kerberos一样cephx凭证也会过期，以使攻击者不能用暗中得到的过期凭证或会话密钥。只要用户的私钥过期前没有泄露 ，这种认证形式就可防止中间线路攻击者以别人的ID发送垃圾消息、或修改用户的正常消息。
To use cephx, an administrator must set up users first. In the following diagram, the client.admin user invokes ceph auth get-or-create-key from the command line to generate a username and secret key. Ceph’s auth subsystem generates the username and key, stores a copy with the monitor(s) and transmits the user’s secret back to the client.admin user. This means that the client and the monitor share a secret key.
要使用cephx，管理员必须先设置好用户。在下面的图解里，client.admin用户从命令行调用 ceph auth get-or-create-key来生成一个用户及其密钥，ceph的认证子系统生成了用户名和密钥、副本存到监视器然后把此用户的密钥回传给client.admin用户，也就是说客户端和监视器共享着相同的密钥。
Note: The client.admin user must provide the user ID and secret key to the user in a secure manner.
注意：client.admin用户必须以安全方式把此用户ID和密钥交给用户。


To authenticate with the monitor, the client passes in the user name to the monitor, and the monitor generates a session key and encrypts it with the secret key associated to the user name. Then, the monitor transmits the encrypted ticket back to the client. The client then decrypts the payload with the shared secret key to retrieve the session key. The session key identifies the user for the current session. The client then requests a ticket on behalf of the user signed by the session key. The monitor generates a ticket, encrypts it with the user’s secret key and transmits it back to the client. The client decrypts the ticket and uses it to sign requests to OSDs and metadata servers throughout the cluster.
要和监视器认证，客户端得把用户名传给监视器，然后监视器生成一个会话密钥、并且用此用户的密钥加密它，然后把加密的凭证回传给客户端，客户端用共享密钥解密载荷就可获取会话密钥。会话密钥在当前会话中标识了此用户，客户端再用此会话密钥签署过的用户名请求一个凭证，监视器生成一个凭证、用客户端的密钥加密它，然后回传给客户端，客户端解密此凭证，然后用它签署连接集群内OSD和元数据服务器的请求。

The cephx protocol authenticates ongoing communications between the client machine and the Ceph servers. Each message sent between a client and server, subsequent to the initial authentication, is signed using a ticket that the monitors, OSDs and metadata servers can verify with their shared secret.
cephx协议认证客户端机器和ceph服务器间正在进行的通讯，二者间认证完成后的每条消息都用凭证签署过，监视器、OSD、元数据服务器可用共享的密钥来校验。

The protection offered by this authentication is between the Ceph client and the Ceph server hosts. The authentication is not extended beyond the Ceph client. If the user accesses the Ceph client from a remote host, Ceph authentication is not applied to the connection between the user’s host and the client host.
认证提供的保护位于ceph客户端和服务器间，没有扩展到ceph客户端之外。如果用户从远程主机访问ceph客户端，ceph认证就不管用了，它不会影响到用户主机和客户端主机间的通讯。
 3.3.4.2  ceph授权（能力）
CEPH AUTHORIZATION (CAPS)
Ceph uses the term “capabilities” (caps) to describe authorizing an authenticated user to exercise the functionality of the monitors, OSDs and metadata servers. Capabilities can also restrict access to data within one or more pools.
ceph用能力(caps)来描述给认证用户的授权，这样才能使用监视器、OSD、和元数据服务器的功能。能力也能限制到一或多个存储池的访问。
Note: Ceph uses the capabilities discussed here for setting up and controlling access between various Ceph client and server instances, and are relevant regardless of what type of client accesses the Ceph object store. CephFS uses a different type of capability for files and directories internal to the CephFS filesystem. CephFS filesystem access controls are relevant to CephFS, but not block devices or the RESTful gateway.
注意：ceph使用能力来设置、控制ceph客户端和服务器例程间的相互访问，并且不管哪种客户端访问ceph对象存储。CephFS内部给文件和目录用了不同于CephFS文件系统的另外一种能力，CephFS访问控制和CephFS有关，但不是块设备或RESTful网关。
A Ceph client.admin user sets a user’s capabilities when creating the user.
ceph的client.admin用户在创建用户时设置了用户的能力。
allow
Description:
Precedes access settings for a daemon. Implies rw for MDS only.
在守护进程的访问设置之前，仅对MDS隐含rw。
Example:
ceph-authtool -n client.foo --cap mds 'allow'
r
Description:
Gives the user read access. Required with monitors to retrieve the CRUSH map.
授予用户读权限，监视器需要它才能搜刮CRUSH图。
Example:
ceph-authtool -n client.foo --cap mon 'allow r'
w
Description:
Gives the user write access to objects.
授予用户写对象的权限。
Example:
ceph-authtool -n client.foo --cap osd 'allow w'
x
Description:
Gives the user the capability to call class methods (i.e., both read and write).
授予用户调用类方法的能力，如同时有读和写。
Example:
ceph-authtool -n client.foo --cap osd 'allow x'
class-read
Descriptions:
Gives the user the capability to call class read methods. Subset of x.
授予用户调用类读取方法的能力，x的子集。
Example:
ceph-authtool -n client.foo --cap osd 'allow class-read'
class-write
Description:
Gives the user the capability to call class write methods. Subset of x.
授予用户调用类写入方法的能力，x的子集。
Example:
ceph-authtool -n client.foo --cap osd 'allow class-write'
*
Description:
Gives the user read, write and execute permissions for a particular daemon/pool, and the ability to execute admin commands.
授权用户读、写和执行某守护进程/存储池，且允许执行管理命令。
Example:
ceph-authtool -n client.foo --cap osd 'allow *'
When setting capabilities for a user, Ceph also supports restricting the capabilities to a particular pool. This means you can have full access to some pools, and restricted (or no) access to other pools for the same user. For example:
给用户设置能力的时候，ceph也支持把能力限制于某存储池。意思是你可以完全地访问一些存储池、访问其他存储池却是受限的（或未受限）。
ceph-authtool -n client.foo --cap osd 'allow rwx pool=customer-pool'
 3.3.4.3  cephx的局限性
CEPHX LIMITATIONS
The cephx protocol authenticates Ceph clients and servers to each other. It is not intended to handle authentication of human users or application programs run on their behalf. If that effect is required to handle your access control needs, you must have another mechanism, which is likely to be specific to the front end used to access the Ceph object store. This other mechanism has the role of ensuring that only acceptable users and programs are able to run on the machine that Ceph will permit to access its object store.
cephx协议提供ceph客户端和服务器间的相互认证，并没打算认证人类用户或者应用程序。如果有访问控制需求，那必须用另外一种机制，它对于前端用户访问ceph对象存储可能是特定的，其任务是确保只有此机器上可接受的用户和程序才能访问ceph的对象存储。
The keys used to authenticate Ceph clients and servers are typically stored in a plain text file with appropriate permissions in a trusted host.
用于认证ceph客户端和服务器的密钥通常以纯文本存储在权限合适的文件里，此文件保存于可信主机上。
Important: Storing keys in plaintext files has security shortcomings, but they are difficult to avoid, given the basic authentication methods Ceph uses in the background. Those setting up Ceph systems should be aware of these shortcomings.
重要：密钥存储为纯文本文件有安全缺陷，但很难避免，它给了ceph可用的基本认证方法，设置ceph时应该注意这些缺陷。
In particular, arbitrary user machines, especially portable machines, should not be configured to interact directly with Ceph, since that mode of use would require the storage of a plaintext authentication key on an insecure machine. Anyone who stole that machine or obtained surreptitious access to it could obtain the key that will allow them to authenticate their own machines to Ceph.
尤其是任意用户、特别是移动机器不应该和ceph直接交互，因为这种用法要求把明文认证密钥存储在不安全的机器上，这些机器的丢失、或盗用将泄露可访问ceph集群的密钥。
Rather than permitting potentially insecure machines to access a Ceph object store directly, users should be required to sign in to a trusted machine in your environment using a method that provides sufficient security for your purposes. That trusted machine will store the plaintext Ceph keys for the human users. A future version of Ceph may address these particular authentication issues more fully.
相比于允许潜在的欠安全机器直接访问ceph对象存储，应该要求用户先登录安全有保障的可信机器，这台可信机器会给人们存储明文密钥。未来的ceph版本也许会更彻底地解决这些特殊认证问题。
At the moment, none of the Ceph authentication protocols provide secrecy for messages in transit. Thus, an eavesdropper on the wire can hear and understand all data sent between clients and servers in Ceph, even if he cannot create or alter them. Further, Ceph does not include options to encrypt user data in the object store. Users can hand-encrypt and store their own data in the Ceph object store, of course, but Ceph provides no features to perform object encryption itself. Those storing sensitive data in Ceph should consider encrypting their data before providing it to the Ceph system.
当前，没有任何ceph认证协议保证传送中消息的私密性。所以，即使物理线路窃听者不能创建用户或修改它们，但可以听到、并理解客户端和服务器间发送过的所有数据。此外，ceph没有可加密用户数据的选项，当然，用户可以手动加密、然后把它们存在对象库里，但ceph没有自己加密对象的功能。在ceph里存储敏感数据的用户应该考虑存入ceph集群前先加密。
 3.3.5  cephx认证
Cephx Guide
Ceph provides two authentication modes:
ceph提供了两种认证模式：
None: Any user can access data without authentication.
none：任何用户无需认证就能访问数据；
Cephx: Ceph requires user authentication in a manner similar to Kerberos.
cephx：ceph要求用户认证，机制类似Kerberos。
If you disable cephx, you do not need to generate keys using the procedures described here. If you re-enable cephx and have already generated keys, you do not need to generate the keys again.
如果你要禁用cephx，就不需要按前述生成密钥；如果你重新启用cephx且已然生成了密钥，也无需再次生成。
Important: The cephx protocol does not address data encryption in transport (e.g., SSL/TLS) or encryption at rest.
重要：cephx协议不解决传输加密（如SSL/TLS）、或者存储加密问题。
For additional information, see our Cephx Intro and ceph-authtool manpage.
额外信息要参见 Cephx Intro和ceph-authtool手册。
 3.3.5.1  配置cephx
CONFIGURING CEPHX
There are several important procedures you must follow to enable the cephx protocol for your Ceph cluster and its daemons.
要为集群及其守护进程启用cephx协议，有几个重要步骤必须遵循。
1. First, you must generate a secret key for the default client.admin user so the administrator can execute Ceph commands.
首先，必须给默认的client.admin生成密钥，以便管理员执行ceph命令；
2. Second, you must generate a monitor secret key and distribute it to all monitors in the cluster.
其次，必须生成一个监视器密钥，并发布到集群内的所有监视器；
3. Finally, you can follow the remaining steps in Enabling Cephx to enable authentication.
最后，你要完成启用cephx里的剩余步骤。
 3.3.5.1.1  client.admin密钥
THE CLIENT.ADMIN KEY
When you first install Ceph, each Ceph command you execute on the command line assumes that you are the client.admin default user. When running Ceph with cephx enabled, you need to have a key for the client.admin user to run ceph commands as the administrator.
你首次安装ceph后，你执行的每个ceph命令都先假设你是默认用户client.admin。集群如果启用了cephx，client.admin用户就需要一个密钥才能运行ceph命令。
Important: To run Ceph commands on the command line with cephx enabled, you need to create a key for the client.admin user, and create a secret file under /etc/ceph.
重要：在启用了cephx的ceph上执行命令时，你得给client.admin用户创建一个密钥、并在/etc/ceph下创建一个密钥文件。
The following command will generate and register a client.admin key on the monitor with admin capabilities and write it to a keyring on the local file system. If the key already exists, its current value will be returned.
下列命令将在监视器上生成并注册一个具有管理能力的client.admin密钥，并将之写入本地文件系统的keyring，如果密钥已存在，将返回其值。
sudo ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon 'allow *' > /etc/ceph/keyring
See Enabling Cephx step 1 for stepwise details to enable cephx.
启用cephx的手把手教程，请参见启用cephx步骤1。
 3.3.5.1.2  监视器密钥环
MONITOR KEYRINGS
Ceph requires a keyring for the monitors. Use the ceph-authtool command to generate a secret monitor key and keyring.
ceph的监视器需要一个密钥环，用ceph-authtool命令生成密钥和密钥环。
sudo ceph-authtool {keyring} --create-keyring --gen-key -n mon.
A cluster with multiple monitors must have identical keyrings for all monitors. So you must copy the keyring to each monitor host under the following directory:
有多个监视器的集群，其所有监视器的密钥环必须相同，所以你必须把下列目录里的密钥环拷到每个监视器：
/var/lib/ceph/mon/$cluster-$id
See Enabling Cephx step 2 and 3 for stepwise details to enable cephx.
启用cephx的详细步骤参见启用cephx的第2、3步。
 3.3.5.1.3  启用cephx
ENABLING CEPHX
When cephx is enabled, Ceph will look for the keyring in the default search path, which includes /etc/ceph/keyring. You can override this location by adding a keyring option in the [global] section of your Ceph configuration file, but this is not recommended.
启用cephx后，ceph将在默认搜索路径（/etc/ceph/keyring）里查找密钥环。你可以在配置文件的[global]段里添加keyring选项来修改，但不推荐。
Execute the following procedures to enable cephx on a cluster with cephx disabled. If you (or your deployment utility) have already generated the keys, you may skip the steps related to generating keys.
在禁用了cephx的集群上执行下面的步骤来启用它，如果你（或者部署工具）已经生成了密钥，你可以跳过相关步骤。
1. Create a client.admin key, and save a copy of the key for your client host:
创建client.admin密钥，并为客户端保存此密钥的副本：
ceph auth get-or-create client.admin mon 'allow *' mds 'allow *' osd 'allow *' -o /etc/ceph/keyring
**Warning:** This will clobber any existing ``/etc/ceph/keyring`` file. Be careful!
2. Generate a secret monitor mon. key:
生成给监视器的mon.密钥：
ceph-authtool --create --gen-key -n mon. /tmp/monitor-key
3. Copy the mon keyring into a keyring file in every monitor’s mon data directory:
把mon密钥环拷贝到keyring文件，再拷到每个监视器的mon data目录下：
cp /tmp/monitor-key /var/lib/ceph/mon/ceph-a/keyring
4. Generate a secret key for every OSD, where {$id} is the OSD number:
为每个OSD生成密钥，{id}是OSD编号：
ceph auth get-or-create osd.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-{$id}/keyring
5. Generate a secret key for every MDS, where {$id} is the MDS letter:
为每个MDS生成密钥，{id}是MDS的标识字母：
ceph auth get-or-create mds.{$id} mon 'allow rwx' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mds/ceph-{$id}/keyring
6. Enable cephx authentication for versions 0.51 and above by setting the following options in the [global] section of your Ceph configuration file:
0.51以上版本启用cephx时，要在配置文件的[global]段下添加如下内容：
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
7. Or, enable cephx authentication for versions 0.50 and below by setting the following option in the [global] section of your Ceph configuration file:
或者，0.50以下版本启用cephx时，要在配置文件的[global]段下添加以下：
auth supported = cephx
Deprecated since version 0.51.
从0.51起死亡了。
1. Start or restart the Ceph cluster.
启动或重启ceph集群：
sudo service ceph -a start
sudo service ceph -a restart
 3.3.5.1.4  禁用cephx
DISABLING CEPHX
The following procedure describes how to disable Cephx. If your cluster environment is relatively safe, you can offset the computation expense of running authentication. We do not recommend it. However, it may be easier during setup and/or troubleshooting to temporarily disable authentication.
下面的步骤描述了如何禁用cephx，如果你的集群环境相对安全，你可以去掉认证带来的计算消耗，然而我们不推荐。但是临时禁用认证会使安装、和/或排障更简单。

1. Disable cephx authentication for versions 0.51 and above by setting the following options in the [global] section of your Ceph configuration file:
在0.51及以上禁用cephx认证，要在配置文件的[global]段下设置：
auth cluster required = none
auth service required = none
auth client required = none
2. Or, disable cephx authentication for versions 0.50 and below (deprecated as of version 0.51) by setting the following option in the [global] section of your Ceph configuration file:
或者，在0.50（从0.51时消失）及以下禁用cephx，要在配置文件的[global]段下设置：
auth supported = none
3. Start or restart the Ceph cluster.
启动或重启ceph集群：
sudo service ceph -a start
sudo service ceph -a restart
 3.3.5.1.5  守护进程密钥环
DAEMON KEYRINGS
With the exception of the monitors, Ceph generates daemon keyrings in the same way that it generates user keyrings. By default, the daemons store their keyrings inside their data directory. The default keyring locations, and the capabilities necessary for the daemon to function, are shown below.
除监视器外，守护进程密钥环和用户密钥环生成方法一样。默认情况下，守护进程把密钥环保存在它们的数据目录下，默认密钥环位置、和守护进程发挥作用必需的能力展示如下：
ceph-mon
Location:
$mon_data/keyring
Capabilities:
N/A
ceph-osd
Location:
$osd_data/keyring
Capabilities:
mon 'allow rwx' osd 'allow *'
ceph-mds
Location:
$mds_data/keyring
Capabilities:
mds 'allow rwx' mds 'allow *' osd 'allow *'
radosgw
Location:
$rgw_data/keyring
Capabilities:
mon 'allow r' osd 'allow rwx'
Note that the monitor keyring contains a key but no capabilities, and is not part of the cluster auth database.
注意，监视器密钥环包含一个密钥，但没有能力，且不是集群auth数据库的一部分。
The daemon data directory locations default to directories of the form:
守护进程数据目录位置默认格式如下：
/var/lib/ceph/$type/$cluster-$id
For example, osd.12 would be:
例如，osd.12的目录会是：
/var/lib/ceph/osd/ceph-12
You can override these locations, but it is not recommended.
你可以覆盖这些位置，但不推荐。
 3.3.5.2  cephx管理
CEPHX ADMINISTRATION
Cephx uses shared secret keys for authentication, meaning both the client and the monitor cluster have a copy of the client’s secret key. The authentication protocol is such that both parties are able to prove to each other they have a copy of the key without actually revealing it. This provides mutual authentication, which means the cluster is sure the user possesses the secret key, and the user is sure that the cluster has a copy of the secret key.
cephx用共享密钥来认证，即客户端和监视器集群各自都有客户端密钥的副本。这样的认证协议使所有参与者没有展现密钥就能相互证明，就是说集群确信用户可处理密钥、而且用户相信集群有密钥的副本。
Default users and pools are suitable for initial testing purposes. For test bed and production environments, you should create users and assign pool access to the users.
默认用户和存储池只适合最初的测试，在试验台和生产环境下，应该创建用户并给其分配存储池访问权限。
 3.3.5.2.1  增加密钥
ADD A KEY
Keys enable a specific user to access the monitor, metadata server and cluster according to capabilities assigned to the key. Capabilities are simple strings specifying some access permissions for a given server type. Each server type has its own string. All capabilities are simply listed in {type} and {capability} pairs on the command line:
具体用户根据分配给密钥的能力可访问监视器、元数据服务器和集群。能力用字符串定义，它给某类服务器指定访问权限，每种服务器都各有其字符串。在命令行下，所有能力都可成对地表示为{type}和{capability}。
sudo ceph auth get-or-create client.{username} {daemon1} {cap1} {daemon2} {cap2} ...
For example, to create a user client.foo with access ‘rw’ for daemon type ‘osd’ and ‘r’ for daemon type ‘mon’:
例如，要创建一个用户client.foo，他对osd有rw权限、对mon有r权限：
sudo ceph auth get-or-create client.foo osd 'allow rw' mon 'allow r' > keyring.foo
Note: User names are associated to user types, which include client osd, mon, and mds. In most cases, you will be creating keys for client users.
注意：用户名和用户类型相关联，包括client和osd、mon、和mds，在大多数情况下，你要给client用户创建密钥。
After you add a key to the cluster keyring, go to the relevant client(s) and copy the keyring from the cluster host to the client(s).
把密钥加入集群密钥环后，把集群密钥环从集群主机拷贝到相关客户端。
sudo scp {user}@{ceph-cluster-host}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring

Tip: Ensure the ceph.keyring file has appropriate permissions set (e.g., chmod 644) on your client machine.
提示：确保你客户端主机上的ceph.keyring文件设置了合适的权限位（如chmod 644）。
 3.3.5.2.2  删除密钥
DELETE A KEY
To delete a key for a user or a daemon, use ceph auth del:
要删除一个用户或守护进程的密钥，用ceph auth del：
ceph auth del {daemon-type}.{ID|username}
Where {daemon-type} is one of client, osd, mon, or mds, and {ID|username} is the ID of the daemon or the username.
{daemon-type}是client、osd、mon、mds中一个，{ID|username}是守护进程或用户名的ID。
After you delete a key from the cluster keyring, go to the relevant client(s) and copy the keyring from the cluster host to the client(s).
从集群密钥环删除密钥后，把集群密钥环从集群主机拷贝到相关客户端。
sudo scp {user}@{ceph-cluster-host}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring

Tip: Ensure the ceph.keyring file has appropriate permissions set (e.g., chmod 644) on your client machine.
提示：确保你客户端主机上的ceph.keyring文件设置了合适的权限位（如chmod 644）。
 3.3.5.2.3  列出集群内的密钥
LIST KEYS IN YOUR CLUSTER
To list the keys registered in your cluster:
要列出集群内注册的密钥：
sudo ceph auth list
 3.3.5.3  cephx命令行选项
CEPHX COMMANDLINE OPTIONS
When Ceph runs with Cephx enabled, you must specify a user name and a secret key on the command line. Alternatively, you may use the CEPH_ARGS environment variable to avoid re-entry of the user name and secret.
在启用了cephx的集群上，你必须在命令行指定用户名及其密钥；另外你也可以用CEPH_ARGS环境变量来避免多次输入用户名和密钥。
ceph --id {user-name} --keyring=/path/to/secret [commands]
For example:
例如：
ceph --id client.admin --keyring=/etc/ceph/ceph.keyring [commands]
Ceph supports the following usage for user name and secret:
ceph支持用户名和密钥的下列用法：
--id | --user
Description:
Ceph identifies users with a type and an ID (e.g., TYPE.ID or client.admin, client.user1). The id, name and -n options enable you to specify the ID portion of the user name (e.g., admin, user1, foo, etc.). You can specify the user with the --id and omit the type. For example, to specify user client.foo enter the following:
ceph用一个类型和ID（如TYPE.ID或client.admin、client.user1）来标识用户，id、name、和-n选项可用于指定用户名（如admin、user1、foo等）的ID部分，你可以用--id指定用户并忽略类型，例如可用下列命令指定client.foo用户：
ceph --id foo --keyring /path/to/keyring health
ceph --user foo --keyring /path/to/keyring health
--name
Description:
Ceph identifies users with a type and an ID (e.g., TYPE.ID or client.admin, client.user1). The --name and -n options enables you to specify the fully qualified user name. You must specify the user type (typically client) with the user ID. For example:
ceph用一个类型和ID（如TYPE.ID或client.admin、client.user1）来标识用户，--name和-n选项可用于指定完整的用户名，但必须指定用户类型（一般是client）和用户ID，例如：
ceph --name client.foo --keyring /path/to/keyring health
ceph -n client.foo --keyring /path/to/keyring health
--keyring
Description:
The path to the keyring containing one or more user name and secret. The --secret option provides the same functionality, but it does not work with Ceph RADOS Gateway, which uses --secret for another purpose. You may retrieve a keyring with ceph auth get-or-create and store it locally. This is a preferred approach, because you can switch user names without switching the keyring path. For example:
包含一或多个用户名、密钥的密钥环路径。--secret选项提供了相同功能，但它不能用于RADOS网关，其--secret另有用途。你可以用ceph auth get-or-create获取密钥环并保存在本地，然后您就可以改用其他用户而无需重指定密钥环路径了。
sudo rbd map foo --pool rbd myimage --id client.foo --keyring /path/to/keyring
--keyfile
Description:
The path to the key file containing the secret key for the user specified by --id, --name, -n, or --user. You may retrieve the key for a specific user with ceph auth get and store it locally. Then, specify the path to the keyfile. For example:
包含用户（用--id、--name、-n、或--user指定）密钥的文件路径。你可以用ceph auth get获取密钥并保存在本地，然后指定给--keyfile，例如：
sudo rbd map foo --pool rbd myimage --id client.foo --keyfile /path/to/file

Note: Add the user and secret to the CEPH_ARGS environment variable so that you don’t need to enter them each time. You can override the environment variable settings on the command line.
注意：把用户和密钥添加到CEPH_ARGS环境变量就不用每次都输入了。命令行下可以覆盖环境变量设置。
 3.3.5.4  向后兼容性
BACKWARD COMPATIBILITY
New in version Bobtail.
bobtail版新功能。
In Ceph Argonaut v0.48 and earlier versions, if you enable cephx authentication, Ceph only authenticates the initial communication between the client and daemon; Ceph does not authenticate the subsequent messages they send to each other, which has security implications. In Ceph Bobtail and subsequent versions, Ceph authenticates all ongoing messages between the entities using the session key set up for that initial authentication.
在ceph-0.48及更早版本，启用cephx认证后，ceph仅认证客户端和守护进程间的最初通讯，不会认证后续相互发送的消息，这导致了安全隐患；bobtail及后续版本会用认证后生成的会话密钥来认证所有消息。
We identified a backward compatibility issue between Argonaut v0.48 (and prior versions) and Bobtail (and subsequent versions). During testing, if you attempted to use Argonaut (and earlier) daemons with Bobtail (and later) daemons, the Argonaut daemons did not know how to perform ongoing message authentication, while the Bobtail versions of the daemons insist on authenticating message traffic subsequent to the initial request/response–making it impossible for Argonaut (and prior) daemons to interoperate with Bobtail (and subsequent) daemons.
我们确定了一个向后兼容性问题，在Argonaut v0.48（及之前版本）和Bobtail（及后续版本）之间。测试发现，如果你混用Argonaut（及更小版）和Bobtail的守护进程，Argonaut的守护进程将对正在进行的消息不知所措，因为Bobtail进程坚持要求认证最初请求/响应之后的消息，导致二者无法交互。
We have addressed this potential problem by providing a means for Argonaut (and prior) systems to interact with Bobtail (and subsequent) systems. Here’s how it works: by default, the newer systems will not insist on seeing signatures from older systems that do not know how to perform them, but will simply accept such messages without authenticating them. This new default behavior provides the advantage of allowing two different releases to interact. We do not recommend this as a long term solution. Allowing newer daemons to forgo ongoing authentication has the unfortunate security effect that an attacker with control of some of your machines or some access to your network can disable session security simply by claiming to be unable to sign messages.
我们已经提供了一种方法，解决了Argonaut（及之前）和Bobtail（及后续）系统间交互的潜在的问题。是这样解决的，默认情况下，较新系统不会再坚持要求较老系统的签名，只是简单地接收这些消息而不对其认证。这个默认行为使得两个不同版本可以交互，但我们不推荐作为长期方案。允许较新进程不认证正在进行的消息会导致安全问题，因为攻击者如果能控制你的机器、或访问你的网络，就可以宣称不能签署消息，从而禁用会话安全。
Note: Even if you don’t actually run any old versions of Ceph, the attacker may be able to force some messages to be accepted unsigned in the default scenario. While running Cephx with the default scenario, Ceph still authenticates the initial communication, but you lose desirable session security.
注意：即使你没有使用旧版的ceph，在默认配置下，攻击者也可以强制一些未签署消息被接受；虽然初始通讯认证通过了，但你失去了会话安全。
If you know that you are not running older versions of Ceph, or you are willing to accept that old servers and new servers will not be able to interoperate, you can eliminate this security risk. If you do so, any Ceph system that is new enough to support session authentication and that has Cephx enabled will reject unsigned messages. To preclude new servers from interacting with old servers, include the following in the [global] section of your Ceph configuration file directly below the line that specifies the use of Cephx for authentication:
如果你确定不会使用旧版ceph、或者新旧服务器不能交互无所谓，那就可以排除这个安全风险；如果你这样做了，任何支持会话认证、启用了cephx的ceph系统都会拒绝未签名的消息。要防止新服务器和旧服务器交互，在配置文件的[global]下添加下列这行，要加到启用cephx之后。
cephx require signatures = true    ; everywhere possible
You can also selectively require signatures for cluster internal communications only, separate from client-facing service:
你也可以选择只对集群内部通讯要求签名，它和面向客户端的服务是分离的：
cephx cluster require signatures = true    ; for cluster-internal communication
cephx service require signatures = true    ; for client-facing service
An option to make a client require signatures from the cluster is not yet implemented.
客户端向集群要求签名的选项还没实现。
We recommend migrating all daemons to the newer versions and enabling the foregoing flag at the nearest practical time so that you may avail yourself of the enhanced authentication.
我们推荐把所有进程迁移到较新版本，然后关闭兼容选项，以增强认证安全性。
 3.3.6  数据归置概览
Data placement overview
Ceph stores, replicates and rebalances data objects across a RADOS cluster dynamically. With many different users storing objects in different pools for different purposes on countless OSDs, Ceph operations require some data placement planning. The main data placement planning concepts in Ceph include:
ceph通过RADOS集群动态地存储、复制和重新均衡数据对象。很多不同用户因不同目的把对象存储在不同的存储池里，而它们都坐落于无数的OSD之上，所以ceph的运营需要些数据归置计划。ceph的数据归置计划概念主要有：

Pools: Ceph stores data within pools, which are logical groups for storing objects. Pools manage the number of placement groups, the number of replicas, and the ruleset for the pool. To store data in a pool, you must have an authenticated user with permissions for the pool. Ceph can snapshot pools. Future versions of Ceph will support namespaces within pools.
存储池：ceph在存储池内存储数据，它是对象存储的逻辑组；存储池管理着归置组数量、复制数量、和存储池规则集。要往存储池里存数据，用户必须认证过、且权限合适，存储池可做快照，它未来将支持名称空间功能。
Placement Groups: Ceph maps objects to placement groups (PGs). Placement groups (PGs) are shards or fragments of a logical object pool that place objects as a group into OSDs. Placement groups reduce the amount of per-object metadata when Ceph stores the data in OSDs. A larger number of placement groups (e.g., 100 per OSD) leads to better balancing.
归置组：ceph把对象映射到归置组（PG），归置组是一系列逻辑对象池的片段，这些对象分组后再存储到OSD，归置组减少了每对象元数据数量，更多的归置组（如每OSD 100个）使得均衡更好。
CRUSH Maps: CRUSH is a big part of what allows Ceph to scale without performance bottlenecks, without limitations to scalability, and without a single point of failure. CRUSH maps provide the physical topology of the cluster to the CRUSH algorithm to determine where the data for an object and its replicas should be stored, and how to do so across failure domains for added data safety among other things.
CRUSH图：CRUSH是使ceph能伸缩自如而没有性能瓶颈、没有扩展限制、没有单点故障，它为CRUSH算法提供集群的物理拓扑，以此确定一个对象的数据及它的副本应该在哪里、怎样才能越过故障域保证数据安全。

When you initially set up a test cluster, you can use the default values. Once you begin planning for a large Ceph cluster, refer to pools, placement groups and CRUSH for data placement operations. If you find some aspects challenging, Inktank provides excellent premium support for Ceph.
起初安装测试集群的时候，可以使用默认值。但开始规划一个大型ceph集群，做数据归置操作的时候会涉及存储池、归置组、和CRUSH。
 3.3.7  存储池
Pools
When you first deploy a cluster without creating a pool, Ceph uses the default pools for storing data. A pool differs from CRUSH’s location-based buckets in that a pool doesn’t have a single physical location, and a pool provides you with some additional functionality, including:
开始部署集群时没有创建存储池，ceph则用默认存储池存数据。存储池不同于CRUSH基于位置的桶，它没有单独的物理位置，而且存储池提供了一些额外的功能：
Replicas: You can set the desired number of copies/replicas of an object. A typical configuration stores an object and one additional copy (i.e., size = 2), but you can determine the number of copies/replicas.
复制：你可以设置一个对象期望的副本数量。典型配置存储一个对象和一个它的副本（如size = 2），但你可以更改副本的数量。
Placement Groups: You can set the number of placement groups for the pool. A typical configuration uses approximately 100 placement groups per OSD to provide optimal balancing without using up too many computing resources. When setting up multiple pools, be careful to ensure you set a reasonable number of placement groups for both the pool and the cluster as a whole.
归置组：你可以设置一个存储池的归置组数量。典型配置在每个OSD上使用大约100个归置组，这样，不用过多计算资源就得到了较优的均衡。设置多个存储池的时候，要注意为这些存储池和集群设置合理的归置组数量。
CRUSH Rules: When you store data in a pool, a CRUSH ruleset mapped to the pool enables CRUSH to identify a rule for the placement of the primary object and object replicas in your cluster. You can create a custom CRUSH rule for your pool.
CRUSH规则：当你在存储池里存数据的时候，映射到存储池的CRUSH规则集使得CRUSH确定一条规则，用于集群内主对象的归置和其副本的复制。你可以给存储池定制CRUSH规则。
Snapshots: When you create snapshots with ceph osd pool mksnap, you effectively take a snapshot of a particular pool.
快照：你用ceph osd pool mksnap创建快照的时候，实际上创建了一小部分存储池的快照。
Set Ownership: You can set a user ID as the owner of a pool.
设置所有者：你可以设置一个用户ID为一个存储池的所有者。
To organize data into pools, you can list, create, and remove pools. You can also view the utilization statistics for each pool.
要把数据组织到存储池里，你可以列出、创建、删除存储池，也可以查看每个存储池的利用率。
 3.3.7.1  列出存储池
LIST POOLS
To list your cluster’s pools, execute:
要列出集群的存储池，命令如下：
ceph osd lspools
The default pools include:
默认存储池有：
data
metadata
rbd
 3.3.7.2  创建存储池
CREATE A POOL
To create a pool, execute:
要创建一个存储池，执行：
ceph osd pool create {pool-name} {pg-num} [{pgp-num}]
Where:
参数含义如下：
{pool-name}
Description:
The name of the pool. It must be unique.
存储池名称，必须唯一。
Type:
String
Required:
Yes
{pg-num}
Description:
The total number of placement groups for the pool. See Placement Groups for details on calculating a suitable number. The default value 8 is NOT suitable for most systems.
存储池拥有的归置组总数。关于如何计算合适的数值，请参见Placement Groups。默认值8对大多数系统都不合适。
Type:
Integer
Required:
Yes
Default:
8
{pgp-num}
Description:
The total number of placement groups for placement purposes. This should be equal to the total number of placement groups, except for placement group splitting scenarios.
用于归置的归置组总数。此值应该等于归置组总数，归置组分割的情况下除外。
Type:
Integer
Required:
Yes
Default:
8
When you create a pool, set the number of placement groups to a reasonable value (e.g., 100). Consider the total number of placement groups per OSD too. Placement groups are computationally expensive, so performance will degrade when you have many pools with many placement groups (e.g., 50 pools with 100 placement groups each). The point of diminishing returns depends upon the power of the OSD host.
创建存储池时，要设置一个合理的归置组数量，如100。也要考虑到每OSD的归置组总数，因为归置组很耗计算资源，所以很多存储池和很多归置组（如50个存储池，各包含100归置组）会导致性能下降。收益递减点取决于OSD主机的强大。
Important: Increasing the number of placement groups in a pool after you create the pool is still an experimental feature in Bobtail (v 0.56). We recommend defining a reasonable number of placement groups and maintaining that number until Ceph’s placement group splitting and merging functionality matures.
重要：增加一现有存储池中的归置组数量仍然是v0.56新增的一个实验功能。我们建议创建时就指定一个合适的归置组数量并维持不变，直到ceph的归置组拆分、合并功能成熟。
See Placement Groups for details on calculating an appropriate number of placement groups for your pool.
如何为存储池计算合适的归置组数量请参见归置组。
 3.3.7.3  删除存储池
DELETE A POOL
To delete a pool, execute:
要删除一存储池，执行：
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
If you created your own rulesets and rules for a pool you created, you should consider removing them when you no longer need your pool. If you created users with permissions strictly for a pool that no longer exists, you should consider deleting those users too.
如果你给自建的存储池创建了定制的规则集，你不需要存储池时最好删除它。如果你曾严格地创建了用户及其权限给一个存储池，但存储池已不存在，最好也删除那些用户。
 3.3.7.4  重命名存储池
RENAME A POOL
To rename a pool, execute:
要重命名一个存储池，执行：
ceph osd pool rename {current-pool-name} {new-pool-name}
If you rename a pool and you have per-pool capabilities for an authenticated user, you must update the user’s capabilities (i.e., caps) with the new pool name.
如果重命名了一个存储池，且认证用户有每存储池能力，那你必须用新存储池名字更新用户的能力（如caps）。
Note: Version 0.48 Argonaut and above.
注意：适用0.48及以上。
 3.3.7.5  查看存储池统计信息
SHOW POOL STATISTICS
To show a pool’s utilization statistics, execute:
要查看某存储池的使用统计信息，执行命令：
rados df
 3.3.7.6  拍下存储池快照
MAKE A SNAPSHOT OF A POOL
To make a snapshot of a pool, execute:
要拍下某存储池的快照，执行命令：
ceph osd pool mksnap {pool-name} {snap-name}

Note: Version 0.48 Argonaut and above.
注意：适用0.48及以上。
 3.3.7.7  删除存储池快照
REMOVE A SNAPSHOT OF A POOL
To remove a snapshot of a pool, execute:
要删除某存储池的一个快照，执行命令：
ceph osd pool rmsnap {pool-name} {snap-name}

Note: Version 0.48 Argonaut and above.
注意：适用0.48及以上。
 3.3.7.8  设置存储池键值
SET POOL VALUES
To set a value to a pool, execute the following:
要设置一个存储池的选项值，执行命令：
ceph osd pool set {pool-name} {key} {value}
You may set values for the following keys:
你可以设置下列键的值：
size
Description:
Sets the number of replicas for objects in the pool. See Set the Number of Object Replicas for further details.
设置存储池中对象的副本数，详情参见设置对象副本数。
Type:
Integer
min_size
Description:
Sets the minimum number of replicas required for io. See Set the Number of Object Replicas for further details
设置IO需要的最小副本数，详情参见设置对象副本数。
Type:
Integer

Note: Version 0.54 and above
注意：0.54版及以上。
crash_replay_interval
Description:
The number of seconds to allow clients to replay acknowledged, but uncommitted requests.
允许客户端重放确认而未提交请求的秒数。
Type:
Integer
pgp_num
Description:
The effective number of placement groups to use when calculating data placement.
计算数据归置时使用的有效归置组数量。
Type:
Integer
Valid Range:
Equal to or less than pg_num.
crush_ruleset
Description:
The ruleset to use for mapping object placement in the cluster.
集群内映射对象归置时使用的规则集。
Type:
Integer

Note: Version 0.48 Argonaut and above.
注意：0.48版及以上。
 3.3.7.9  获取存储池键值
GET POOL VALUES
To set a value to a pool, execute the following:
要给一个存储池设置值，执行命令：
ceph osd pool get {pool-name} {key}
pg_num
Description:
The number of placement groups for the pool.
存储池的归置组数量。
Type:
Integer
pgp_num
Description:
The effective number of placement groups to use when calculating data placement.
计算数据归置时使用的归置组有效数量。
Type:
Integer
Valid Range:
Equal to or less than pg_num.
小于等于pg_num。

 3.3.7.10  设置对象副本数
SET THE NUMBER OF OBJECT REPLICAS
To set the number of object replicas, execute the following:
要设置对象副本数，执行命令：
ceph osd pool set {poolname} size {num-replicas}

Important: The {num-replicas} includes the object itself. If you want the object and two copies of the object for a total of three instances of the object, specify 3.
重要：{num-replicas}包括对象自身，如果你想要对象自身及其两份拷贝共计三份，指定3。
For example:
例如：
ceph osd pool set data size 3
You may execute this command for each pool.
你可以在每个存储池上执行这个命令。
Note, however, that pool size is more of a best-effort setting: an object might accept IOs in degraded mode with fewer than size replicas. To set a minimum number of required replicas for io, you should use the min_size setting.
注意，pool size是倾向于尽最大努力的设置：一个处于降级模式的对象其副本数小于设置值，但仍可接受IO请求。min_size选项可设置必需给IO的最小副本数。
For example:
例如：
ceph osd pool set data min_size 2
This ensures that no object in the data pool will receive IO with fewer than min_size replicas.
这确保数据存储池里任何副本数小于min_size的对象都不会收到IO了。
 3.3.7.11  获取对象副本数
GET THE NUMBER OF OBJECT REPLICAS
To get the number of object replicas, execute the following:
要获取对象副本数，执行命令：
ceph osd dump | grep 'rep size'
Ceph will list the pools, with the rep size attribute highlighted. By default, Ceph creates two replicas of an object (two copies).
ceph会列出存储池，且高亮rep size属性，它创建了一个对象的两个副本（两份拷贝）。
 3.3.8  归置组
Placement groups
A Placement Group (PG) aggregates a series of objects into a group, and maps the group to a series of OSDs. Tracking object placement and object metadata on a per-object basis is computationally expensive–i.e., a system with millions of objects cannot realistically track placement on a per-object basis. Placement groups address this barrier to performance and scalability. Additionally, placement groups reduce the number of processes and the amount of per-object metadata Ceph must track when storing and retrieving data.
一个归置组(PG)把一系列对象汇聚到一组，并且把这个组映射到一系列OSD。跟踪每个对象的位置和元数据需要大量计算。例如，一个拥有数百万对象的系统，不可能在每对象级追踪位置。归置组可应对这个影响性能和扩展性的问题，另外，归置组减小了ceph存储、检索数据时必须追踪的每对象元数据的处理量和尺寸。


Each placement group requires some amount of system resources:
每个归置组都需要一定量系统资源：
Directly: Each PG requires some amount of memory and CPU.
直接地：每个PG需要一些内存和CPU；
Indirectly: The total number of PGs increases the peering count.
间接地：PG总量增加了连接建立数量。
Increasing the number of placement groups reduces the variance in per-OSD load across your cluster. We recommend approximately 50-100 placement groups per OSD to balance out memory and CPU requirements and per-OSD load. For a single pool of objects, you can use the following formula:
增加PG数量能减小集群内每个OSD间的变迁，我们推荐每个OSD大约50-100个归置组，以均衡内存、CPU需求、和每OSD负载。对于单存储池里的对象，你可用下面的公式：
            (OSDs * 100)
Total PGs = ------------
              Replicas
When using multiple data pools for storing objects, you need to ensure that you balance the number of placement groups per pool with the number of placement groups per OSD so that you arrive at a reasonable total number of placement groups that provides reasonably low variance per OSD without taxing system resources or making the peering process too slow.
当用了多个数据存储池来存储数据时，你得确保均衡每个存储池的归置组数量、且归置组数量分摊到每个OSD，这样才能达到较合理的归置组总量，并因此使得每个OSD无需耗费过多系统资源或拖慢连接进程就能实现较小变迁。
 3.3.8.1  设置归置组数量
SET THE NUMBER OF PLACEMENT GROUPS
To set the number of placement groups in a pool, you must specify the number of placement groups at the time you create the pool.
你必须在创建存储池时设置一个存储池的归置组数量。
See Create a Pool for details.
详情参见创建存储池。
 3.3.8.2  获取归置组数量
GET THE NUMBER OF PLACEMENT GROUPS
To get the number of placement groups in a pool, execute the following:
要获取一个存储池的归置组数量，执行命令：
ceph osd pool get {pool-name} pg_num
 3.3.8.3  获取归置组统计信息
GET A CLUSTER’S PG STATISTICS
To get the statistics for the placement groups in your cluster, execute the following:
要获取集群里归置组的统计信息，执行命令：
ceph pg dump [--format {format}]
Valid formats are plain (default) and json.
可用格式有纯文本（默认）和json。
 3.3.8.4  获取卡住的归置组统计信息
GET STATISTICS FOR STUCK PGS
To get the statistics for all placement groups stuck in a specified state, execute the following:
要获取所有卡在某状态的归置组统计信息，执行命令：
ceph pg dump_stuck inactive|unclean|stale [--format <format>] [-t|--threshold <seconds>]
Inactive Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come up and in.
inactive（不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的OSD复活且进入集群。
Unclean Placement groups contain objects that are not replicated the desired number of times. They should be recovering.
unclean（不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。
Stale Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by mon_osd_report_timeout).
stale（不新鲜）归置组处于未知状态：存储它们的OSD有段时间没向监视器报告了（由 mon_osd_report_timeout配置）。

Valid formats are plain (default) and json. The threshold defines the minimum number of seconds the placement group is stuck before including it in the returned statistics (default 300 seconds).
可用格式有plain（默认）和json。阀值定义的是，归置组被认为卡住前等待的最小时间（默认300秒）。
 3.3.8.5  获取一归置组运行图
GET A PG MAP
To get the placement group map for a particular placement group, execute the following:
要获取一个具体归置组的归置组图，执行命令：
ceph pg map {pg-id}
For example:
例如：
ceph pg map 1.6c
Ceph will return the placement group map, the placement group, and the OSD status:
ceph将返回归置组图、归置组、和OSD状态：
osdmap e13 pg 1.6c (1.6c) -> up [1,0] acting [1,0]
 3.3.8.6  获取一PG的统计信息
GET A PGS STATISTICS
To retrieve statistics for a particular placement group, execute the following:
要查看一个具体归置组的统计信息，执行命令：
ceph pg {pg-id} query
 3.3.8.7  洗刷归置组
Scrub a placement group
To scrub a placement group, execute the following:
要洗刷一个归置组，执行命令：
ceph pg scrub {pg-id}
Ceph checks the primary and any replica nodes, generates a catalog of all objects in the placement group and compares them to ensure that no objects are missing or mismatched, and their contents are consistent. Assuming the replicas all match, a final semantic sweep ensures that all of the snapshot-related object metadata is consistent. Errors are reported via logs.
ceph检查原始的和任何复制节点，生成归置组里所有对象的目录，然后再对比，确保没有对象丢失或不匹配，并且它们的内容一致。
 3.3.8.8  恢复丢失的
REVERT LOST
If the cluster has lost one or more objects, and you have decided to abandon the search for the lost data, you must mark the unfound objects as lost.
如果集群丢了一或多个对象，而且必须放弃搜索这些数据，你就要把未找到的对象标记为丢失。
If all possible locations have been queried and objects are still lost, you may have to give up on the lost objects. This is possible given unusual combinations of failures that allow the cluster to learn about writes that were performed before the writes themselves are recovered.
如果所有可能的位置都查询过了，而仍找不到这些对象，你也许得放弃它们了。这可能是罕见的失败组合导致的，集群在写入完成前，未能得知写入是否已执行。
Currently the only supported option is “revert”, which will either roll back to a previous version of the object or (if it was a new object) forget about it entirely. To mark the “unfound” objects as “lost”, execute the following:
当前只支持revert选项，它使得回滚到对象的前一个版本（如果它是新对象）或完全忽略它。要把unfound对象标记为lost，执行命令：
ceph pg {pg-id} mark_unfound_lost revert

Important: Use this feature with caution, because it may confuse applications that expect the object(s) to exist.
重要：要谨慎使用，它可能迷惑那些期望对象存在的应用程序。
 3.3.8.8.1  归置组状态
Placement Group States
When checking a cluster’s status (e.g., running ceph -w or ceph -s), Ceph will report on the status of the placement groups. A placement group has one or more states. The optimum state for placement groups in the placement group map is active + clean.
检查集群状态时（如运行ceph -s或ceph -w），ceph会报告归置组状态。一个归置组有一到多种状态，其最优状态为active+clean。
Creating
Ceph is still creating the placement group.
ceph仍在创建归置组。
Active
Ceph will process requests to the placement group.
ceph可处理到归置组的请求。
Clean
Ceph replicated all objects in the placement group the correct number of times.
ceph把归置组内的对象复制了规定次数。
Down
A replica with necessary data is down, so the placement group is offline.
包含必备数据的副本挂了，所以归置组离线。
Replay
The placement group is waiting for clients to replay operations after an OSD crashed.
某OSD崩溃后，归置组在等待客户端重放操作。
Splitting
Ceph is splitting the placment group into multiple placement groups. (functional?)
ceph正在把一归置组分割为多个。（实现了？）
Scrubbing
Ceph is checking the placement group for inconsistencies.
ceph正在检查归置组的一致性。
Degraded
Ceph has not replicated some objects in the placement group the correct number of times yet.
归置组内的对象还没复制到规定次数。
Inconsistent
Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica after recovery finished, etc.).
ceph检测到了归置组内一或多个副本间不一致（如各对象大小不一、恢复后对象还没复制到副本那里、等等）。
Peering
The placement group is undergoing the peering process
归置组正在互联。
Repair
Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).
ceph正在检查归置组、并试图修复发现的不一致（如果可能的话）。
Recovering
Ceph is migrating/synchronizing objects and their replicas.
ceph正在迁移/同步对象及其副本。
Backfill
Ceph is scanning and synchronizing the entire contents of a placement group instead of inferring what contents need to be synchronized from the logs of recent operations. Backfill is a special case of recovery.
ceph正在扫描并同步整个归置组的内容，而不是根据日志推算哪些最新操作需要同步。Backfill是recovery的一种特殊情况。
Wait-backfill
The placement group is waiting in line to start backfill.
归置组正在排队，等候回填。
Incomplete
Ceph detects that a placement group is missing a necessary period of history from its log. If you see this state, report a bug, and try to start any failed OSDs that may contain the needed information.
ceph根据一归置组的日志检测到它丢失了一段历史。如果你看到这种状态，请报告一个bug，并试着重启所有包含相关信息的OSD。
Stale
The placement group is in an unknown state - the monitors have not received an update for it since the placement group mapping changed.
归置组处于一种未知状态——从归置组运行图变更起就没再收到它的更新。
Remapped
The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.
归置组被临时映射到了另外一组OSD，它们不是CRUSH算法指定的。
 3.3.8.8.2  归置组术语解释
Placement Group Concepts
When you execute commands like ceph -w, ceph osd dump, and other commands related to placement groups, Ceph may return values using some of the following terms:
当你执行诸如ceph -w、ceph osd dump、及其他和归置组相关的命令时，ceph会返回下列术语及其值：
Peering
The process of bringing all of the OSDs that store a Placement Group (PG) into agreement about the state of all of the objects (and their metadata) in that PG. Note that agreeing on the state does not mean that they all have the latest contents.
是一种过程，它使得存储着同一归置组的所有OSD对归置组内的所有对象及其元数据统一意见。需要注意的是，达成一致不意味着它们都有最新内容。
Acting Set
The ordered list of OSDs who are (or were as of some epoch) responsible for a particular placement group.
一列有序OSD，它们为某一特定归置组（或其中一些元版本）负责。
Up Set
The ordered list of OSDs responsible for a particular placment group for a particular epoch according to CRUSH. Normally this is the same as the Acting Set, except when the Acting Set has been explicitly overridden via pg_temp in the OSD Map.
［译者：此处原文没看懂……瞎译如下］
一列有序OSD，它们为某一特定元版本所在的特定归置组负责。它通常和Acting Set相同，除非Acting Set被OSD运行图里的pg_temp显式地覆盖掉了。
Current Interval or Past Interval
A sequence of OSD map epochs during which the Acting Set and Up Set for particular placement group do not change.
某一特定归置组所在Acting Set和Up Set未更改时的一系列OSD运行图元版本。
Primary
The member (and by convention first) of the Acting Set, that is responsible for coordination peering, and is the only OSD that will accept client-initiated writes to objects in a placement group.
Acting Set的成员（按惯例第一个），它负责协调互联，并且是归置组内惟一接受客户端初始写入的OSD。
Replica
A non-primary OSD in the Acting Set for a placement group (and who has been recognized as such and activated by the primary).
一归置组的Acting Set内不是主OSD的其它OSD，它们被同等对待、并由主OSD激活。
Stray
An OSD that is not a member of the current Acting Set, but has not yet been told that it can delete its copies of a particular placement group.
某一OSD，它不再是当前Acting Set的成员，但还没被告知它可以删除那个归置组拷贝。
Recovery
Ensuring that copies of all of the objects in a placement group are on all of the OSDs in the Acting Set. Once Peering has been performed, the Primary can start accepting write operations, and Recovery can proceed in the background.
确保Acting Set内、一归置组中的所有对象的副本都存在于所有OSD上。一旦互联完成，主OSD就以接受写操作，且恢复进程可在后台进行。
PG Info
Basic metadata about the placement group’s creation epoch, the version for the most recent write to the placement group, last epoch started, last epoch clean, and the beginning of the current interval. Any inter-OSD communication about placement groups includes the PG Info, such that any OSD that knows a placement group exists (or once existed) also has a lower bound on last epoch clean or last epoch started.
［译者：此处原文没看透……瞎译如下］
基本元数据，关于归置组创建元版本、向归置组的最新写版本、最近的开始元版本（last epoch started）、最近的干净元版本（last epoch clean）、和当前间隔（current interval）的起点。OSD间关于归置组的任何通讯都包含PG Info，这样任何知道一归置组存在（或曾经存在）的OSD也必定有last epoch clean或last epoch started的下限。
PG Log
A list of recent updates made to objects in a placement group. Note that these logs can be truncated after all OSDs in the Acting Set have acknowledged up to a certain point.
一归置组内对象的一系列最近更新。注意，这些日志在Acting Set内的所有OSD确认更新到某点后可以删除。
Missing Set
Each OSD notes update log entries and if they imply updates to the contents of an object, adds that object to a list of needed updates. This list is called the Missing Set for that <OSD,PG>.
各OSD都记录更新日志，而且如果它们包含对象内容的更新，会把那个对象加入一个待更新列表，这个列表叫做那个<OSD,PG>的Missing Set。
Authoritative History
A complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.
一个完整、完全有序的操作集合，如果应用，可把一OSD上的归置组副本带到最新。
Epoch
A (monotonically increasing) OSD map version number
单递增OSD运行图版本号。
Last Epoch Start
The last epoch at which all nodes in the Acting Set for a particular placement group agreed on an Authoritative History. At this point, Peering is deemed to have been successful.
一最新元版本，在这点上，一归置组所对应Acting Set内的所有节点都对权威历史达成了一致、并且互联被认为成功了。
up_thru
Before a Primary can successfully complete the Peering process, it must inform a monitor that is alive through the current osd map Epoch by having the monitor set its up_thru in the osd map. This helps Peering ignore previous Acting Sets for which Peering never completed after certain sequences of failures, such as the second interval below:
主OSD要想成功完成互联，它必须通过当前OSD运行图epoch通知一个监视器，并在osd运行图中设置其up_thru。这会使互联进程忽略之前的Acting Set，因为它经历特定顺序的失败后一直不能互联，比如像下面的第二周期：
acting set = [A,B]
acting set = [A]
acting set = [] very shortly after (e.g., simultaneous failure, but staggered detection)
acting set = [] 之后很短时间（例如同时失败、但探测是交叉的）
acting set = [B] (B restarts, A does not)
Last Epoch Clean
The last Epoch at which all nodes in the Acting set for a particular placement group were completely up to date (both placement group logs and object contents). At this point, recovery is deemed to have been completed.
最近的Epoch，这时某一特定归置组所在Acting Set内的所有节点都全部更新了（包括归置组日志和对象内容）。在这点上，恢复被认为已完成。
 3.3.9  CRUSH图
CRUSH MAPS
The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OSDs directly rather than through a centralized server or broker. With an algorithmically determined method of storing and retrieving data, Ceph avoids a single point of failure, a performance bottleneck, and a physical limit to its scalability.
CRUSH算法通过计算数据存储位置来确定如何存储和检索。CRUSH授权ceph客户端直接连接OSD，而非通过一个中央服务器或经纪人。数据存储、检索算法的使用，使ceph避免了单点失败、性能瓶颈、和伸缩的物理限制。
CRUSH requires a map of your cluster, and uses the CRUSH map to pseudo-randomly store and retrieve data in OSDs with a uniform distribution of data across the cluster. For a detailed discussion of CRUSH, see CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data
CRUSH需要一张集群的地图，且使用CRUSH把数据伪随机地存储、检索于整个集群的OSD里。CRUSH的讨论详情参见CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data。
CRUSH Maps contain a list of OSDs, a list of ‘buckets’ for aggregating the devices into physical locations, and a list of rules that tell CRUSH how it should replicate data in a Ceph cluster’s pools. By reﬂecting the underlying physical organization of the installation, CRUSH can model—and thereby address—potential sources of correlated device failures. Typical sources include physical proximity, a shared power source, and a shared network. By encoding this information into the cluster map, CRUSH placement policies can separate object replicas across different failure domains while still maintaining the desired distribution. For example, to address the possibility of concurrent failures, it may be desirable to ensure that data replicas are on devices in different shelves, racks, power supplies, controllers, and/or physical locations.
CRUSH图包含OSD列表、把设备汇聚为物理位置的“桶”列表、和指示CRUSH如何复制存储池里的数据的规则列表。由于对所安装底层物理组织的表达，CRUSH能模型化、并因此定位到潜在的相关失败设备源头，典型的源头有物理距离、共享电源、和共享网络，把这些信息编码到集群图里，CRUSH归置策略可把对象副本分离到不同的失败域，却仍能保持期望的分布。例如，要定位同时失败的可能性，可能希望保证数据复制到的设备位于不同机架、不同托盘、不同电源、不同控制器、甚至不同物理位置。
When you create a configuration file and deploy Ceph with mkcephfs, Ceph generates a default CRUSH map for your configuration. The default CRUSH map is fine for your Ceph sandbox environment. However, when you deploy a large-scale data cluster, you should give significant consideration to developing a custom CRUSH map, because it will help you manage your Ceph cluster, improve performance and ensure data safety.
当你写好配置文件，用mkcephfs部署ceph后，它生成了一个默认的CRUSH图，对于你的沙盒环境来说它很好。然而，部署一个大规模数据集群的时候，应该好好设计自己的CRUSH图，因为它帮你管理ceph集群、提升性能、和保证数据安全性。
For example, if an OSD goes down, a CRUSH Map can help you can locate the physical data center, room, row and rack of the host with the failed OSD in the event you need to use onsite support or replace hardware.
例如，如果一个OSD挂了，CRUSH图可帮你定位此事件中OSD所在主机的物理数据中心、房间、行和机架，据此你可以请求在线支持或替换硬件。
Similarly, CRUSH may help you identify faults more quickly. For example, if all OSDs in a particular rack go down simultaneously, the fault may lie with a network switch or power to the rack or the network switch rather than the OSDs themselves.
类似地，CRUSH可帮你更快地找出问题。例如，如果一个机架上的所有OSD同时挂了，问题可能在于机架的交换机或电源，而非OSD本身。
A custom CRUSH map can also help you identify the physical locations where Ceph stores redundant copies of data when the placement group(s) associated with a failed host are in a degraded state.
定制的CRUSH图也能在归置组降级时，帮你找出冗余副本所在主机的物理位置。
Inktank provides excellent premium support for developing CRUSH maps.
Inktank提供优秀的商业支持，帮您开发CRUSH图。
Note: Lines of code in example boxes may extend past the edge of the box. Please scroll when reading or copying longer examples.
注意：文本框里的代码实例可能超出了边界，读或拷贝时注意滚动。
 3.3.9.1  编辑CRUSH图
EDITING A CRUSH MAP
To edit an existing CRUSH map:
要编辑现有的CRUSH图：
1. Get the CRUSH Map.
2. Decompile the CRUSH Map.
3. Edit at least one of Devices, Buckets and Rules.
4. Recompile the CRUSH Map.
5. Set the CRUSH Map.

1. 获取CRUSH图；
2. 反编译CRUSH图；
3. 至少编辑一个设备、桶、规则；
4. 重编译CRUSH图；
5. 应用CRUSH图。
To activate CRUSH Map rules for a specific pool, identify the common ruleset number for those rules and specify that ruleset number for the pool. See Set Pool Values for details.
要激活CRUSH图里某存储池的规则，找到通用规则集编号，然后把它指定到那个规则集。详情参见设置存储池键值。
 3.3.9.1.1  获取CRUSH图
GET A CRUSH MAP
To get the CRUSH Map for your cluster, execute the following:
要获取集群的CRUSH图，执行命令：
ceph osd getcrushmap -o {compiled-crushmap-filename}
Ceph will output (-o) a compiled CRUSH Map to the filename you specified. Since the CRUSH Map is in a compiled form, you must decompile it first before you can edit it.
ceph将把CRUSH输出(-o)到你指定的文件，由于CRUSH图是已编译的，所以编辑前必须先反编译。
 3.3.9.1.2  反编译CRUSH图
DECOMPILE A CRUSH MAP
To decompile a CRUSH Map, execute the following:
要反编译CRUSH图，执行命令：
crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename}
Ceph will decompile (-d) the compiled CRUSH map and output (-o) it to the filename you specified.
ceph将反编译(-d)二进制CRUSH图，且输出(-o)到你指定的文件。
 3.3.9.1.3  编译CRUSH图
COMPILE A CRUSH MAP
To compile a CRUSH Map, execute the following:
要编译CRUSH图，执行命令：
crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename}
Ceph will store a compiled CRUSH map to the filename you specified.
ceph将把已编译的CRUSH图保存到你指定的文件。
 3.3.9.1.4  设置CRUSH图
SET A CRUSH MAP
To set the CRUSH Map for your cluster, execute the following:
要把CRUSH图应用到集群，执行命令：
ceph osd setcrushmap -i  {compiled-crushmap-filename}
Ceph will input the compiled CRUSH Map of the filename you specified as the CRUSH Map for the cluster.
ceph将把你指定的已编译CRUSH图输入到集群。
 3.3.9.2  CRUSH图参数
CRUSH MAP PARAMETERS
There are four main sections to a CRUSH Map.
CRUSH图主要有4个主要段落。
1. Devices: Devices consist of any object storage device–i.e., the storage drive corresponding to a ceph-osd daemon. You should have a device for each OSD daemon in your Ceph configuration file.
devices由任意对象存储设备组成，如对应一个ceph-osd进程的存储器。ceph配置文件里的每个OSD都应该有一个设备。
2. Bucket Types: Bucket types define the types of buckets used in your CRUSH hierarchy. Buckets consist of a hierarchical aggregation of storage locations (e.g., rows, racks, hosts, etc.) and their assigned weights.
bucket types（桶类型）：定义了CRUSH分级结构里要用的桶类型，桶由逐级汇聚的存储位置（如行、机柜、主机等等）及其权重组成。
3. Bucket Instances: Once you define bucket types, you must declare bucket instances for your hosts, and any other failure domain partitioning you choose.
Bucket Instances:定义了桶类型后，还必须声明主机的桶类型、以及规划的其它故障域。
4. Rules: Rules consist of the manner of selecting buckets.
rules:由选择桶的方法组成。
If you launched Ceph using one of our Quick Start guides, you’ll notice that you didn’t need to create a CRUSH map. Ceph’s deployment tools generate a default CRUSH map that lists devices from the OSDs you defined in your Ceph configuration file, and it declares a bucket for each host you specified in the [osd] sections of your Ceph configuration file. You should create your own CRUSH maps with buckets that reflect your cluster’s failure domains to better ensure data safety and availability.
如果你用我们的某个Quick Start Guide配起了ceph，应该注意到了，你并不需要创建CRUSH运行图。ceph部署工具生成了默认CRUSH运行图，它列出了你定义在ceph配置文件中的OSD设备、并把配置文件[osd]段下定义的各OSD主机声明为桶。为保证数据安全和可用，你应该创建自己的CRUSH图，以反映出自己集群的故障域。
Note: The generated CRUSH map doesn’t take your larger grained failure domains into account. So you should modify your CRUSH map to account for larger grained failure domains such as racks, rows, data centers, etc.
注意：生成的CRUSH图没考虑大粒度故障域，所以你修改CRUSH图时要考虑上，像机柜、行、数据中心。
 3.3.9.2.1  CRUSH图之设备
CRUSH MAP DEVICES
To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices (i.e., the name of the OSD daemon). The list of devices appears first in the CRUSH Map. To declare a device in the CRUSH map, create a new line under your list of devices, enter device followed by a unique numeric ID, followed by the corresponding ceph-osd daemon instance.
为把归置组映射到OSD，CRUSH图需要OSD列表（如OSD守护进程名称），所以它们首先出现在CRUSH图里。要在CRUSH图里声明一个设备，在设备列表后面新建一行，输入device、之后是唯一的数字ID、之后是相应的ceph-osd守护进程例程名字。
#devices
device {num} {osd.name}
For example:
例如：
#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3
As a general rule, an OSD daemon maps to a single disk or to a RAID.
一般来说，一个OSD映射到一个单独的硬盘或RAID。
 3.3.9.2.2  CRUSH图之桶类型
CRUSH Map Bucket Types
The second list in the CRUSH map defines ‘bucket’ types. Buckets facilitate a hierarchy of nodes and leaves. Node (or non-leaf) buckets typically represent physical locations in a hierarchy. Nodes aggregate other nodes or leaves. Leaf buckets represent ceph-osd daemons and their corresponding storage media.
CRUSH图里的第二个列表定义了bucket（桶）类型，桶简化了节点和叶子层次。节点（或非叶）桶在分级结构里一般表示物理位置，节点汇聚了其它节点或叶子，叶桶表示ceph-osd守护进程及其对应的存储媒体。
Tip: The term “bucket” used in the context of CRUSH means a node in the hierarchy, i.e. a location or a piece of physical hardware. It is a different concept from the term “bucket” when used in the context of RADOS Gateway APIs.
提示：CRUSH中用到的bucket意思是分级结构中的一个节点，比如一个位置或一部分硬件。但是在RADOS网关接口的术语中，它又是不同的概念。
To add a bucket type to the CRUSH map, create a new line under your list of bucket types. Enter type followed by a unique numeric ID and a bucket name. By convention, there is one leaf bucket and it is type 0; however, you may give it any name you like (e.g., osd, disk, drive, storage, etc.):
要往CRUSH图中增加一种bucket类型，在现有桶类型列表下方新增一行，输入type、之后是惟一数字ID和一个桶名。按惯例，会有一个叶子桶为type 0;，然而你可以指定任何名字（如osd、disk、drive、storage等等）：
#types
type {num} {bucket-name}
For example:
例如：
# types
type 0 osd
type 1 host
type 2 rack
 3.3.9.2.3  CRUSH图之桶层次
CRUSH Map Bucket Hierarchy
The CRUSH algorithm distributes data objects among storage devices according to a per-device weight value, approximating a uniform probability distribution. CRUSH distributes objects and their replicas according to the hierarchical cluster map you define. Your CRUSH map represents the available storage devices and the logical elements that contain them.
CRUSH算法根据各设备的权重、大致统一的概率把数据对象分布到存储设备中。CRUSH根据你定义的集群运行图分布对象及其副本，CRUSH图表达了可用存储设备以及包含它们的逻辑单元。
To map placement groups to OSDs across failure domains, a CRUSH map defines a hierarchical list of bucket types (i.e., under #types in the generated CRUSH map). The purpose of creating a bucket hierarchy is to segregate the leaf nodes by their failure domains, such as hosts, racks, rows, rooms, and data centers. With the exception of the leaf nodes representing OSDs, the rest of the hierarchy is arbitrary, and you may define it according to your own needs.
要把归置组映射到跨故障域的OSD，一个CRUSH图需定义一系列分级桶类型（如在现有CRUSH图的#type下）。创建桶分级结构的目的是按故障域隔离叶节点，像主机、机柜、行、房间、和数据中心。除了表示叶节点的OSD，其它分级结构都是任意的，你可以按需定义。
We recommend adapting your CRUSH map to your firms’s hardware naming conventions and using instances names that reflect the physical hardware. Your naming practice can make it easier to administer the cluster and troubleshoot problems when an OSD and/or other hardware malfunctions and the administrator need access to physical hardware.
我们建议CRUSH图内的命名符合贵公司的硬件命名规则，并且采用反映物理硬件的例程名。良好的命名可简化集群管理和故障排除，当OSD和/或其它硬件出问题时，管理员可轻易找到对应物理硬件。
In the following example, the bucket hierarchy has a leaf bucket named osd, and two node buckets named host and rack respectively.
在下例中，桶分级结构有一个分支名为osd、和两个节点分别名为host和rack。

Note: The higher numbered rack bucket type aggregates the lower numbered host bucket type.
注意：编号较高的rack桶类型汇聚编号较低的host桶类型。
Since leaf nodes reflect storage devices declared under the #devices list at the beginning of the CRUSH map, you do not need to declare them as bucket instances. The second lowest bucket type in your hierarchy usually aggregates the devices (i.e., it’s usually the computer containing the storage media, and uses whatever term you prefer to describe it, such as “node”, “computer”, “server,” “host”, “machine”, etc.).
位于CRUSH图起始部分的#devices列表表示叶节点的存储设备，没必要声明为桶例程。位于分级结构第二低层的桶一般用于汇聚设备（如它通常是包含存储媒体的计算机，你可以用自己喜欢的名字描述，如节点、计算机、服务器、主机、机器等等）。
When declaring a bucket instance, you must specify its type, give it a unique name (string), assign it a unique ID expressed as a negative integer (optional), specify a weight relative to the total capacity/capability of its item(s), specify the bucket algorithm (usually straw), and the hash (usually 0, reflecting hash algorithm rjenkins1). A bucket may have one or more items. The items may consist of node buckets or leaves. Items may have a weight that reflects the relative weight of the item.
声明一个桶例程时，你必须指定其类型、惟一名称（字符串）、惟一负整数ID（可选）、指定和各条目总容量/能力相关的权重、指定桶算法（通常是straw）、和哈希（通常为0，表示哈希算法rjenkins1）。一个桶可以包含一到多条，这些条目可以由节点桶或叶子组成，它们可以有个权重用来反映条目的相对权重。
You may declare a node bucket with the following syntax:
你可以按下列语法声明一个节点桶：
[bucket-type] [bucket-name] {
        id [a unique negative numeric ID]
        weight [the relative capacity/capability of the item(s)]
        alg [the bucket type: uniform | list | tree | straw ]
        hash [the hash type: 0 by default]
        item [item-name] weight [weight]
}
For example, using the diagram above, we would define two host buckets and one rack bucket. The OSDs are declared as items within the host buckets:
例如，用上面的图表，我们可以定义两个主机桶和一个机柜桶，OSD被声明为主机桶内的条目：
host node1 {
        id -1
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

host node2 {
        id -2
        alg straw
        hash 0
        item osd.2 weight 1.00
        item osd.3 weight 1.00
}

rack rack1 {
        id -3
        alg straw
        hash 0
        item node1 weight 2.00
        item node2 weight 2.00
}

Note: In the foregoing example, note that the rack bucket does not contain any OSDs. Rather it contains lower level host buckets, and includes the sum total of their weight in the item entry.
注意：在前述示例中，机柜桶不包含任何OSD，它只包含低一级的主机桶、以及其内条目的权重之和。

桶类型
Bucket Types
Ceph supports four bucket types, each representing a tradeoff between performance and reorganization efficiency. If you are unsure of which bucket type to use, we recommend using a straw bucket. For a detailed discussion of bucket types, refer to CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data, and more specifically to Section 3.4. The bucket types are:
ceph支持四种桶，每种都是性能和组织简易间的折衷。如果你不确定用哪种桶，我们建议straw，关于各类桶的详细讨论见CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data，特别是Section 3.4。支持的桶类型有：
1. Uniform: Uniform buckets aggregate devices with exactly the same weight. For example, when firms commission or decommission hardware, they typically do so with many machines that have exactly the same physical configuration (e.g., bulk purchases). When storage devices have exactly the same weight, you may use the uniform bucket type, which allows CRUSH to map replicas into uniform buckets in constant time. With non-uniform weights, you should use another bucket algorithm.
Uniform: 这种桶用完全相同的权重汇聚设备。例如，公司采购或淘汰硬件时，一般都有相同的物理配置（如批发）。当存储设备权重都相同时，你可以用uniform桶类型，它允许CRUSH按常数把副本映射到uniform桶。权重不统一时，你应该采用其它算法。
2. List: List buckets aggregate their content as linked lists. Based on the P algorithm, a list is a natural and intuitive choice for an expanding cluster: either an object is relocated to the newest device with some appropriate probability, or it remains on the older devices as before. The result is optimal data migration when items are added to the bucket. Items removed from the middle or tail of the list, however, can result in a signiﬁcant amount of unnecessary movement, making list buckets most suitable for circumstances in which they never (or very rarely) shrink.
list: 这种桶把它们的内容汇聚为链表。它基于P算法，一个列表就是一个自然、直观的扩张集群：对象会按一定概率被重定位到最新的设备、或者像从前一样仍保留在较老的设备上。结果是优化了新条目加入桶时的数据迁移。然而，如果从链表的中间或末尾删除了一些条目，将会导致大量没必要的挪动。所以这种桶适合永不或极少缩减的场景。
3. Tree: Tree buckets use a binary search tree. They are more efficient than list buckets when a bucket contains a larger set of items. Based on the R algorithm, tree buckets reduce the placement time to O(log n), making them suitable for managing much larger sets of devices or nested buckets.
tree: 它用一种二进制搜索树，在桶包含大量条目时比list桶更高效。它基于R算法，tree桶把归置时间减少到了O(log n)，这使得它们更适合管理更大规模的设备或嵌套桶。
4. Straw: List and Tree buckets use a divide and conquer strategy in a way that either gives certain items precedence (e.g., those at the beginning of a list) or obviates the need to consider entire subtrees of items at all. That improves the performance of the replica placement process, but can also introduce suboptimal reorganization behavior when the contents of a bucket change due an addition, removal, or re-weighting of an item. The straw bucket type allows all items to fairly “compete” against each other for replica placement through a process analogous to a draw of straws.
straw: list和tree桶用分而治之策略，给特定条目一定优先级（如位于链表开头的条目）、或避开对整个子树上所有条目的考虑。这样提升了副本归置进程的性能，但是也导致了重新组织时的次优结果，如增加、拆除、或重设某条目的权重。straw桶类型允许所有条目模拟拉稻草的过程公平地相互“竞争”副本归置。

Hash
Each bucket uses a hash algorithm. Currently, Ceph supports rjenkins1. Enter 0 as your hash setting to select rjenkins1.
各个桶都用了一种哈希算法，当前ceph仅支持rjenkins1，输入0表示哈希算法设置为rjenkins1。

调整桶的权重
Weighting Bucket Items
Ceph expresses bucket weights as double integers, which allows for fine weighting. A weight is the relative difference between device capacities. We recommend using 1.00 as the relative weight for a 1TB storage device. In such a scenario, a weight of 0.5 would represent approximately 500GB, and a weight of 3.00 would represent approximately 3TB. Higher level buckets have a weight that is the sum total of the leaf items aggregated by the bucket.
ceph用双整形表示桶权重。权重和设备容量不同，我们建议用1.00作为1TB存储设备的相对权重，这样0.5的权重大概代表500GB、3.00大概代表3TB。较高级桶的权重是所有枝叶桶的权重之和。
A bucket item weight is one dimensional, but you may also calculate your item weights to reflect the performance of the storage drive. For example, if you have many 1TB drives where some have relatively low data transfer rate and the others have a relatively high data transfer rate, you may weight them differently, even though they have the same capacity (e.g., a weight of 0.80 for the first set of drives with lower total throughput, and 1.20 for the second set of drives with higher total throughput).
一个桶的权重是一维的，你也可以计算条目权重来反映存储设备性能。例如，如果你有很多1TB的硬盘，其中一些数据传输速率相对低、其他的数据传输率相对高，即使它们容量相同，也应该设置不同的权重（如给吞吐量较低的硬盘设置权重0.8，较高的设置1.2）。
 3.3.9.2.4  CRUSH图之规则
CRUSH MAP RULES
CRUSH maps support the notion of ‘CRUSH rules’, which are the rules that determine data placement for a pool. For large clusters, you will likely create many pools where each pool may have its own CRUSH ruleset and rules. The default CRUSH map has a rule for each pool, and one ruleset assigned to each of the default pools, which include:
CRUSH图支持CRUSH规则概念，用以确定一个存储池里数据的归置。对大型集群来说，你可能创建很多存储池，且每个存储池都有它自己的CRUSH规则集和规则。默认的CRUSH图里，每个存储池有一条规则、一个规则集被分配到每个默认存储池，它们有：
data
metadata
rbd

Note: In most cases, you will not need to modify the default rules. When you create a new pool, its default ruleset is 0.
注意：大多数情况下，你都不需要修改默认规则。新创建存储池的默认规则集是0。
CRUSH rules deﬁnes placement and replication strategies or distribution policies that allow you to specify exactly how CRUSH places object replicas. For example, you might create a rule selecting a pair of targets for 2-way mirroring, another rule for selecting three targets in two different data centers for 3-way mirroring, and yet another rule for RAID-4 over six storage devices. For a detailed discussion of CRUSH rules, refer to CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data, and more specifically to Section 3.2.
CRUSH规则定义了归置和复制策略、或分布策略，用它可以规定CRUSH如何放置对象副本。例如，你也许想创建一条规则用以选择一对目的地做双路复制；另一条规则用以选择位于两个数据中心的三个目的地做三路镜像；又一条规则用6个设备做RAID-4。关于CRUSH规则的详细研究见CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data，主要是Section 3.2。
A rule takes the following form:
规则格式如下：
rule <rulename> {

        ruleset <ruleset>
        type [ replicated | raid4 ]
        min_size <min-size>
        max_size <max-size>
        step take <bucket-type>
        step [choose|chooseleaf] [firstn|indep] <N> <bucket-type>
        step emit
}
ruleset
Description:
A means of classifying a rule as belonging to a set of rules. Activated by setting the ruleset in a pool.
区分一条规则属于某个规则集的手段。在存储池里设置ruleset后激活。
Purpose:
A component of the rule mask.
规则掩码的一个组件。
Type:
Integer
Required:
Yes
Default:
0
type
Description:
Describes a rule for either a storage drive (replicated) or a RAID.
为硬盘（复制的）或RAID写一条规则。
Purpose:
A component of the rule mask.
Type:
String
Required:
Yes
Default:
replicated
Valid Values:
Currently only replicated
min_size
Description:
If a pool makes fewer replicas than this number, CRUSH will NOT select this rule.
如果一个归置组副本数小于此数，CRUSH将不应用此规则。
Type:
Integer
Purpose:
A component of the rule mask.
Required:
Yes
Default:
1
max_size
Description:
If a pool makes more replicas than this number, CRUSH will NOT select this rule.
如果一个归置组副本数大于此数，CRUSH将不应用此规则。
Type:
Integer
Purpose:
A component of the rule mask.
Required:
Yes
Default:
10
step take <bucket-type>
Description:
Takes a bucket name, and begins iterating down the tree.
选取桶名并类推到树底。
Purpose:
A component of the rule.
Required:
Yes
Example:
step take data
step choose firstn {num} type {bucket-type}
Description:
Selects the number of buckets of the given type. The number is usually the number of replicas in the pool (i.e., pool size).
If {num} == 0, choose pool-num-replicas buckets (all available).
If {num} > 0 && < pool-num-replicas, choose that many buckets.
If {num} < 0, it means pool-num-replicas - {num}.
选取指定类型桶的数量，这个数字通常是存储池的副本数（如pool size）。
如果{num}==0选择pool-num-replicas个桶（所有可用的）。
如果{num}>0 && <pool-num-replicas就选择那么多的桶；
如果{num}<0它意为pool-num-replicas-{num}；
Purpose:
A component of the rule.
Prerequisite:
Follows step take or step choose.
Example:
step choose firstn 1 type row
step chooseleaf firstn {num} type {bucket-type}
Description:
Selects a set of buckets of {bucket-type} and chooses a leaf node from the subtree of each bucket in the set of buckets. The number of buckets in the set is usually the number of replicas in the pool (i.e., pool size).
选择{bucket-type}类型的一堆桶，并从各桶的子树里选择一个叶子节点。集合内桶的数量通常是存储池的副本数（如pool size）。
If {num} == 0, choose pool-num-replicas buckets (all available).
If {num} > 0 && < pool-num-replicas, choose that many buckets.
If {num} < 0, it means pool-num-replicas - {num}.
Purpose:
A component of the rule. Usage removes the need to select a device using two steps.
规则的一组件。它的使用避免了通过两步来选择一设备。
Prerequisite:
Follows step take or step choose.
Example:
step chooseleaf firstn 0 type row
step emit
Description:
Outputs the current value and empties the stack. Typically used at the end of a rule, but may also be used to pick from different trees in the same rule.
输出当前值并清空堆栈。通常用于规则末尾，也适用于相同规则应用到不同树的情况。
Purpose:
A component of the rule.
规则组件。
Prerequisite:
Follows step choose.
Example:
step emit

Important: To activate one or more rules with a common ruleset number to a pool, set the ruleset number to the pool.
重要：要把规则集编号设置到存储池，才能用一个通用规则集编号激活一或多条规则。
 3.3.9.3  不同存储池置于不同OSD
Placing Different Pools on Different OSDS:
Suppose you want to have most pools default to OSDs backed by large hard drives, but have some pools mapped to OSDs backed by fast solid-state drives (SSDs). It’s possible to have multiple independent CRUSH heirarchies within the same CRUSH map. Define two hierachies with two different root nodes–one for hard disks (e.g., “root platter”) and one for SSDs (e.g., “root ssd”) as shown below:
假设你想让大多数存储池坐落到使用大硬盘的OSD上，但是其中一些存储池映射到使用高速SSD的OSD上。在同一个CRUSH图内有多个独立的CRUSH树是可能的，定义两棵树、分别有自己的根节点——一个用于硬盘（如root platter）、一个用于SSD（如root ssd），如：
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3
device 4 osd.4
device 5 osd.5
device 6 osd.6
device 7 osd.7

      host ceph-osd-ssd-server-1 {
              id -1
              alg straw
              hash 0
              item osd.0 weight 1.00
              item osd.1 weight 1.00
      }

      host ceph-osd-ssd-server-2 {
              id -2
              alg straw
              hash 0
              item osd.2 weight 1.00
              item osd.3 weight 1.00
      }

      host ceph-osd-platter-server-1 {
              id -3
              alg straw
              hash 0
              item osd.4 weight 1.00
              item osd.5 weight 1.00
      }

      host ceph-osd-platter-server-2 {
              id -4
              alg straw
              hash 0
              item osd.6 weight 1.00
              item osd.7 weight 1.00
      }

      root platter {
              id -5
              alg straw
              hash 0
              item ceph-osd-platter-server-1 weight 2.00
              item ceph-osd-platter-server-2 weight 2.00
      }

      root ssd {
              id -6
              alg straw
              hash 0
              item ceph-osd-ssd-server-1 weight 2.00
              item ceph-osd-ssd-server-2 weight 2.00
      }

      rule data {
              ruleset 0
              type replicated
              min_size 2
              max_size 2
              step take platter
              step chooseleaf firstn 0 type host
              step emit
      }

      rule metadata {
              ruleset 1
              type replicated
              min_size 0
              max_size 10
              step take platter
              step chooseleaf firstn 0 type host
              step emit
      }

      rule rbd {
              ruleset 2
              type replicated
              min_size 0
              max_size 10
              step take platter
              step chooseleaf firstn 0 type host
              step emit
      }

      rule platter {
              ruleset 3
              type replicated
              min_size 0
              max_size 10
              step take platter
              step chooseleaf firstn 0 type host
              step emit
      }

      rule ssd {
              ruleset 4
              type replicated
              min_size 0
              max_size 10
              step take ssd
              step chooseleaf firstn 0 type host
              step emit
      }

      rule ssd-primary {
              ruleset 4
              type replicated
              min_size 0
              max_size 10
              step take ssd
              step chooseleaf firstn 1 type host
              step emit
              step take platter
              step chooseleaf firstn -1 type host
              step emit
      }
You can then set a pool to use the SSD rule by:
然后你可以设置一个存储池，让它使用SSD规则：
ceph osd pool set <poolname> crush_ruleset 4
Similarly, using the ssd-primary rule will cause each placement group in the pool to be placed with an SSD as the primary and platters as the replicas.
同样，用ssd-primary规则将使存储池内的各归置组用SSD作主OSD，普通硬盘作副本。
 3.3.9.4  增加/移动OSD
ADD/MOVE AN OSD
To add or move an OSD in the CRUSH map of a running cluster, execute the ceph osd crush set. For Argonaut (v 0.48), execute the following:
要增加或删除在线集群里OSD所对应的CRUSH图条目，执行ceph osd crush set命令。对于v0.48版，执行下列：
ceph osd crush set {id} {name} {weight} pool={pool-name}  [{bucket-type}={bucket-name} ...]
For Bobtail (v 0.56), execute the following:
bobtail (v0.56)可执行下列：
ceph osd crush set {id-or-name} {weight} root={pool-name}  [{bucket-type}={bucket-name} ...]
Where:
其中：
id
Description:
The numeric ID of the OSD.
OSD的数字标识符。
Type:
Integer
Required:
Yes
Example:
0
name
Description:
The full name of the OSD.
OSD的全名。
Type:
String
Required:
Yes
Example:
osd.0
weight
Description:
The CRUSH weight for the OSD.
OSD的CRUSH权重。
Type:
Double
Required:
Yes
Example:
2.0
root
Description:
The root of the tree in which the OSD resides.
OSD所在树的根。
Type:
Key/value pair.
Required:
Yes
Example:
root=default
bucket-type
Description:
You may specify the OSD’s location in the CRUSH hierarchy.
定义OSD在CRUSH分级结构中的位置。
Type:
Key/value pairs.
Required:
No
Example:
datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1

The following example adds osd.0 to the hierarchy, or moves the OSD from a previous location.
下例把osd.0添加到分级结构里、或者说从前一个位置挪动一下。
ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
 3.3.9.5  调整一OSD的CRUSH权重
ADJUST AN OSD’S CRUSH WEIGHT
To adjust an OSD’s crush weight in the CRUSH map of a running cluster, execute the following:
要调整在线集群中一OSD的CRUSH权重，执行命令：
ceph osd crush reweight {name} {weight}
Where:
其中：
name
Description:
The full name of the OSD.
OSD的全名。
Type:
String
Required:
Yes
Example:
osd.0
weight
Description:
The CRUSH weight for the OSD.
OSD的CRUSH权重。
Type:
Double
Required:
Yes
Example:
2.0

 3.3.9.6  删除OSD
REMOVE AN OSD
To remove an OSD from the CRUSH map of a running cluster, execute the following:
要从在线集群里把一OSD踢出CRUSH图，执行命令：
ceph osd crush remove {name}
Where:
其中：
name
Description:
The full name of the OSD.
OSD全名。
Type:
String
Required:
Yes
Example:
osd.0

 3.3.9.7  移动桶
MOVE A BUCKET
To move a bucket to a different location or position in the CRUSH map hierarchy, execute the following:
要把一个桶挪动到CRUSH图里的不同位置，执行命令：
ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]
Where:
其中：
bucket-name
Description:
The name of the bucket to move/reposition.
要移动或复位的桶名。
Type:
String
Required:
Yes
Example:
foo-bar-1
bucket-type
Description:
You may specify the bucket’s location in the CRUSH hierarchy.
你可以指定桶在CRUSH分级结构里的位置。
Type:
Key/value pairs.
Required:
No
Example:
datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1

 3.3.9.8  可调选项
TUNABLES
New in version 0.48.
0.48时加入。
There are several magic numbers that were used in the original CRUSH implementation that have proven to be poor choices. To support the transition away from them, newer versions of CRUSH (starting with the v0.48 argonaut series) allow the values to be adjusted or tuned.
在CRUSH最初实现时加入的几个幻数，现在看来已成问题。作为过渡方法，较新版的CRUSH（从0.48起）允许调整这些值。
Clusters running recent Ceph releases support using the tunable values in the CRUSH maps. However, older clients and daemons will not correctly interact with clusters using the “tuned” CRUSH maps. To detect this situation, there is now a feature bit CRUSH_TUNABLES (value 0x40000) and CRUSH_TUNABLES2 to reflect support for tunables.
最近发布的ceph允许在CRUSH图里使用可调值，然而老客户端和守护进程不会正确地和调整过的CRUSH图交互，为应对这种情况，现在多了个功能位CRUSH_TUNABLES（值 0x40000）和CRUSH_TUNABLES2 来反映支持可调值。
If the OSDMap currently used by the ceph-mon or ceph-osd daemon has non-legacy values, it will require the CRUSH_TUNABLES or CRUSH_TUNABLES2 feature bit from clients and daemons who connect to it. This means that old clients will not be able to connect.
如果ceph-mon或ceph-osd进程现在用的OSDMap有非遗留值，它将要求连接它的客户端和守护进程有 CRUSH_TUNABLES或CRUSH_TUNABLES2 功能位。
At some future point in time, newly created clusters will have improved default values for the tunables. This is a matter of waiting until the support has been present in the Linux kernel clients long enough to make this a painless transition for most users.
将来，新建集群的可调值其默认值会更好。这要等到此功能进入内核客户端的时间足够长，对大多数用户来说已是无痛的过渡。
 3.3.9.8.1  过时值的影响
IMPACT OF LEGACY VALUES
The legacy values result in several misbehaviors:
遗留值导致几个不当行为：
For hiearchies with a small number of devices in the leaf buckets, some PGs map to fewer than the desired number of replicas. This commonly happens for hiearchies with “host” nodes with a small number (1-3) of OSDs nested beneath each one.
如果分级结构的支部只有少量设备，一些PG的副本数小于期望值，这通常出现在一些子结构里，host节点下少数OSD嵌套到了其他OSD里。
For large clusters, some small percentages of PGs map to less than the desired number of OSDs. This is more prevalent when there are several layers of the hierarchy (e.g., row, rack, host, osd).
大型集群里，小部分PG映射到的OSD数目小于期望值，有多层结构（如：机架行、机架、主机、OSD）时这种情况更普遍。
When some OSDs are marked out, the data tends to get redistributed to nearby OSDs instead of across the entire hierarchy.
当一些OSD标记为out时，数据倾向于重分布到附近OSD而非整个分级结构。
 3.3.9.8.2  CRUSH_TUNABLES
choose_local_tries: Number of local retries. Legacy value is 2, optimal value is 0.
choose_local_tries：本地重试次数。以前是2，最优值是0。
choose_local_fallback_tries: Legacy value is 5, optimal value is 0.
choose_local_fallback_tries：以前5，现在是0。
choose_total_tries: Total number of attempts to choose an item. Legacy value was 19, subsequent testing indicates that a value of 50 is more appropriate for typical clusters. For extremely large clusters, a larger value might be necessary.
choose_total_tries：选择一个条目的最大尝试次数。以前19，后来的测试表明，对典型的集群来说50更合适。最相当大的集群来说，更大的值也许必要。
 3.3.9.8.3  CRUSH_TUNABLES2
chooseleaf_descend_once: Whether a recursive chooseleaf attempt will retry, or only try once and allow the original placement to retry. Legacy default is 0, optimal value is 1.
chooseleaf_descend_once：是否重递归叶子选择，或只试一次、并允许最初归置组重试。以前默认0，最优为1。
 3.3.9.8.4  支持CRUSH_TUNABLES的客户端版本
Which client versions support CRUSH_TUNABLES
argonaut series, v0.48.1 or later
v0.49 or later
Linux kernel version v3.6 or later (for the file system and RBD kernel clients)
 3.3.9.8.5  支持CRUSH_TUNABLES2的客户端版本
Which client versions support CRUSH_TUNABLES2
v0.55 or later, including bobtail series (v0.56.x)
Linux kernel version v3.9 or later (for the file system and RBD kernel clients)
 3.3.9.8.6  一些要点
A FEW IMPORTANT POINTS
Adjusting these values will result in the shift of some PGs between storage nodes. If the Ceph cluster is already storing a lot of data, be prepared for some fraction of the data to move.
调整这些值将使一些PG在存储节点间移位，如果ceph集群已经存储了大量数据，做好移动一部分数据的准备。
The ceph-osd and ceph-mon daemons will start requiring the feature bits of new connections as soon as they get the updated map. However, already-connected clients are effectively grandfathered in, and will misbehave if they do not support the new feature.
一旦更新运行图，ceph-osd和ceph-mon就会开始向新建连接要求功能位，然而，之前已经连接的客户端如果不支持新功能将行为失常。
If the CRUSH tunables are set to non-legacy values and then later changed back to the defult values, ceph-osd daemons will not be required to support the feature. However, the OSD peering process requires examining and understanding old maps. Therefore, you should not run old (pre-v0.48) versions of the ceph-osd daemon if the cluster has previosly used non-legacy CRUSH values, even if the latest version of the map has been switched back to using the legacy defaults.
如果CRUSH可调值更改过、然后又改回了默认值，ceph-osd守护进程将不要求支持此功能，然而，OSD连接建立进程要能检查和理解旧地图。因此，集群如果用过非默认CRUSH值就不应该再运行版本小于0.48.1的ceph-osd，即使最新版地图已经回滚到了遗留默认值。
 3.3.9.8.7  调整CRUSH
TUNING CRUSH
The simplest way to adjust the crush tunables is by changing to a known profile. Those are:
更改crush可调值的最简方法就是改到一个已知配置，它们有：
legacy: the legacy behavior from argonaut and earlier.
argonaut: the legacy values supported by the original argonaut release
bobtail: the values supported by the bobtail release
optimal: the current best values
default: the current default values for a new cluster
Currently, legacy, default, and argonaut are the same, and bobtail and optimal include CRUSH_TUNABLES and CRUSH_TUNABLES2.
当前，legacy、default、和argonaut相同；bobtail和optimal包含 CRUSH_TUNABLES和CRUSH_TUNABLES2。
You can select a profile on a running cluster with the command:
你可以在运行着的集群上选择一个配置：
ceph osd crush tunables {PROFILE}
Note that this may result in some data movement.
要注意，这可能产生一些数据迁移。
 3.3.9.8.8  调整CRUSH——强硬方法
Tuning CRUSH, the hard way
If you can ensure that all clients are running recent code, you can adjust the tunables by extracting the CRUSH map, modifying the values, and reinjecting it into the cluster.
如果你能保证所有客户端都运行最新代码，你可以这样调整可调值：从集群抽取CRUSH图、修改值、重注入。
Extract the latest CRUSH map:
抽取最新CRUSH图：
ceph osd getcrushmap -o /tmp/crush
Adjust tunables. These values appear to offer the best behavior for both large and small clusters we tested with. You will need to additionally specify the --enable-unsafe-tunables argument to crushtool for this to work. Please use this option with extreme care.:
调整可调参数。这些值在我们测试过的大、小型集群上都有最佳表现。在极端情况下，你需要给crushtool额外指定--enable-unsafe-tunables参数才行：
crushtool -i /tmp/crush --set-choose-local-tries 0 --set-choose-local-fallback-tries 0 --set-choose-total-tries 50 -o /tmp/crush.new
Reinject modified map:
重注入修改的地图：
ceph osd setcrushmap -i /tmp/crush.new
 3.3.9.8.9  遗留值
LEGACY VALUES
For reference, the legacy values for the CRUSH tunables can be set with:
CRUSH可调参数的遗留值可以用下面命令设置：
crushtool -i /tmp/crush --set-choose-local-tries 2 --set-choose-local-fallback-tries 5 --set-choose-total-tries 19 -o /tmp/crush.legacy
Again, the special --enable-unsafe-tunables option is required. Further, as noted above, be careful running old versions of the ceph-osd daemon after reverting to legacy values as the feature bit is not perfectly enforced.
再次申明，--enable-unsafe-tunables是必需的，而且前面也提到了，回退到遗留值后慎用旧版ceph-osd进程，因为此功能位不是完全强制的。
 3.3.10  增加/删除OSD
Adding/Removing OSDs
When you have a cluster up and running, you may add OSDs or remove OSDs from the cluster at runtime.
如果您的集群已经在运行，你可以在运行时添加或删除OSD。
 3.3.10.1  增加OSD
When you want to expand a cluster, you may add an OSD at runtime. With Ceph, an OSD is generally one Ceph ceph-osd daemon for one storage drive within a host machine. If your host has multiple storage drives, you may map one ceph-osd daemon for each drive.
你迟早要扩容集群，ceph允许在运行时增加OSD。在ceph里，一个OSD一般是一个ceph-osd守护进程，它运行在硬盘之上，如果你有多个硬盘，可以给每个硬盘启动一个ceph-osd守护进程。

Generally, it’s a good idea to check the capacity of your cluster to see if you are reaching the upper end of its capacity. As your cluster reaches its near full ratio, you should add one or more OSDs to expand your cluster’s capacity.
通常，你应该监控集群容量，看是否达到了容量上限，因为达到了它的near full ratio值后，要增加一或多个OSD来扩容。

Warning: Do not let your cluster reach its full ratio before adding an OSD. OSD failures that occur after the cluster reaches its near full ratio may cause the cluster to exceed its full ratio.
警告：不要等空间满了再增加OSD，空间使用率达到near full ratio值后，OSD失败可能导致集群空间占满。
 3.3.10.1.1  部署硬件
DEPLOY YOUR HARDWARE
If you are adding a new host when adding a new OSD, see Hardware Recommendations for details on minimum recommendations for OSD hardware. To add a OSD host to your cluster, first make sure you have an up-to-date version of Linux installed (typically Ubuntu 12.04 precise), and you have made some initial preparations for your storage drives. See Filesystem Recommendations for details.
如果你通过增加主机来增加OSD，关于OSD服务器硬件的配置请参见硬件推荐。要把一台OSD主机加入到集群，首先要安装最新版的Linux（如Ubuntu 12.04），而且存储硬盘要做好必要的准备，详情参见硬盘和文件系统。

Add your OSD host to a rack in your cluster, connect it to the network and ensure that it has network connectivity.
把OSD主机添加到集群机架上，连接好网络、确保网络通畅。
 3.3.10.1.2  安装必要软件
INSTALL THE REQUIRED SOFTWARE
For manually deployed clusters, you must install Ceph packages manually. See Installing Debian/Ubuntu Packages for details. You should configure SSH to a user with password-less authentication and root permissions.
在手动部署的集群里，你必须手动安装ceph软件包，详情参见安装ceph软件包。你应该配置一个无密码登录SSH的用户，且他有root权限。
For clusters deployed with Chef, create a chef user, configure SSH keys, install Ruby and install the Chef client on your host. See Installing Chef for details.
对于用chef部署的集群，要在主机上创建一个cheg用户、配置SSH密钥对、安装Ruby、安装chef客户端，详情参见错误：引用源未找到。
 3.3.10.1.3  增加OSD（手动）
ADDING AN OSD (MANUAL)
This procedure sets up an ceph-osd daemon, configures it to use one drive, and configures the cluster to distribute data to the OSD. If your host has multiple drives, you may add an OSD for each drive by repeating this procedure.
此过程要设置一个ceph-osd守护进程，让它使用一个硬盘，且让集群把数据发布到OSD。如果一台主机有多个硬盘，可以重复此过程，把每个硬盘配置为一个OSD。

To add an OSD, create a data directory for it, mount a drive to that directory, add the OSD to your configuration file, add the OSD to the cluster, and then add it to the CRUSH map.
要添加OSD，要依次创建数据目录、把硬盘挂载到目录、把OSD添加到配置文件、把OSD加入集群、把OSD加入CRUSH图。

When you add the OSD to the CRUSH map, consider the weight you give to the new OSD. Hard drive capacity grows 40% per year, so newer OSD hosts may have larger hard drive than older hosts in the cluster (i.e., they may have greater weight).
往CRUSH图里添加OSD时建议设置权重，硬盘容量每年增长40%，所以较新的OSD主机拥有更大的空间（如他们可以有更大的权重）。
1. Create the OSD. If no UUID is given, it will be set automatically when the OSD starts up. The following command will output the OSD number, which you will need for subsequent steps.
ceph osd create [{uuid}]
2. Create the default directory on your new OSD.
在新OSD主机上创建默认目录。
ssh {new-osd-host}
sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}
3. If the OSD is for a drive other than the OS drive, prepare it for use with Ceph, and mount it to the directory you just created:
如果给OSD用的是单独的而非系统盘，先把它挂载到刚创建的目录下：
ssh {new-osd-host}
sudo mkfs -t {fstype} /dev/{disk}
sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
4. Navigate to the host where you keep the master copy of the cluster’s ceph.conf file.
登录到保存ceph.conf主拷贝的主机上：
ssh {admin-host}
cd /etc/ceph
vim ceph.conf
5. Add the new OSD to your ceph.conf file.
把新OSD添加到ceph.conf文件里：
[osd.1]
    host = {hostname}
6. From the host where you keep the master copy of the cluster’s ceph.conf file, copy the updated ceph.conf file to your new OSD’s /etc/ceph directory and to other hosts in your cluster.
从保存集群ceph.conf主拷贝的主机上，把更新过的ceph.conf拷贝到新OSD主机和其他主机的/etc/ceph/目录下。
ssh {new-osd} sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
7. Initialize the OSD data directory.
初始化OSD数据目录。
ssh {new-osd-host}
ceph-osd -i {osd-num} --mkfs --mkkey
The directory must be empty before you can run ceph-osd.
运行ceph-osd时目录必须是空的。
8. Register the OSD authentication key. The value of ceph for ceph-{osd-num} in the path is the $cluster-$id. If your cluster name differs from ceph, use your cluster name instead.:
注册OSD认证密钥，ceph-{osd-num}路径里的ceph值应该是$cluster-$id，如果你的集群名字不是ceph，那就用改过的名字：
ceph auth add osd.{osd-num} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring
9. Add the OSD to the CRUSH map so that it can begin receiving data. You may also decompile the CRUSH map, add the OSD to the device list, add the host as a bucket (if it’s not already in the CRUSH map), add the device as an item in the host, assign it a weight, recompile it and set it. See Add/Move an OSD for details.
把OSD加入CRUSH图以便它接收数据，也可以反编译CRUSH图、把OSD加入device list、以桶的形式加入主机（如果它没在CRUSH图里）、以条目形式把设备加入主机、分配权重、重编译并应用它。详情参见增加/移动OSD。
For Argonaut (v 0.48), execute the following:
ceph osd crush set {id} {name} {weight} pool={pool-name}  [{bucket-type}={bucket-name} ...]
For Bobtail (v 0.56), execute the following:
ceph osd crush set {id-or-name} {weight} root={pool-name}  [{bucket-type}={bucket-name} ...]
0.48版最佳实践
Argonaut (v0.48) Best Practices
To limit impact on user I/O performance, add an OSD to the CRUSH map with an initial weight of 0. Then, ramp up the CRUSH weight a little bit at a time. For example, to ramp by increments of 0.2, start with:
为降低对用户I/O性能的影响，加入CRUSH图时应该把OSD的初始权重设为0，然后每次增大一点、逐步增大CRUSH权重。例如每次增加0.2：

and allow migration to complete before reweighting to 0.4, 0.6, and so on until the desired CRUSH weight is reached.
迁移完成前，可以依次把权重重置为0.4、0.6等等，直到达到期望权重。
To limit the impact of OSD failures, you can set:
为降低OSD失败的影响，你可以设置：

which prevents down OSDs from automatically being marked out, and then ramp them down manually with:
它防止挂了的OSD自动被标记为out，然后逐步降低其权重：

Again, wait for the cluster to finish migrating data, and then adjust the weight further until you reach a weight of 0. Note that this problem prevents the cluster to automatically re-replicate data after a failure, so please ensure that sufficient monitoring is in place for an administrator to intervene promptly.
还是等着集群完成数据迁移，然后再次调整权重，直到权重为0。注意，这会阻止集群在发生故障时自动重复制数据，所以要确保监控的及时性，以便管理员迅速介入。
Note that this practice will no longer be necessary in Bobtail and subsequent releases.
注意：以上经验在Bobtail及后续版本已不再必要。
 3.3.10.1.4  增加OSD（chef）
ADDING AN OSD (CHEF)
This procedure configures your OSD using chef-client. If your host has multiple drives, you may need to execute the procedure for preparing an OSD drive for each data drive on your host.
这个步骤用chef-client配置OSD，如果主机有多个硬盘，每个硬盘都要重复此步骤。
When you add the OSD to the CRUSH map, consider the weight you give to the new OSD. Hard drive capacity grows 40% per year, so newer OSD hosts may have larger hard drive than older hosts in the cluster.
往CRUSH图里添加OSD时建议设置权重，硬盘容量每年增长40%，所以较新的OSD主机拥有更大的空间（如他们可以有更大的权重）。

1. Execute chef-client to register it with Chef as a Chef node.
执行chef-client来注册为chef节点。
2. Edit the node. See Configure Nodes for details. Change its environment to your Chef environment. Add "role[ceph-osd]" to the run list.
编辑节点，详情参见错误：引用源未找到。进入chef环境，把"role[ceph-osd]"添加到运行列表。
3. Execute Prepare OSD Drives for each drive.
在每个硬盘上执行 Prepare OSD Disks。
4. Execute chef-client to invoke the run list.
执行 chef-client调用运行列表。
5. Add the OSD to the CRUSH map so that it can begin receiving data. You may also decompile the CRUSH map edit the file, recompile it and set it. See Add/Move an OSD for details.
把OSD加入CRUSH图以便它接收数据，也可以反编译CRUSH图、编辑文件、重编译并应用它。详情参见增加/移动OSD。
ceph osd crush set {name} {weight} [{bucket-type}={bucket-name} ...]
 3.3.10.1.5  启动OSD
STARTING THE OSD
After you add an OSD to Ceph, the OSD is in your configuration. However, it is not yet running. The OSD is down and out. You must start your new OSD before it can begin receiving data. You may use service ceph from your admin host or start the OSD from its host machine:
把OSD加入ceph后，OSD就在配置里了。然而，它还没运行，其状态为down且out，开始收数据前必须先启动。可以用管理主机上的服务、或从OSD所在主机启动。
service ceph -a start osd.{osd.num}
#or alternatively
ssh {new-osd-host}
sudo /etc/init.d/ceph start osd.{osd-num}
Once you start your OSD, it is up.
只要启动了OSD，它就是up状态。
 3.3.10.1.6  把OSD推进集群
PUT THE OSD IN THE CLUSTER
After you start your OSD, it is up and out. You need to put it in to the cluster so that Ceph can begin writing data to it.
启动OSD后，它是up且out的，你得把它推入集群，ceph才能向其写入数据。
ceph osd in {osd-num}
 3.3.10.1.7  观察数据迁移
OBSERVE THE DATA MIGRATION
Once you have added your new OSD to the CRUSH map, Ceph will begin rebalancing the server by migrating placement groups to your new OSD. You can observe this process with the ceph tool.
把新OSD加入CRUSH图后，ceph会重新均衡服务器，一些归置组会迁移到新OSD里，你可以用ceph命令观察此过程。
ceph -w
You should see the placement group states change from active+clean to active, some degraded objects, and finally active+clean when migration completes. (Control-c to exit.)
你会看到归置组状态从active+clean变为active、一些降级的对象、且迁移完成后回到active+clean状态。（Ctrl-c退出）
 3.3.10.2  删除OSD（手动）
Removing OSDs (Manual)
When you want to reduce the size of a cluster or replace hardware, you may remove an OSD at runtime. With Ceph, an OSD is generally one Ceph ceph-osd daemon for one storage drive within a host machine. If your host has multiple storage drives, you may need to remove one ceph-osd daemon for each drive. Generally, it’s a good idea to check the capacity of your cluster to see if you are reaching the upper end of its capacity. Ensure that when you remove an OSD that your cluster is not at its near full ratio.
要想缩减集群尺寸或替换硬件，可在运行时删除OSD。在ceph里，一个OSD通常是一台主机上的一个ceph-osd守护进程、它运行在一个硬盘之上。如果一台主机上有多个数据盘，你得挨个删除其对应ceph-osd。通常，操作前应该检查集群容量，看是否快达到上限了，确保删除OSD后不会使集群达到near full ratio值。
Warning: Do not let your cluster reach its full ratio when removing an OSD. Removing OSDs could cause the cluster to reach or exceed its full ratio.
警告：删除OSD时不要让集群达到其最大利用率，删除OSD可能导致集群达到或超过最大利用率。
 3.3.10.2.1  把OSD踢出集群
TAKE THE OSD OUT OF THE CLUSTER
Before you remove an OSD, it is usually up and in. You need to take it out of the cluster so that Ceph can begin rebalancing and copying its data to other OSDs.
删除OSD前，它通常是up且in的，要先把它踢出集群，以使ceph启动重新均衡、把数据拷贝到其他OSD。
ceph osd out {osd-num}
 3.3.10.2.2  观察数据迁移
OBSERVE THE DATA MIGRATION
Once you have taken your OSD out of the cluster, Ceph will begin rebalancing the cluster by migrating placement groups out of the OSD you removed. You can observe this process with the ceph tool.
一旦把OSD踢出集群，ceph就会开始重新均衡集群、把归置组迁移出删除的OSD。你可以用ceph程序观察此过程。
ceph -w
You should see the placement group states change from active+clean to active, some degraded objects, and finally active+clean when migration completes. (Control-c to exit.)
你会看到归置组状态从active+clean变为active、有一些降级的对象、迁移完成后最终回到active+clean状态。（Ctrl-c中止）
 3.3.10.2.3  停止OSD
STOPPING THE OSD
After you take an OSD out of the cluster, it may still be running. That is, the OSD may be up and out. You must stop your OSD before you remove it from the configuration.
把OSD踢出集群后，它可能仍在运行，就是说其状态为up且out。删除前要先停止OSD进程。
ssh {new-osd-host}
sudo /etc/init.d/ceph stop osd.{osd-num}
Once you stop your OSD, it is down.
停止OSD后，状态变为down。
 3.3.10.2.4  删除OSD
This procedure removes an OSD from a cluster map, removes its authentication key, removes the OSD from the OSD map, and removes the OSD from the ceph.conf file. If your host has multiple drives, you may need to remove an OSD for each drive by repeating this procedure.
此步骤依次把一个OSD移出集群CRUSH图、删除认证密钥、删除OSD图条目、删除ceph.conf条目。如果主机有多个硬盘，每个硬盘对应的OSD都得重复此步骤。
1. Remove the OSD from the CRUSH map so that it no longer receives data. You may also decompile the CRUSH map, remove the OSD from the device list, remove the device as an item in the host bucket or remove the host bucket (if it’s in the CRUSH map and you intend to remove the host), recompile the map and set it. See Remove an OSD for details.
删除CRUSH图的对应OSD条目，它就不再接收数据了。你也可以反编译CRUSH图、删除device列表条目、删除对应的host桶条目或删除host桶（如果它在CRUSH图里，而且你想删除主机），重编译CRUSH图并应用它。详情参见删除OSD。
ceph osd crush remove {name}
2. Remove the OSD authentication key.
删除OSD认证密钥：
ceph auth del osd.{osd-num}
The value of ceph for ceph-{osd-num} in the path is the $cluster-$id. If your cluster name differs from ceph, use your cluster name instead.
ceph-{osd-num}路径里的ceph值是$cluster-$id，如果集群名字不是ceph，这里要更改。
3. Remove the OSD.
删除OSD。
ceph osd rm {osd-num}
#for example
ceph osd rm 1
4. Navigate to the host where you keep the master copy of the cluster’s ceph.conf file.
登录到保存ceph.conf主拷贝的主机：
ssh {admin-host}
cd /etc/chef
vim ceph.conf
5. Remove the OSD entry from your ceph.conf file.
从ceph.conf配置文件里删除对应条目。
[osd.1]
        host = {hostname}
6. From the host where you keep the master copy of the cluster’s ceph.conf file, copy the updated ceph.conf file to the /etc/ceph directory of other hosts in your cluster.
从保存ceph.conf主拷贝的主机，把更新过的ceph.conf拷贝到集群其他主机的/etc/ceph目录下。
ssh {osd} sudo tee /etc/ceph/ceph.conf < /etc/ceph/ceph.conf
 3.3.11  CPU剖析
CPU Profiling
If you built Ceph from source and compiled Ceph for use with oprofile you can profile Ceph’s CPU usage. See Installing Oprofile for details.
如果你从源码编译、且启用了oprofile，那么你就可以剖析ceph的CPU用法了，详情参见 Installing Oprofile。
 3.3.11.1  初始化oprofile
INITIALIZING OPROFILE
The first time you use oprofile you need to initialize it. Locate the vmlinux image corresponding to the kernel you are now running.
你首次使用oprofile的时候要初始化，找到对应于当前运行内核的vmlinux位置：
ls /boot
sudo opcontrol --init
sudo opcontrol --setup --vmlinux={path-to-image} --separate=library --callgraph=6
 3.3.11.2  启动oprofile
STARTING OPROFILE
To start oprofile execute the following command:
执行下面的命令启动opeofile：
opcontrol --start

Once you start oprofile, you may run some tests with Ceph.
启动oprofile后，你可以运行一些ceph测试：
 3.3.11.3  停止oprofile
STOPPING OPROFILE
To stop oprofile execute the following command:
执行下面的命令停止oprofile：
opcontrol --stop
 3.3.11.4  查看oprofile运行结果
RETRIEVING OPROFILE RESULTS
To retrieve the top cmon results, execute the following command:
要查看cmon最近的结果，执行下面的命令：
opreport -gal ./cmon | less

To retrieve the top cmon results with call graphs attached, execute the following command:
要检索cmon最近的调用图结果，执行下面的命令：
opreport -cal ./cmon | less

Important: After reviewing results, you should reset oprofile before running it again. Resetting oprofile removes data from the session directory.
重要：回顾结果后，重新剖析前应该先重置，重置动作从会话目录里删除了数据。
 3.3.11.5  重置oprofile
RESETTING OPROFILE
To reset oprofile, execute the following command:
要重置oprofile，执行下面的命令：
sudo opcontrol --reset

Important: You should reset oprofile after analyzing data so that you do not commingle results from different tests.
重要：你应该分析后再重置，以免混合不同的剖析结果。
 3.3.12  内存剖析
Memory Profiling
Ceph OSD and metadata server daemons can generate heap profiles using tcmalloc. To generate heap profiles, ensure you have google-perftools installed:
sudo apt-get google-perftools
The profiler dumps output to your log file directory (i.e., /var/log/ceph). See Logging and Debugging Config Reference for details. To view the profiler logs with Google’s performance tools, execute the following:
google-pprof -gv {log-path/filename}
Refer to Google Heap Profiler for additional details.
Once you have the heap profiler installed, start your cluster and begin using the heap profiler. You may enable or disable the heap profiler at runtime, or ensure that it runs continously. For the following commandline usage, replace {daemon-type} with osd or mds, and replace daemon-id with the OSD number or metadata server letter.
 3.3.12.1  启动剖析器
Starting the Profiler
To start the heap profiler, execute the following:
ceph {daemon-type} tell {daemon-id} heap start_profiler
For example:
ceph osd tell 1 heap start_profiler
 3.3.12.2  打印统计
Printing Stats
To print out statistics, execute the following:
ceph {daemon-type} tell {daemon-id} heap stats
For example:
ceph osd tell 0 heap stats

Note: Printing stats does not require the profiler to be running and does not dump the heap allocation information to a file.
 3.3.12.3  转储堆栈信息
Dumping Heap Information
To dump heap information, execute the following:
ceph {daemon-type} tell {daemon-id} heap dump
For example:
ceph mds tell a heap dump

Note: Dumping heap information only works when the profiler is running.

 3.3.12.4  释放内存
Releasing Memory
To release memory that tcmalloc has allocated but which is not being used by the Ceph daemon itself, execute the following:
ceph {daemon-type} tell {daemon-id} heap release
For example:
ceph osd tell 2 heap release
 3.3.12.5  停止剖析器
Stopping the Profiler
To stop the heap profiler, execute the following:
ceph {daemon-type} tell {daemon-id} heap stop_profiler
For example:
ceph {daemon-type} tell {daemon-id} heap stop_profiler
 3.3.13  故障排除
Troubleshooting
When monitoring your cluster, you may receive health warnings and you may also notice that not all of your daemons are running properly. The following sections will help you identify and resolve daemon operations issues.

 3.3.13.1  OSD和PG排障
Troubleshooting OSDs and PGs
Before troubleshooting your OSDs, check your monitors and network first. If you execute ceph health or ceph -s on the command line and Ceph returns a health status, the return of a status means that the monitors have a quorum. If you don’t have a monitor quorum or if there are errors with the monitor status, address the monitor issues first. Check your networks to ensure they are running properly, because networks may have a significant impact on OSD operation and performance.
 3.3.13.1.1  ceph社区
The Ceph Community
The Ceph community is an excellent source of information and help. For operational issues with Ceph releases we recommend you subscribe to the ceph-users email list. When you no longer want to receive emails, you can unsubscribe from the ceph-users email list.
If you have read through this guide and you have contacted ceph-users, but you haven’t resolved your issue, you may contact Inktank for support.
You may also subscribe to the ceph-devel email list. You should do so if your issue is:
Likely related to a bug
Related to a development release package
Related to a development testing package
Related to your own builds
If you no longer want to receive emails from the ceph-devel email list, you may unsubscribe from the ceph-devel email list.
Tip: The Ceph community is growing rapidly, and community members can help you if you provide them with detailed information about your problem. See Obtaining Data About OSDs before you post questions to ensure that community members have sufficient data to help you.
 3.3.13.1.2  收集OSD数据
Obtaining Data About OSDs
A good first step in troubleshooting your OSDs is to obtain information in addition to the information you collected while monitoring your OSDs (e.g., ceph osd tree).
 3.3.13.1.2.1  ceph日志
Ceph Logs
If you haven’t changed the default path, you can find Ceph log files at /var/log/ceph:
ls /var/log/ceph
If you don’t get enough log detail, you can change your logging level. See Ceph Logging and Debugging and Logging and Debugging Config Reference in the Ceph Configuration documentation for details. Also, see Debugging and Logging in the Ceph Operations documentation to ensure that Ceph performs adequately under high logging volume.
 3.3.13.1.2.2  管理套接字
Admin Socket
Use the admin socket tool to retrieve runtime information. For details, list the sockets for your Ceph processes:
ls /var/run/ceph
Then, execute the following, replacing {socket-name} with an actual socket name to show the list of available options:
ceph --admin-daemon /var/run/ceph/{socket-name} help
The admin socket, among other things, allows you to:
List your configuration at runtime
Dump historic operations
Dump the operation priority queue state
Dump operations in flight
Dump perfcounters
 3.3.13.1.2.3  显示剩余空间
Display Freespace
Filesystem issues may arise. To display your filesystem’s free space, execute df.
df -h
Execute df --help for additional usage.
 3.3.13.1.2.4  I/O统计信息
I/O Statistics
Use iostat to identify I/O-related issues.
iostat -x
 3.3.13.1.2.5  诊断消息
Diagnostic Messages
To retrieve diagnostic messages, use dmesg with less, more, grep or tail. For example:
dmesg | grep scsi
 3.3.13.1.3  停止自动重均衡
Stopping w/out Rebalancing
Periodically, you may need to perform maintenance on a subset of your cluster, or resolve a problem that affects a failure domain (e.g., a rack). If you do not want CRUSH to automatically rebalance the cluster as you stop OSDs for maintenance, set the cluster to noout first:
ceph osd set noout
Once the cluster is set to noout, you can begin stopping the OSDs within the failure domain that requires maintenance work.
ceph osd stop osd.{num}

Note: Placement groups within the OSDs you stop will become degraded while you are addressing issues with within the failure domain.

Once you have completed your maintenance, restart the OSDs.
ceph osd start osd.{num}
Finally, you must unset the cluster from noout.
ceph osd unset noout
 3.3.13.1.4  OSD没运行
OSD Not Running
Under normal circumstances, simply restarting the ceph-osd daemon will allow it to rejoin the cluster and recover.
 3.3.13.1.4.1  OSD起不来
An OSD Won’t Start
If you start your cluster and an OSD won’t start, check the following:
Configuration File: If you were not able to get OSDs running from a new installation, check your configuration file to ensure it conforms (e.g., host not hostname, etc.).
Check Paths: Check the paths in your configuration, and the actual paths themselves for data and journals. If you separate the OSD data from the journal data and there are errors in your configuration file or in the actual mounts, you may have trouble starting OSDs. If you want to store the journal on a block device, you should partition your journal disk and assign one partition per OSD.
Kernel Version: Identify the kernel version and distribution you are using. Ceph uses some third party tools by default, which may be buggy or may conflict with certain distributions and/or kernel versions (e.g., Google perftools). Check the OS recommendations to ensure you have addressed any issues related to your kernel.
Segment Fault: If there is a segment fault, turn your logging up (if it isn’t already), and try again. If it segment faults again, contact the ceph-devel email list and provide your Ceph configuration file, your monitor output and the contents of your log file(s).
If you cannot resolve the issue and the email list isn’t helpful, you may contact Inktank for support.
 3.3.13.1.4.2  OSD失败
An OSD Failed
When a ceph-osd process dies, the monitor will learn about the failure from surviving ceph-osd daemons and report it via the ceph health command:
ceph-osd挂的时候，监视器将了解ceph-osd的存活情况，且通过ceph health命令报告：
ceph health
HEALTH_WARN 1/3 in osds are down
Specifically, you will get a warning whenever there are ceph-osd processes that are marked in and down. You can identify which ceph-osds are down with:
而且，有ceph-osd进程标记为in且down的时候，你会得到警告，你可以用下面的命令得知哪个ceph-osd进程挂了：
ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
If there is a disk failure or other fault preventing ceph-osd from functioning or restarting, an error message should be present in its log file in /var/log/ceph.
如果有个硬盘失败或其它错误使ceph-osd不能正常运行或重启，一条错误信息将会出现在日志文件/var/log/ceph/里。
If the daemon stopped because of a heartbeat failure, the underlying kernel file system may be unresponsive. Check dmesg output for disk or other kernel errors.
如果守护进程因心跳失败、或者底层文件系统无响应而停止，查看dmesg获取硬盘或者内核错误。
If the problem is a software error (failed assertion or other unexpected error), it should be reported to the ceph-devel email list.
如果是软件错误（失败的插入或其它意外错误），就应该回馈到邮件列表。
 3.3.13.1.4.3  硬盘没有剩余空间
No Free Drive Space
Ceph prevents you from writing to a full OSD so that you don’t lose data. In an operational cluster, you should receive a warning when your cluster is getting near its full ratio. The mon osd full ratio defaults to 0.95, or 95% of capacity before it stops clients from writing data. The mon osd nearfull ratio defaults to 0.85, or 85% of capacity when it generates a health warning.
Full cluster issues usually arise when testing how Ceph handles an OSD failure on a small cluster. When one node has a high percentage of the cluster’s data, the cluster can easily eclipse its nearfull and full ratio immediately. If you are testing how Ceph reacts to OSD failures on a small cluster, you should leave ample free disk space and consider temporarily lowering the mon osd full ratio and mon osd nearfull ratio.
Full ceph-osds will be reported by ceph health:
ceph health
HEALTH_WARN 1 nearfull osds
osd.2 is near full at 85%
Or:
ceph health
HEALTH_ERR 1 nearfull osds, 1 full osds
osd.2 is near full at 85%
osd.3 is full at 97%
The best way to deal with a full cluster is to add new ceph-osds, allowing the cluster to redistribute data to the newly available storage.
处理这种情况的最好方法就是增加新的ceph-osd，这允许集群把数据重分布到新OSD里。
If you cannot start an OSD because it is full, you may delete some data by deleting some placement group directories in the full OSD.
Important: If you choose to delete a placement group directory on a full OSD, DO NOT delete the same placement group directory on another full OSD, or YOU MAY LOSE DATA. You MUST maintain at least one copy of your data on at least one OSD.

 3.3.13.1.5  OSD龟速或没反应
OSDs are Slow/Unresponsive
A commonly recurring issue involves slow or unresponsive OSDs. Ensure that you have eliminated other troubleshooting possibilities before delving into OSD performance issues. For example, ensure that your network(s) is working properly and your OSDs are running. Check to see if OSDs are throttling recovery traffic.
Tip: Newer versions of Ceph provide better recovery handling by preventing recovering OSDs from using up system resources so that up and in OSDs aren’t available or are otherwise slow.
提示：
 3.3.13.1.5.1  网络问题
Networking Issues
Ceph is a distributed storage system, so it depends upon networks to peer with OSDs, replicate objects, recover from faults and check heartbeats. Networking issues can cause OSD latency and flapping OSDs. See Flapping OSDs for details.
Ensure that Ceph processes and Ceph-dependent processes are connected and/or listening.
netstat -a | grep ceph
netstat -l | grep ceph
sudo netstat -p | grep ceph
Check network statistics.
netstat -s
 3.3.13.1.5.2  驱动器配置
Drive Configuration
A storage drive should only support one OSD. Sequential read and sequential write throughput can bottleneck if other processes share the drive, including journals, operating systems, monitors, other OSDs and non-Ceph processes.
Ceph acknowledges writes after journaling, so fast SSDs are an attractive option to accelerate the response time–particularly when using the ext4 or XFS filesystems. By contrast, the btrfs filesystem can write and journal simultaneously.
Note: Partitioning a drive does not change its total throughput or sequential read/write limits. Running a journal in a separate partition may help, but you should prefer a separate physical drive.
 3.3.13.1.5.3  坏扇区/碎片化硬盘
Bad Sectors / Fragmented Disk
Check your disks for bad sectors and fragmentation. This can cause total throughput to drop substantially.
 3.3.13.1.5.4  监视器和OSD蜗居
Co-resident Monitors/OSDs
Monitors are generally light-weight processes, but they do lots of fsync(), which can interfere with other workloads, particularly if monitors run on the same drive as your OSDs. Additionally, if you run monitors on the same host as the OSDs, you may incur performance issues related to:
Running an older kernel (pre-3.0)
Running Argonaut with an old glibc
Running a kernel with no syncfs(2) syscall.
In these cases, multiple OSDs running on the same host can drag each other down by doing lots of commits. That often leads to the bursty writes.
 3.3.13.1.5.5  进程蜗居
Co-resident Processes
Spinning up co-resident processes such as a cloud-based solution, virtual machines and other applications that write data to Ceph while operating on the same hardware as OSDs can introduce significant OSD latency. Generally, we recommend optimizing a host for use with Ceph and using other hosts for other processes. The practice of separating Ceph operations from other applications may help improve performance and may streamline troubleshooting and maintenance.
 3.3.13.1.5.6  日志级别
Logging Levels
If you turned logging levels up to track an issue and then forgot to turn logging levels back down, the OSD may be putting a lot of logs onto the disk. If you intend to keep logging levels high, you may consider mounting a drive to the default path for logging (i.e., /var/log/ceph/$cluster-$name.log).
 3.3.13.1.5.7  恢复节流
Recovery Throttling
Depending upon your configuration, Ceph may reduce recovery rates to maintain performance or it may increase recovery rates to the point that recovery impacts OSD performance. Check to see if the OSD is recovering.
 3.3.13.1.5.8  内核版本
Kernel Version
Check the kernel version you are running. Older kernels may not receive new backports that Ceph depends upon for better performance.
 3.3.13.1.5.9  内核与syncfs问题
Kernel Issues with SyncFS
Try running one OSD per host to see if performance improves. Old kernels might not have a recent enough version of glibc to support syncfs(2).
试着
 3.3.13.1.5.10  文件系统问题
Filesystem Issues
Currently, we recommend deploying clusters with XFS or ext4. The btrfs filesystem has many attractive features, but bugs in the filesystem may lead to performance issues.
 3.3.13.1.5.11  内存不足
Insufficient RAM
We recommend 1GB of RAM per OSD daemon. You may notice that during normal operations, the OSD only uses a fraction of that amount (e.g., 100-200MB). Unused RAM makes it tempting to use the excess RAM for co-resident applications, VMs and so forth. However, when OSDs go into recovery mode, their memory utilization spikes. If there is no RAM available, the OSD performance will slow considerably.
 3.3.13.1.5.12  old requests和 slow requests
Old Requests or Slow Requests
If a ceph-osd daemon is slow to respond to a request, it will generate log messages complaining about requests that are taking too long. The warning threshold defaults to 30 seconds, and is configurable via the osd op complaint time option. When this happens, the cluster log will receive messages.
Legacy versions of Ceph complain about ‘old requests`:
osd.0 192.168.106.220:6800/18813 312 : [WRN] old request osd_op(client.5099.0:790 fatty_26485_object789 [write 0~4096] 2.5e54f643) v4 received at 2012-03-06 15:42:56.054801 currently waiting for sub ops
New versions of Ceph complain about ‘slow requests`:
{date} {osd.num} [WRN] 1 slow requests, 1 included below; oldest blocked for > 30.005692 secs
{date} {osd.num}  [WRN] slow request 30.005692 seconds old, received at {date-time}: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 [write 0~4194304] 0.69848840) v4 currently waiting for subops from [610]
Possible causes include:
A bad drive (check dmesg output)
A bug in the kernel file system bug (check dmesg output)
An overloaded cluster (check system load, iostat, etc.)
A bug in the ceph-osd daemon.
Possible solutions
Remove VMs Cloud Solutions from Ceph Hosts
Upgrade Kernel
Upgrade Ceph
Restart OSDs
 3.3.13.1.6  打摆子的OSD
Flapping OSDs
We recommend using both a public (front-end) network and a cluster (back-end) network so that you can better meet the capacity requirements of object replication. Another advantage is that you can run a cluster network such that it isn’t connected to the internet, thereby preventing some denial of service attacks. When OSDs peer and check heartbeats, they use the cluster (back-end) network when it’s available. See Monitor/OSD Interaction for details.
However, if the cluster (back-end) network fails or develops significant latency while the public (front-end) network operates optimally, OSDs currently do not handle this situation well. What happens is that OSDs mark each other down on the monitor, while marking themselves up. We call this scenario ‘flapping`.
If something is causing OSDs to ‘flap’ (repeatedly getting marked down and then up again), you can force the monitors to stop the flapping with:
如果有东西导致OSD摆动（反复地被标记为down，然后又up），你可以强制监视器停止：
ceph osd set noup      # prevent osds from getting marked up
ceph osd set nodown    # prevent osds from getting marked down
These flags are recorded in the osdmap structure:
这些标记记录在osdmap数据结构里：
ceph osd dump | grep flags
flags no-up,no-down
You can clear the flags with:
下列命令可清除标记：
ceph osd unset noup
ceph osd unset nodown
Two other flags are supported, noin and noout, which prevent booting OSDs from being marked in (allocated data) or down ceph-osds from eventually being marked out (regardless of what the current value for mon osd down out interval is).
还支持其它两个标记noin和noout，它们分别可阻止OSD被标记为in、死亡的ceph-osd被标记为out（不管 mon osd down out interval的值是什么）。
Note: noup, noout, and nodown are temporary in the sense that once the flags are cleared, the action they were blocking should occur shortly after. The noin flag, on the other hand, prevents OSDs from being marked in on boot, and any daemons that started while the flag was set will remain that way.
注意，noup、noout和nodown从某种意义上说是临时的，一旦标记清除了，它们被阻塞的动作短时间内就会发生；相反，noin标记阻止ceph-osd启动时进入集群，任何设置了此标记的守护进程启动后都维持原样。
 3.3.13.1.7  PG错误排障
Troubleshooting PG Errors
 3.3.13.1.7.1  归置组总不整洁
Placement Groups Never Get Clean
There are a few cases where Ceph placement groups never get clean:
1. One OSD: If you deviate from the quick start and use only one OSD, you will likely run into problems. OSDs report other OSDs to the monitor, and also interact with other OSDs when replicating data. If you have only one OSD, a second OSD cannot check its heartbeat. Also, if you remove an OSD and have only one OSD remaining, you may encounter problems. An secondary or tertiary OSD expects another OSD to tell it which placement groups it should have. The lack of another OSD prevents this from occurring. So a placement group can remain stuck “stale” forever.
2. Pool Size = 1: If you have only one copy of an object, no other OSD will tell the OSD which objects it should have. For each placement group mapped to the remaining OSD (see ceph pg dump), you can force the OSD to notice the placement groups it needs by running:
ceph pg force_create_pg <pgid>
As a general rule, you should run your cluster with more than one OSD and a pool size greater than 1 object replica.
 3.3.13.1.7.2  卡住的归置组
Stuck Placement Groups
It is normal for placement groups to enter states like “degraded” or “peering” following a failure. Normally these states indicate the normal progression through the failure recovery process. However, if a placement group stays in one of these states for a long time this may be an indication of a larger problem. For this reason, the monitor will warn when placement groups get “stuck” in a non-optimal state. Specifically, we check for:
有失败时归置组进入“degraded”（降级）或“peering”（连接建立中）状态，这事时有发生，通常这些状态意味着正常的失败恢复正在进行。然而，如果一个归置组长时间处于某个这些状态就意味着有更大的问题，因此监视器在归置组卡(stuck)在非最优状态时会警告，具体地，我们检查：
inactive - The placement group has not been active for too long (i.e., it hasn’t been able to service read/write requests).
inactive（不活跃）——归置组长时间无活跃（例如它不能提供读写服务了）；
unclean - The placement group has not been clean for too long (i.e., it hasn’t been able to completely recover from a previous failure).
unclean（不干净）——归置组长时间不干净（例如它未能从前面的失败完全恢复）；
stale - The placement group status has not been updated by a ceph-osd, indicating that all nodes storing this placement group may be down.
stale（不新鲜）——归置组状态没有被ceph-osd更新，表明存储这个归置组的所有节点可能都挂了。
You can explicitly list stuck placement groups with one of:
你可以明确列出卡住的归置组：
ceph pg dump_stuck stale
ceph pg dump_stuck inactive
ceph pg dump_stuck unclean
For stuck stale placement groups, it is normally a matter of getting the right ceph-osd daemons running again. For stuck inactive placement groups, it is usually a peering problem (see Placement Group Down - Peering Failure). For stuck unclean placement groups, there is usually something preventing recovery from completing, like unfound objects (see Unfound Objects);
处于stuck stale状态的归置组通过修复ceph-osd进程通常可以修复；处于stuck inactive状态的归置组通常是连接建立问题（参见错误：引用源未找到）；处于stuck unclean状态的归置组通常是由于某些东西阻止了恢复的完成，像未找到的对象（参见错误：引用源未找到）。
 3.3.13.1.7.3  归置组挂了——连接建立失败
Placement Group Down - Peering Failure
In certain cases, the ceph-osd Peering process can run into problems, preventing a PG from becoming active and usable. For example, ceph health might report:
在某些情况下，ceph-osd连接建立进程会遇到问题，使PG不能活跃、可用，例如ceph health也许显示：
ceph health detail
HEALTH_ERR 7 pgs degraded; 12 pgs down; 12 pgs peering; 1 pgs recovering; 6 pgs stuck unclean; 114/3300 degraded (3.455%); 1/3 in osds are down
...
pg 0.5 is down+peering
pg 1.4 is down+peering
...
osd.1 is down since epoch 69, last address 192.168.106.220:6801/8651
We can query the cluster to determine exactly why the PG is marked down with:
可以查询到PG为何被标记为down：
ceph pg 0.5 query

{ "state": "down+peering",
  ...
  "recovery_state": [
       { "name": "Started\/Primary\/Peering\/GetInfo",
         "enter_time": "2012-03-06 14:40:16.169679",
         "requested_info_from": []},
       { "name": "Started\/Primary\/Peering",
         "enter_time": "2012-03-06 14:40:16.169659",
         "probing_osds": [
               0,
               1],
         "blocked": "peering is blocked due to down osds",
         "down_osds_we_would_probe": [
               1],
         "peering_blocked_by": [
               { "osd": 1,
                 "current_lost_at": 0,
                 "comment": "starting or marking this osd lost may let us proceed"}]},
       { "name": "Started",
         "enter_time": "2012-03-06 14:40:16.169513"}
   ]
}
The recovery_state section tells us that peering is blocked due to down ceph-osd daemons, specifically osd.1. In this case, we can start that ceph-osd and things will recover.
recovery_state段告诉我们连接建立因ceph-osd进程挂了而被阻塞，本例是osd.1挂了，启动这个进程应该就可以恢复。
Alternatively, if there is a catastrophic failure of osd.1 (e.g., disk failure), we can tell the cluster that it is lost and to cope as best it can.
另外，如果osd.1是灾难性的失败（如硬盘损坏），我们可以告诉集群它丢失了，让集群尽力完成副本拷贝。
Important: This is dangerous in that the cluster cannot guarantee that the other copies of the data are consistent and up to date.
重要：集群不能保证其它数据副本是一致且最新就危险了！
To instruct Ceph to continue anyway:
无论如何让ceph继续：
ceph osd lost 1
Recovery will proceed.
恢复将继续。
 3.3.13.1.7.4  未找到的对象
Unfound Objects
Under certain combinations of failures Ceph may complain about unfound objects:
某几种失败相组合可能导致ceph抱怨有对象丢失：
ceph health detail
HEALTH_WARN 1 pgs degraded; 78/3778 unfound (2.065%)
pg 2.4 is active+degraded, 78 unfound
This means that the storage cluster knows that some objects (or newer copies of existing objects) exist, but it hasn’t found copies of them. One example of how this might come about for a PG whose data is on ceph-osds 1 and 2:
这意味着存储集群知道一些对象（或者存在对象的较新副本）存在，却没有找到它们的副本。下例展示了这种情况是如何发生的，一个PG的数据存储在ceph-osd 1和2上：
1 goes down
2 handles some writes, alone
1 comes up
1 and 2 repeer, and the objects missing on 1 are queued for recovery.
Before the new objects are copied, 2 goes down.

1挂了；
2独自处理一些写动作；
1起来了；
1和2重新建立连接，1上面丢失的对象加入队列准备恢复；
新对象还未拷贝完，2挂了。
Now 1 knows that these object exist, but there is no live ceph-osd who has a copy. In this case, IO to those objects will block, and the cluster will hope that the failed node comes back soon; this is assumed to be preferable to returning an IO error to the user.
这时，1知道这些对象存在，但是活着的ceph-osd都没有副本，这种情况下，读写这些对象的IO就会被阻塞，集群只能指望节点早点恢复。这时我们假设用户希望先得到一个IO错误。
First, you can identify which objects are unfound with:
首先，你应该确认哪些对象找不到了：
ceph pg 2.4 list_missing [starting offset, in json]

{ "offset": { "oid": "",
     "key": "",
     "snapid": 0,
     "hash": 0,
     "max": 0},
 "num_missing": 0,
 "num_unfound": 0,
 "objects": [
    { "oid": "object 1",
      "key": "",
      "hash": 0,
      "max": 0 },
    ...
 ],
 "more": 0}
If there are too many objects to list in a single result, the more field will be true and you can query for more. (Eventually the command line tool will hide this from you, but not yet.)
如果在一次查询里列出的对象太多，more这个域将为true，因此你可以查询more。（命令行工具可能隐藏了，但这里没有）
Second, you can identify which OSDs have been probed or might contain data:
其次，你可以找出哪些OSD上探测到、或可能包含数据：
ceph pg 2.4 query

"recovery_state": [
     { "name": "Started\/Primary\/Active",
       "enter_time": "2012-03-06 15:15:46.713212",
       "might_have_unfound": [
             { "osd": 1,
               "status": "osd is down"}]},
In this case, for example, the cluster knows that osd.1 might have data, but it is down. The full range of possible states include:
本例中，集群知道osd.1可能有数据，但它挂了。所有可能的状态有：
* already probed	已经探测到了
* querying	 	在查询
* osd is down	 	OSD挂了
* not queried (yet)	 尚未查询
Sometimes it simply takes some time for the cluster to query possible locations.
有时候集群要花一些时间来查询可能的位置。
It is possible that there are other locations where the object can exist that are not listed. For example, if a ceph-osd is stopped and taken out of the cluster, the cluster fully recovers, and due to some future set of failures ends up with an unfound object, it won’t consider the long-departed ceph-osd as a potential location to consider. (This scenario, however, is unlikely.)
还有一种可能性，对象存在于其它位置却未被列出，例如，集群里的一个ceph-osd停止且被剔出，然后完全恢复了；后来的失败、恢复后仍有未找到的对象，它也不会觉得早已死亡的ceph-osd上仍可能包含这些对象。（这种情况几乎不太可能发生）。
If all possible locations have been queried and objects are still lost, you may have to give up on the lost objects. This, again, is possible given unusual combinations of failures that allow the cluster to learn about writes that were performed before the writes themselves are recovered. To mark the “unfound” objects as “lost”:
如果所有位置都查询过了仍有对象丢失，那就得放弃丢失的对象了。这仍可能是罕见的失败组合导致的，集群在写入完成前，未能得知写入是否已执行。以下命令把未找到的(unfound)对象标记为丢失(lost)。
ceph pg 2.5 mark_unfound_lost revert
This the final argument specifies how the cluster should deal with lost objects. Currently the only supported option is “revert”, which will either roll back to a previous version of the object or (if it was a new object) forget about it entirely. Use this with caution, as it may confuse applications that expected the object to exist.
上述最后一个参数告诉集群应如何处理丢失的对象。当前只支持revert选项，它使得回滚到对象的前一个版本（如果它是新对象）或完全忽略它。要谨慎使用，它可能迷惑那些期望对象存在的应用程序。
 3.3.13.1.7.5  无根归置组
Homeless Placement Groups
It is possible for all OSDs that had copies of a given placement groups to fail. If that’s the case, that subset of the object store is unavailable, and the monitor will receive no status updates for those placement groups. To detect this situation, the monitor marks any placement group whose primary OSD has failed as stale. For example:
拥有归置组拷贝的OSD都可以失败，在这种情况下，那一部分的对象存储不可用，监视器就不会收到那些归置组的状态更新了。为检测这种情况，监视器把任何主OSD失败的归置组标记为stale（不新鲜），例如：
ceph health
HEALTH_WARN 24 pgs stale; 3/300 in osds are down
You can identify which placement groups are stale, and what the last OSDs to store them were, with:
你能找出哪些归置组不新鲜、和存储这些归置组的最新OSD，命令如下：
ceph health detail
HEALTH_WARN 24 pgs stale; 3/300 in osds are down
...
pg 2.5 is stuck stale+active+remapped, last acting [2,0]
...
osd.10 is down since epoch 23, last address 192.168.106.220:6800/11080
osd.11 is down since epoch 13, last address 192.168.106.220:6803/11539
osd.12 is down since epoch 24, last address 192.168.106.220:6806/11861
If we want to get placement group 2.5 back online, for example, this tells us that it was last managed by osd.0 and osd.2. Restarting those ceph-osd daemons will allow the cluster to recover that placement group (and, presumably, many others).
如果想使归置组2.5重新在线，例如，上面的输出告诉我们它最后由osd.0和osd.2处理，重启这些ceph-osd将恢复之（还有其它的很多PG）。
 3.3.13.2  监视器故障恢复
Recovering from Monitor Failures
In production clusters, we recommend running the cluster with a minimum of three monitors. The failure of a single monitor should not take down the entire monitor cluster, provided a majority of the monitors remain available. If the majority of nodes are available, the remaining nodes will be able to form a quorum.
在生产集群，我们推荐至少要运行3个监视器。这样单个监视器失败不会拖垮整个监视器集群，因为大部分仍可用，这样剩余节点仍能形成法定人数。
When you check your cluster’s health, you may notice that a monitor has failed. For example:
检查集群健康状况的时候，也许会看到一个监视器失败了，例如：
ceph health
HEALTH_WARN 1 mons down, quorum 0,2
For additional detail, you may check the cluster status:
额外详情可检查集群状态：
ceph status
HEALTH_WARN 1 mons down, quorum 0,2
mon.b (rank 1) addr 192.168.106.220:6790/0 is down (out of quorum)
In most cases, you can simply restart the affected node. For example:
大多情况下，都可以简单地重启相应节点，例如：
service ceph -a restart {failed-mon}
If there are not enough monitors to form a quorum, the ceph command will block trying to reach the cluster. In this situation, you need to get enough ceph-mon daemons running to form a quorum before doing anything else with the cluster.
如果监视器数量不足以形成法定人数，ceph命令将拦截你的操作尝试，你得先启动足够的ceph-mon守护进程来形成法定人数，才能在集群里做其它的事。
 3.3.13.2.1  客户端不能连接/挂载
Client Can’t Connect/Mount
Check your IP tables. Some OS install utilities add a REJECT rule to iptables. The rule rejects all clients trying to connect to the host except for ssh. If your monitor host’s IP tables have such a REJECT rule in place, clients connecting from a separate node will fail to mount with a timeout error. You need to address iptables rules that reject clients trying to connect to Ceph daemons. For example, you would need to address rules that look like this appropriately:
REJECT all -- anywhere anywhere reject-with icmp-host-prohibited
You may also need to add rules to IP tables on your Ceph hosts to ensure that clients can access the ports associated with your Ceph monitors (i.e., port 6789 by default) and Ceph OSDs (i.e., 6800 et. seq. by default). For example:
iptables -A INPUT -m multiport -p tcp -s {ip-address}/{netmask} --dports 6789,6800:6810 -j ACCEPT
 3.3.14  调试和日志记录
Debugging and logging
You may view Ceph log files under /var/log/ceph (the default location).
你可以在默认位置/var/log/ceph下翻阅ceph的日志。
Important: If you enable or increase the rate of Ceph logging, ensure that you have sufficient disk space on your OS disk. Verbose logging can generate over 1GB of data per hour. If your OS disk reaches its capacity, the node will stop working.

Accelerating Log Rotation
If your OS disk is relatively full, you can accelerate log rotation by modifying the Ceph log rotation file at /etc/logrotate.d/ceph. Add a size setting after the rotation frequency to accelerate log rotation (via cronjob) if your logs exceed the size setting. For example, the default setting looks like this:

Modify it by adding a size setting.

Then, start the crontab editor for your user space.

Finally, add an entry to check the etc/logrotate.d/ceph file.

The preceding example checks the etc/logrotate.d/ceph file every 30 minutes.

Ceph is still on the leading edge, so you may encounter situations that require using Ceph’s debugging and logging. To activate and configure Ceph’s debug logging, refer to Ceph Logging and Debugging. For additional logging settings, refer to the Logging and Debugging Config Reference.
ceph仍在技术前沿，所以你也许会碰到一些状况，需要使用ceph的调试和日志功能。激活和配置ceph的调试日志，参见日志、调试；额外的日志设置参见错误：引用源未找到。

You can change the logging settings at runtime so that you don’t have to stop and restart the cluster. Refer to Ceph Configuration - Runtime Changes for additional details.
可以在运行时更改日志选项，这样你就不必停止、重启集群了，详情参见运行时更改。

Debugging may also require you to track down memory and threading issues. You can run a single daemon, a type of daemon, or the whole cluster with Valgrind. You should only use Valgrind when developing or debugging Ceph. Valgrind is computationally expensive, and will slow down your system otherwise. Valgrind messages are logged to stderr.
调试也许要求你捕捉内存和线程问题，你可以在Valgrind下运行单个、一类守护进程、或者整个集群。你应该只在开发、调试期间使用Valgrind，它需要大量计算，会减慢你的系统。Valgrind消息记录在标准错误。
 3.3.15  增加/删除监视器
Adding/removing monitors
When you have a cluster up and running, you may add or remove monitors from the cluster at runtime.
你的集群启动并运行后，可以在运行时增加、或删除监视器。
 3.3.15.1  增加监视器
ADDING MONITORS
Ceph monitors are light-weight processes that maintain a master copy of the cluster map. You can run a cluster with 1 monitor. We recommend at least 3 monitors for a production cluster. Ceph monitors use PAXOS to establish consensus about the master cluster map, which requires a majority of monitors running to establish a quorum for consensus about the cluster map (e.g., 1; 3 out of 5; 4 out of 6; etc.).
ceph监视器是轻量级进程，它维护着集群运行图的副本。一个集群可以只有一个监视器，我们推荐生产环境至少3个监视器。ceph使用PAXOS算法对主集群运行图达成共识，它要求大部分监视器在运行，以达到关于集群运行图建立共识所需的法定人数（如1、5个中的3个、6个中的4个等等）。
Since monitors are light-weight, it is possible to run them on the same host as an OSD; however, we recommend running them on separate hosts.
正因为监视器是轻量级的，所以有可能在作为OSD的主机上同时运行它；然而，我们推荐运行于单独主机。

Important A majority of monitors in your cluster must be able to reach each other in order to establish a quorum.
重要：要确立法定人数，集群内的大部分监视器必须互相可达。
 3.3.15.1.1  部署硬件
DEPLOY YOUR HARDWARE
If you are adding a new host when adding a new monitor, see Hardware Recommendations for details on minimum recommendations for monitor hardware. To add a monitor host to your cluster, first make sure you have an up-to-date version of Linux installed (typically Ubuntu 12.04 precise).
如果你增加新监视器时要新增一台主机，关于其最低硬件配置请参见硬件推荐。要增加一个监视器主机，首先要安装最新版的Linux（如Ubuntu 12.04）。
Add your monitor host to a rack in your cluster, connect it to the network and ensure that it has network connectivity.
把监视器主机安装上架，连通网络。
 3.3.15.1.2  安装必要软件
INSTALL THE REQUIRED SOFTWARE
For manually deployed clusters, you must install Ceph packages manually. See Installing Debian/Ubuntu Packages for details. You should configure SSH to a user with password-less authentication and root permissions.
手动部署的集群，ceph软件包必须手动装，详情参见安装ceph软件包。应该配置一个用户，使之可以无密码登录SSH、且有root权限。

For clusters deployed with Chef, create a chef user, configure SSH keys, install Ruby and install the Chef client on your host. See Installing Chef for details.
chef部署的集群，创建一个chef用户、配置SSH密钥、安装Ruby并且在主机上安装chef客户端。详情参见错误：引用源未找到。
 3.3.15.1.3  增加监视器（手动）
ADDING A MONITOR (MANUAL)
This procedure creates a ceph-mon data directory, retrieves the monitor map and monitor keyring, and adds a ceph-mon daemon to your cluster. If this results in only two monitor daemons, you may add more monitors by repeating this procedure until you have a sufficient number of ceph-mon daemons to achieve a quorum.
本步骤创建ceph-mon数据目录、获取监视器运行图和监视器密钥环、增加一个ceph-mon守护进程。如果这导致只有2个监视器守护进程，你可以重演此步骤来增加一或多个监视器，直到你拥有足够多ceph-mon建立法定人数。
At this point you should define your monitor’s id. Traditionally, monitors have been named with single letters (a, b, c, ...), but you are free to define the id as you see fit. For the purpose of this document, please take into account that {mon-id} should be the id you chose, without the mon. prefix (i.e., {mon-id} should be the a on mon.a).

1. Create the default directory on your new monitor.
在新监视器上创建默认目录：
ssh {new-mon-host}
sudo mkdir /var/lib/ceph/mon/ceph-{mon-letter}
2. Create a temporary directory {tmp} to keep the files needed during this process. This directory should be different from monitor’s default directory created in the previous step, and can be removed after all the steps are taken.
创建临时目录，用以保存此过程中用到的文件。此目录应不同于前面步骤创建的监视器数据目录，且完成后可删除。
mkdir {tmp}
3. Retrieve the keyring for your monitors, where {tmp} is the path to the retrieved keyring, and {filename} is the name of the file containing the retrieved monitor key.
获取监视器密钥环，{tmp}是密钥环文件保存路径、{filename}是包含密钥的文件名。
ceph auth get mon. -o {tmp}/{filename}
4. Retrieve the monitor map, where {tmp} is the path to the retrieved monitor map, and {filename} is the name of the file containing the retrieved monitor monitor map.
获取监视器运行图，{tmp}是获取到的监视器运行图、{filename}是包含监视器运行图的文件名。
ceph mon getmap -o {tmp}/{filename}
5. Prepare the monitor’s data directory created in the first step. You must specify the path to the monitor map so that you can retrieve the information about a quorum of monitors and their fsid. You must also specify a path to the monitor keyring:
准备第一步创建的监视器数据目录。必须指定监视器运行图路径，这样才能获得监视器法定人数和它们fsid的信息；还要指定监视器密钥环路径。
sudo ceph-mon -i {mon-letter} --mkfs --monmap {tmp}/{filename} --keyring {tmp}/{filename}
6. Add a [mon.{letter}] entry for your new monitor in your ceph.conf file.
把新监视器的[mon.{letter}]条目添加到ceph.conf文件里。
[mon.c]
        host = new-mon-host
        addr = ip-addr:6789
7. Add the new monitor to the list of monitors for you cluster (runtime). This enables other nodes to use this monitor during their initial startup.
把新监视器添加到集群的监视器列表里（运行时），这允许其它节点开始启动时使用这个节点。
ceph mon add <name> <ip>[:<port>]
8. Start the new monitor and it will automatically join the cluster. The daemon needs to know which address to bind to, either via --public-addr {ip:port} or by setting mon addr in the appropriate section of ceph.conf. For example:
启动新监视器，它会自动加入机器。守护进程需知道绑定到哪个地址，通过--public-addr {ip:port}或在ceph.conf里的相应段设置mon addr可以指定。
ceph-mon -i newname --public-addr {ip:port}
 3.3.15.2  删除监视器
REMOVING MONITORS
When you remove monitors from a cluster, consider that Ceph monitors use PAXOS to establish consensus about the master cluster map. You must have a sufficient number of monitors to establish a quorum for consensus about the cluster map.
从集群删除监视器时，必须认识到，ceph监视器用PASOX算法关于主集群运行图达成共识。必须有足够多的监视器才能对集群运行图达成共识。
 3.3.15.2.1  删除监视器（手动）
REMOVING A MONITOR (MANUAL)
This procedure removes a ceph-mon daemon from your cluster. If this procedure results in only two monitor daemons, you may add or remove another monitor until you have a number of ceph-mon daemons that can achieve a quorum.
本步骤从集群删除ceph-mon守护进程，如果此步骤导致只剩2个监视器了，你得另外增加一或多个监视器，直到达成法定人数所必需的ceph-mon数量。
1. Stop the monitor.
停止监视器。
service ceph -a stop mon.{mon-id}
2. Remove the monitor from the cluster.
从集群删除监视器。
ceph mon remove {mon-id}
3. Remove the monitor entry from ceph.conf.
删除ceph.conf对应条目。
 3.3.15.2.2  从不健康集群删除监视器
REMOVING MONITORS FROM AN UNHEALTHY CLUSTER
This procedure removes a ceph-mon daemon from an unhealhty cluster–i.e., a cluster that has placement groups that are persistently not active + clean.
本步骤从不健康集群删除ceph-mon，即集群有些归置组永久偏离active+clean。
1. Identify a surviving monitor.
找出活着的监视器。
ceph mon dump
2. Navigate to a surviving monitor’s monmap directory.
进入幸存监视器的monmap目录。
ssh {mon-host}
cd /var/lib/ceph/mon/ceph-{mon-letter}/monmap
3. List the directory contents and identify the last commmitted map. Directory contents will show a numeric list of maps.
列出目录内容，找出最后提交的运行图。目录内容是运行图的数字列表。
ls
1  2  3  4  5  first_committed  last_committed  last_pn  latest
4. Identify the most recently committed map.
找出最近提交的运行图。
sudo cat last_committed
5. Copy the most recently committed file to a temporary directory.
把最近提交的文件拷到一个临时目录。
cp /var/lib/ceph/mon/ceph-{mon-letter}/monmap/{last_committed} /tmp/surviving_map
6. Remove the non-surviving monitors. For example, if you have three monitors, mon.a, mon.b, and mon.c, where only mon.a will survive, follow the example below:
删除死亡监视器。例如，假设你有3个监视器，mon.a、mon.b和mon.c，其中只有mon.a要保留，如下：
monmaptool /tmp/surviving_map --rm {mon-letter}
#for example
monmaptool /tmp/surviving_map --rm b
monmaptool /tmp/surviving_map --rm c
7. Stop all monitors.
停止所有监视器。
service ceph -a stop mon
8. Inject the surviving map with the removed monitors into the surviving monitors. For example, to inject a map into monitor mon.a, follow the example below:
把找出的幸存运行图注入保留的监视器，例如，要把运行图注入mon.a，模仿如下：
ceph-mon -i {mon-letter} --inject-monmap {map-path}
#for example
ceph-mon -i a --inject-monmap /etc/surviving_map
 3.3.15.3  更改监视器IP地址
Changing a Monitor’s IP Address
Important Existing monitors are not supposed to change their IP addresses.
重要：现有监视器不应该更改其IP地址。
Monitors are critical components of a Ceph cluster, and they need to maintain a quorum for the whole system to work properly. To establish a quorum, the monitors need to discover each other. Ceph has strict requirements for discovering monitors.
监视器是ceph集群的关键组件，它们要维护一个法定人数，这样整个系统才能正常工作。要确立法定人数，监视器得互相发现对方，ceph对监视器的发现要求严格。
Ceph clients and other Ceph daemons use ceph.conf to discover monitors. However, monitors discover each other using the monitor map, not ceph.conf. For example, if you refer to Adding a Monitor (Manual) you will see that you need to obtain the current monmap for the cluster when creating a new monitor, as it is one of the required arguments of ceph-mon -i {mon-id} --mkfs. The following sections explain the consistency requirements for Ceph monitors, and a few safe ways to change a monitor’s IP address.
ceph客户端及其它ceph守护进程用ceph.conf发现监视器，然而，监视器之间用监视器运行图发现对方，而非ceph.conf。例如，如果你参照了增加监视器，会发现创建新监视器时得获取当前集群的monmap，因为它是ceph-mon -i {mon-id} --mkfs命令的必要参数。下面几段解释了ceph监视器的一致性要求，和几种改IP的安全方法。
 3.3.15.3.1  一致性要求
Consistency Requirements
A monitor always refers to the local copy of the monmap when discovering other monitors in the cluster. Using the monmap instead of ceph.conf avoids errors that could break the cluster (e.g., typos in ceph.conf when specifying a monitor address or port). Since monitors use monmaps for discovery and they share monmaps with clients and other Ceph daemons, the monmap provides monitors with a strict guarantee that their consensus is valid.
监视器发现集群内的其它监视器时总是参照monmap的本地副本，用monmap而非ceph.conf可避免因配置错误（例如在ceph.conf里指定监视器地址或端口时拼写错误）而损坏集群。正因为监视器用monmaps相互发现、且共享于客户端和其它ceph守护进程间，所以monmap以苛刻的一致性提供监视器。
Strict consistency also applies to updates to the monmap. As with any other updates on the monitor, changes to the monmap always run through a distributed consensus algorithm called Paxos. The monitors must agree on each update to the monmap, such as adding or removing a monitor, to ensure that each monitor in the quorum has the same version of the monmap. Updates to the monmap are incremental so that monitors have the latest agreed upon version, and a set of previous versions, allowing a monitor that has an older version of the monmap to catch up with the current state of the cluster.
苛刻的一致性要求也适用于monmap的更新，因为任何有关监视器的更新、monmap的更改都通过名为Pasox的分布式一致性算法运行。为保证法定人数里的所有监视器都持有同版本monmap，所有监视器都要赞成monmap的每一次更新，像增加、删除监视器。monmap的更新是增量的，这样监视器都有最近商定的版本以及一系列之前版本，这样可使一个有较老monmap的监视器赶上集群当前的状态。
If monitors discovered each other through the Ceph configuration file instead of through the monmap, it would introduce additional risks because the Ceph configuration files aren’t updated and distributed automatically. Monitors might inadvertantly use an older ceph.conf file, fail to recognize a monitor, fall out of a quorum, or develop a situation where Paxos isn’t able to determine the current state of the system accurately. Consequently, making changes to an existing monitor’s IP address must be done with great care.
如果监视器通过ceph配置文件而非monmap相互发现，就会引进额外风险，因为ceph配置文件不会自动更新和发布。监视器有可能用了较老的ceph.conf而导致不能识别某监视器、掉出法定人数、或者发展为一种Paxos不能精确确定当前系统状态的情形。总之，更改现有监视器的IP地址必须慎之又慎。
 3.3.15.3.2  更改监视器IP地址（正确方法）
Changing a Monitor’s IP address (The Right Way)
Changing a monitor’s IP address in ceph.conf only is not sufficient to ensure that other monitors in the cluster will receive the update. To change a monitor’s IP address, you must add a new monitor with the IP address you want to use (as described in Adding a Monitor (Manual)), ensure that the new monitor successfully joins the quorum; then, remove the monitor that uses the old IP address. Then, update the ceph.conf file to ensure that clients and other daemons know the IP address of the new monitor.
仅仅在ceph.conf里更改监视器的IP不足以让集群内的其它监视器接受更新。要更改一个监视器的IP地址，你必须以先以想用的IP地址增加一个监视器（见增加监视器），确保新监视器成功加入法定人数，然后删除用旧IP的监视器，最后更新ceph.conf以确保客户端和其它守护进程得知新监视器的IP地址。
For example, lets assume there are three monitors in place, such as
例如，我们假设有3个监视器，如下：
[mon.a]
        host = host01
        addr = 10.0.0.1:6789
[mon.b]
        host = host02
        addr = 10.0.0.2:6789
[mon.c]
        host = host03
        addr = 10.0.0.3:6789
To change mon.c to host04 with the IP address 10.0.0.4, follow the steps in Adding a Monitor (Manual) by adding a new monitor mon.d. Ensure that mon.d is running before removing mon.c, or it will break the quorum. Remove mon.c as described on Removing a Monitor (Manual). Moving all three monitors would thus require repeating this process as many times as needed.
要把host04上mon.c的IP改为10.0.0.4，按照增加监视器（手动）里的步骤增加一个新监视器mon.d，确认它运行正常后再删除mon.c，否则会破坏法定人数；最后依照删除监视器（手动）删除mon.c。3个监视器都要更改的话，每次都要重复一次。
 3.3.15.3.3  更改监视器IP地址（凌乱方法）
Changing a Monitor’s IP address (The Messy Way)
There may come a time when the monitors must be moved to a different network, a different part of the datacenter or a different datacenter altogether. While it is possible to do it, the process becomes a bit more hazardous.
可能有时候监视器不得不挪到不同的网络、数据中心的不同位置、甚至不同的数据中心，这是可能的，但过程有点惊险。
In such a case, the solution is to generate a new monmap with updated IP addresses for all the monitors in the cluster, and inject the new map on each individual monitor. This is not the most user-friendly approach, but we do not expect this to be something that needs to be done every other week. As it is clearly stated on the top of this section, monitors are not supposed to change IP addresses.
在这种情形下，一种方法是用所有监视器的新IP地址生成新monmap，并注入到集群内的所有监视器。对大多数用户来说，这并不简单，好在它不常见。再次重申，监视器不应该更改IP地址。
Using the previous monitor configuration as an example, assume you want to move all the monitors from the 10.0.0.x range to 10.1.0.x, and these networks are unable to communicate. Use the following procedure:
以前面的监视器配置为例，假设你想把所有监视器的IP从10.0.0.x改为10.1.0.x，并且两个网络互不相通，步骤如下：
1. Retrieve the monitor map, where {tmp} is the path to the retrieved monitor map, and {filename} is the name of the file containing the retrieved monitor monitor map.
获取监视器运行图，其中{tmp}是所获取的运行图路径，{filename}是监视器运行图的文件名。
ceph mon getmap -o {tmp}/{filename}
2. The following example demonstrates the contents of the monmap.
下面是一个monmap内容示例：
$ monmaptool --print {tmp}/{filename}

monmaptool: monmap file {tmp}/{filename}
epoch 1
fsid 224e376d-c5fe-4504-96bb-ea6332a19e61
last_changed 2012-12-17 02:46:41.591248
created 2012-12-17 02:46:41.591248
0: 10.0.0.1:6789/0 mon.a
1: 10.0.0.2:6789/0 mon.b
2: 10.0.0.3:6789/0 mon.c
3. Remove the existing monitors.
删除现有监视器。
$ monmaptool --rm a --rm b --rm c {tmp}/{filename}

monmaptool: monmap file {tmp}/{filename}
monmaptool: removing a
monmaptool: removing b
monmaptool: removing c
monmaptool: writing epoch 1 to {tmp}/{filename} (0 monitors)
4. Add the new monitor locations.
添加新监视器位置。
$ monmaptool --add a 10.1.0.1:6789 --add b 10.1.0.2:6789 --add c 10.1.0.3:6789 {tmp}/{filename}

monmaptool: monmap file {tmp}/{filename}
monmaptool: writing epoch 1 to {tmp}/{filename} (3 monitors)
5. Check new contents.
检查新内容。
$ monmaptool --print {tmp}/{filename}

monmaptool: monmap file {tmp}/{filename}
epoch 1
fsid 224e376d-c5fe-4504-96bb-ea6332a19e61
last_changed 2012-12-17 02:46:41.591248
created 2012-12-17 02:46:41.591248
0: 10.1.0.1:6789/0 mon.a
1: 10.1.0.2:6789/0 mon.b
2: 10.1.0.3:6789/0 mon.c
At this point, we assume the monitors (and stores) are installed at the new location. The next step is to propagate the modified monmap to the new monitors, and inject the modified monmap into each new monitor.
从这里开始，假设监视器（及存储）已经被安装到了新位置。下一步把修正的monmap散播到新监视器，并且注入每个监视器。

1. First, make sure to stop all your monitors. Injection must be done while the daemon is not running.
首先，停止所有监视器，注入必须在守护进程停止时进行。
2. Inject the monmap.
注入monmap。
ceph-mon -i {mon-id} --inject-monmap {tmp}/{filename}
3. Restart the monitors.
重启监视器。
After this step, migration to the new location is complete and the monitors should operate successfully.
到这里，到新位置的迁移完成，监视器应该照常运行了。
 3.3.16  命令参考
Control commands
 3.3.16.1  监视器命令
MONITOR COMMANDS
Monitor commands are issued using the ceph utility:
监视器命令用ceph工具发出：
ceph [-m monhost] {command}
The command is usually (though not always) of the form:
命令格式通常是（不总是）：
ceph {subsystem} {command}
 3.3.16.2  系统命令
SYSTEM COMMANDS
Execute the following to display the current status of the cluster.
下列命令显示集群状态：
ceph -s
ceph status
Execute the following to display a running summary of the status of the cluster, and major events.
下列命令显示集群状态的运行摘要、及主要事件：
ceph -w
Execute the following to show the monitor quorum, including which monitors are participating and which one is the leader.
下列命令显示监视器法定人数状态，包括哪些监视器参与着、哪个是首领。
ceph quorum_status
Execute the following to query the status of a single monitor, including whether or not it is in the quorum.
下列命令查询单个监视器状态，包括是否在法定人数里。
ceph [-m monhost] mon_status
 3.3.16.3  认证子系统
AUTHENTICATION SUBSYSTEM
To add a keyring for an OSD, execute the following:
要添加一个OSD的密钥环，执行下列命令：
ceph auth add {osd} {--in-file|-i} {path-to-osd-keyring}
To list the cluster’s keys and their capabilities, execute the following:
要列出集群的密钥及其能力，执行下列命令：
ceph auth list
 3.3.16.4  归置组子系统
PLACEMENT GROUP SUBSYSTEM
To display the statistics for all placement groups, execute the following:
要显示所有归置组的统计信息，执行下列命令：
ceph -- pg dump [--format {format}]
The valid formats are plain (default) and json.
可用输出格式有纯文本（默认）和json。
To display the statistics for all placement groups stuck in a specified state, execute the following:
要显示卡在某状态的所有归置组，执行下列命令：
ceph -- pg dump_stuck inactive|unclean|stale [--format {format}] [-t|--threshold {seconds}]
--format may be plain (default) or json
--threshold defines how many seconds “stuck” is (default: 300)
--format可以是plain（默认）或json
--threshold定义了多久算“卡”（默认300秒）

Inactive Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back.
Unclean Placement groups contain objects that are not replicated the desired number of times. They should be recovering.
Stale Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by mon_osd_report_timeout).
inactive归置组不能处理读或写，因为它们在等待数据及时更新的OSD回来。
unclean归置组包含副本数未达期望值的对象，它们应该在恢复中。
stale归置组处于未知状态——归置组所托付的OSD有一阵没向监视器报告了（由mon osd report timeout配置）。

Revert “lost” objects to their prior state, either a previous version or delete them if they were just created.
把“丢失”对象恢复到其先前状态，可以是前一版本、或如果刚创建就干脆删除。
ceph pg {pgid} mark_unfound_lost revert
 3.3.16.5  OSD子系统
OSD SUBSYSTEM
Query osd subsystem status.
查询OSD子系统状态：
ceph osd stat
Write a copy of the most recent osd map to a file. See osdmaptool.
把最新的OSD运行图拷贝到一个文件，参见osdmaptool：
ceph osd getmap -o file
Write a copy of the crush map from the most recent osd map to file.
从最新OSD运行图拷出CRUSH图：
ceph osd getcrushmap -o file
The foregoing functionally equivalent to
前述功能等价于：
ceph osd getmap -o /tmp/osdmap
osdmaptool /tmp/osdmap --export-crush file
Dump the OSD map. Valid formats for -f are plain and json. If no --format option is given, the OSD map is dumped as plain text.
转储OSD运行图，-f的可用格式有plain和json，如未指定--format则转储为纯文本。
ceph osd dump [--format {format}]
Dump the OSD map as a tree with one line per OSD containing weight and state.
把OSD运行图转储为树，每个OSD一行、包含权重和状态。
ceph osd tree [--format {format}]
Find out where a specific object is or would be stored in the system:
找出某对象在哪里或应该在哪里：
ceph osd map <pool-name> <object-name>
Add or move a new item (OSD) with the given id/name/weight at the specified location.
增加或挪动一个新OSD条目，要给出id/name/weight、和位置参数。
ceph osd crush set {id} {weight} [{loc1} [{loc2} ...]]
Remove an existing item from the CRUSH map.
从现有CRUSH图删除存在的条目：
ceph osd crush remove {id}

Move an existing bucket from one position in the hierarchy to another.
把有效的桶从分级结构里的一个位置挪到另一个：
ceph osd crush move {id} {loc1} [{loc2} ...]

Set the weight of the item given by {name} to {weight}.
设置{name}所指条目的权重为{weight}：
ceph osd crush reweight {name} {weight}
Create a cluster snapshot.
创建集群快照：
ceph osd cluster_snap {name}
Mark an OSD as lost. This may result in permanent data loss. Use with caution.
把OSD标记为丢失，有可能导致永久性数据丢失，慎用！
ceph osd lost [--yes-i-really-mean-it]
Create a new OSD. If no UUID is given, it will be set automatically when the OSD starts up.
创建新OSD。如果未指定ID，有可能的话将自动分配个新ID：
ceph osd create [{uuid}]
Remove the given OSD(s).
删除指定OSD：
ceph osd rm [{id}...]
Query the current max_osd parameter in the osd map.
查询OSD运行图里的max_osd参数。
ceph osd getmaxosd
Import the given OSD map. Note that this can be a bit dangerous, since the OSD map includes dynamic state about which OSDs are current on or offline; only do this if you’ve just modified a (very) recent copy of the map.
导入指定OSD运行图。注意，此动作有风险，因为OSD运行图包含当前OSD在线、离线的动态状态，只可用于相当新的副本。
ceph osd setmap -i file
Import the given crush map.
导入指定CRUSH图。
ceph osd setcrushmap -i file
Set the max_osd parameter in the OSD map. This is necessary when expanding the storage cluster.
设置OSD运行图的max_osd参数，扩展存储集群时有必要。
ceph osd setmaxosd
Mark OSD {osd-num} down.
把ID为{osd-num}的OSD标记为down。
ceph osd down {osd-num}
Mark OSD {osd-num} out of the distribution (i.e. allocated no data).
把OSD {osd-num}标记为数据分布之外（如不给分配数据）。
ceph osd out {osd-num}
Mark {osd-num} in the distribution (i.e. allocated data).
把OSD {osd-num}标记为数据分布之内（如分配了数据）。
ceph osd in {osd-num}
List classes that are loaded in the ceph cluster.
列出ceph集群载入的类：
ceph class list
Set or clear the pause flags in the OSD map. If set, no IO requests will be sent to any OSD. Clearing the flags via unpause results in resending pending requests.
设置或清空OSD运行图里的暂停标记。若设置了，不会有IO请求发送到任何OSD；用unpause清空此标记会导致重发未决的请求。
ceph osd pause
ceph osd unpause
Set the weight of {osd-num} to {weight}. Two OSDs with the same weight will receive roughly the same number of I/O requests and store approximately the same amount of data.
把{osd-num}的权重设置为{weight}，权重相同的两个OSD大致会收到相同的I/O请求、并存储相同数量的数据。
ceph osd reweight {osd-num} {weight}
Reweights all the OSDs by reducing the weight of OSDs which are heavily overused. By default it will adjust the weights downward on OSDs which have 120% of the average utilization, but if you include threshold it will use that percentage instead.
重设所有滥用OSD的权重，它默认向下调整达到120%利用率的OSD，除非你指定了threshold值。
ceph osd reweight-by-utilization [threshold]
Adds/removes the address to/from the blacklist. When adding an address, you can specify how long it should be blacklisted in seconds; otherwise, it will default to 1 hour. A blacklisted address is prevented from connecting to any OSD. Blacklisting is most often used to prevent a lagging metadata server from making bad changes to data on the OSDs.
增加、删除黑名单里的地址。增加地址的时候可以指定有效期，否则有效期为1小时。黑名单里的地址不允许连接任何OSD，此技术常用于防止滞后的元数据服务器“错爱”OSD上的数据。
These commands are mostly only useful for failure testing, as blacklists are normally maintained automatically and shouldn’t need manual intervention.
这些命令大多只在故障测试时有用，因为黑名单是自动维护的，无需手动干涉。
ceph osd blacklist add ADDRESS[:source_port] [TIME]
ceph osd blacklist rm ADDRESS[:source_port]
Creates/deletes a snapshot of a pool.
创建/删除存储池快照。
ceph osd pool mksnap {pool-name} {snap-name}
ceph osd pool rmsnap {pool-name} {snap-name}
Creates/deletes/renames a storage pool.
创建/删除/重命名存储池。
ceph osd pool create {pool-name} pg_num [pgp_num]
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
ceph osd pool rename {old-name} {new-name}
Changes a pool setting.
更改存储池设置。
ceph osd pool set {pool-name} {field} {value}

Valid fields are:
size: Sets the number of copies of data in the pool.
crash_replay_interval: The number of seconds to allow clients to replay acknowledged but uncommited requests.
pg_num: The placement group number.
pgp_num: Effective number when calculating pg placement.
crush_ruleset: rule number for mapping placement.
可用的field有：
size ——设置存储池内数据的副本数；
crash_replay_interval ——允许客户端重放确认而未提交的请求前等待的时间，秒；
pg_num ——归置组数量；
pgp_num ——计算归置组存放的有效数量；
crush_ruleset ——用于归置映射的规则号。
Get the value of a pool setting.
获取存储池配置值。
ceph osd pool get {pool-name} {field}
Valid fields are:
pg_num: The placement group number.
pgp_num: Effective number of placement groups when calculating placement.
lpg_num: The number of local placement groups.
lpgp_num: The number used for placing the local placement groups.
可用的field有：
pg_num ——归置组数量；
pgp_num ——计算归置组存放的有效数量；
lpg_num ——本地归置组数量；
lpgp_num ——用于存放本地归置组的数量。

Sends a scrub command to OSD {osd-num}. To send the command to all OSDs, use *.
向OSD {osd-num}下达一个洗刷命令，用通配符*把命令下达到所有OSD。
ceph osd scrub {osd-num}
Sends a repair command to osdN. To send the command to all osds, use *.
向osdN下达修复命令，用*下达到所有OSD。
ceph osd repair N
Runs a simple throughput benchmark against osdN, writing TOTAL_BYTES in write requests of BYTES_PER_WRITE each. By default, the test writes 1 GB in total in 4-MB increments.
在osdN上进行个简单的吞吐量测试，每次写入BYTES_PER_WRITE、一共写入TOTAL_BYTES。默认以4MB增量写入1GB。
ceph osd tell N bench [BYTES_PER_WRITE] [TOTAL_BYTES]
 3.3.16.6  MDS子系统
MDS SUBSYSTEM
Change configuration parameters on a running mds.
更改在运行mds的参数：
ceph mds tell {mds-id} injectargs '--{switch} {value} [--{switch} {value}]'
Example:
例如：
ceph mds tell 0 injectargs '--debug_ms 1 --debug_mds 10'
Enables debug messages.
打开了调试消息。

ceph mds stat
Displays the status of all metadata servers.
显示所有元数据服务器状态。
ceph mds fail 0
Marks the active MDS as failed, triggering failover to a stadnby if present.

Todo ceph mds subcommands missing docs: set_max_mds, dump, getmap, stop, setmap
待完成：ceph mds子命令缺少文档：set_max_mds、dump、getmap、stop、setmap。
 3.3.16.7  监视器子系统
MON SUBSYSTEM
Show monitor stats:
查看监视器状态：
ceph mon stat
2011-12-14 10:40:59.044395 mon {- [mon,stat]
2011-12-14 10:40:59.057111 mon.1 -} 'e3: 5 mons at {a=10.1.2.3:6789/0,b=10.1.2.4:6789/0,c=10.1.2.5:6789/0,d=10.1.2.6:6789/0,e=10.1.2.7:6789/0}, election epoch 16, quorum 0,1,2,3' (0)

The quorum list at the end lists monitor nodes that are part of the current quorum.
末尾的法定人数列表列出了当前法定人数里的监视器节点。

This is also available more directly:
也可以更直接地获取：
$ ./ceph quorum_status
2011-12-14 10:44:20.417705 mon {- [quorum_status]
2011-12-14 10:44:20.431890 mon.0 -}

'{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}]}}' (0)

The above will block until a quorum is reached.
如果法定人数未形成，上述命令会一直等待。

For a status of just the monitor you connect to (use -m HOST:PORT to select):
你刚刚连接的监视器的状态（用-m HOST:PORT另外指定）：
ceph mon_status
2011-12-14 10:45:30.644414 mon {- [mon_status]
2011-12-14 10:45:30.644632 mon.0 -}

'{ "name": "a",
  "rank": 0,
  "state": "leader",
  "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "outside_quorum": [],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}]}}' (0)

A dump of the monitor state:
监视器状态转储：
ceph mon dump
2011-12-14 10:43:08.015333 mon {- [mon,dump]
2011-12-14 10:43:08.015567 mon.0 -} 'dumped monmap epoch 1' (0)
epoch 1
fsid 444b489c-4f16-4b75-83f0-cb8097468898
last_changed 2011-12-12 13:28:27.505520
created 2011-12-12 13:28:27.505520
0: 127.0.0.1:6789/0 mon.a
1: 127.0.0.1:6790/0 mon.b
2: 127.0.0.1:6791/0 mon.c
 3.4  手册页<尚未整理>
Manual Pages
Object Store Manpages
ceph – ceph file system control utility
ceph-authtool – ceph keyring manipulation tool
ceph-clsinfo – show class object information
ceph-conf – ceph conf file tool
ceph-debugpack – ceph debug packer utility
ceph-dencoder – ceph encoder/decoder utility
ceph-mon – ceph monitor daemon
ceph-osd – ceph object storage daemon
ceph-run – restart daemon on core dump
ceph-syn – ceph synthetic workload generator
crushtool – CRUSH map manipulation tool
librados-config – display information about librados
monmaptool – ceph monitor cluster map manipulation tool
osdmaptool – ceph osd cluster map manipulation tool
rados – rados object storage utility

 3.5  API接口
APIS
RADOS object store APIs
RADOS对象存储API
Ceph’s RADOS Object Store has a messaging layer protocol that enables clients to interact with Ceph monitors and OSDs. librados provides this functionality to object store clients in the form of a library. All Ceph clients either use librados or the same functionality encapsulated in librados to interact with the object store. For example, librbd and libcephfs leverage this functionality. You may use librados to interact with Ceph directly (e.g., an application that talks to Ceph, your own interface to Ceph, etc.).
ceph的RADOS对象存储提供了消息传递层协议，用于客户端和ceph监视器和OSD交互，librados以库形式为对象存储客户端提供了这个功能。所有ceph客户端可以用librados或librados里封装的相同功能和对象存储交互，例如librbd和libcephfs就利用了此功能。你可以用librados直接和ceph交互（如和ceph兼容的应用程序、你自己的ceph接口、等等）。

For an overview of where librados appears in the technology stack, see Architecture.
要大致了解librados在技术栈里的地位，参见Architecture。
 3.5.1  librados (C)
librados provides low-level access to the RADOS service. For an overview of RADOS, see Architecture.
librados提供了RADOS服务的底层访问功能，RADOS概览参见Architecture。
 3.5.1.1  实例：连接并写入一个对象
Example: connecting and writing an object
To use Librados, you instantiate a rados_t variable (a cluster handle) and call rados_create() with a pointer to it:
要使用librados，先实例化一个rados_t变量（集群句柄）、再用指向它的指针调用rados_create()：
int err;
rados_t cluster;

err = rados_create(&cluster, NULL);
if (err < 0) {
        fprintf(stderr, "%s: cannot create a cluster handle: %s\n", argv[0], strerror(-err));
        exit(1);
}

Then you configure your rados_t to connect to your cluster, either by setting individual values (rados_conf_set()), using a configuration file (rados_conf_read_file()), using command line options (rados_conf_parse_argv()), or an environment variable (rados_conf_parse_env()):
然后配置rados_t以连接集群，可以挨个设置值（rados_conf_set()）、或者用配置文件（rados_conf_read_file()）、或者用命令行选项（rados_conf_parse_argv()）、或者环境变量（rados_conf_parse_env()）：
err = rados_conf_read_file(cluster, "/path/to/myceph.conf");
if (err < 0) {
        fprintf(stderr, "%s: cannot read config file: %s\n", argv[0], strerror(-err));
        exit(1);
}
Once the cluster handle is configured, you can connect to the cluster with rados_connect():
集群句柄配置好后，就可以用rados_connect()连接了：
err = rados_connect(cluster);
if (err < 0) {
        fprintf(stderr, "%s: cannot connect to cluster: %s\n", argv[0], strerror(-err));
        exit(1);
}
Then you open an “IO context”, a rados_ioctx_t, with rados_ioctx_create():
然后打开一个"IO上下文"，用rados_ioctx_create()打开rados_ioctx_t：
rados_ioctx_t io;
char *poolname = "mypool";

err = rados_ioctx_create(cluster, poolname, &io);
if (err < 0) {
        fprintf(stderr, "%s: cannot open rados pool %s: %s\n", argv[0], poolname, strerror(-err));
        rados_shutdown(cluster);
        exit(1);
}
Note that the pool you try to access must exist.
注意，你访问的存储池必须存在。
Then you can use the RADOS data manipulation functions, for example write into an object called greeting with rados_write_full():
这时你能用RADOS数据修改函数了，例如用rados_write_full()写入名为greeting的对象：
err = rados_write_full(io, "greeting", "hello", 5);
if (err < 0) {
        fprintf(stderr, "%s: cannot write pool %s: %s\n", argv[0], poolname, strerror(-err));
        rados_ioctx_destroy(io);
        rados_shutdown(cluster);
        exit(1);
}
In the end, you’ll want to close your IO context and connection to RADOS with rados_ioctx_destroy() and rados_shutdown():
最后，用rados_ioctx_destroy()和rados_shutdown()分别关闭IO上下文、到RADOS的连接。
rados_ioctx_destroy(io);
rados_shutdown(cluster);
 3.5.1.2  异步IO
Asychronous IO
When doing lots of IO, you often don’t need to wait for one operation to complete before starting the next one. Librados provides asynchronous versions of several operations:
处理大量IO时，通常不必等一个完成再开始下一个。librados提供了几种操作的异步版本：
rados_aio_write()
rados_aio_append()
rados_aio_write_full()
rados_aio_read()
For each operation, you must first create a rados_completion_t that represents what to do when the operation is safe or complete by calling rados_aio_create_completion(). If you don’t need anything special to happen, you can pass NULL:
对每种操作，都必须先创建一个rados_completion_t数据结构来表达做什么、何时安全或显式地调用rados_aio_create_completion()来结束，如果没什么特殊需求，可以仅传递NULL：
rados_completion_t comp;
err = rados_aio_create_completion(NULL, NULL, NULL, &comp);
if (err < 0) {
        fprintf(stderr, "%s: could not create aio completion: %s\n", argv[0], strerror(-err));
        rados_ioctx_destroy(io);
        rados_shutdown(cluster);
        exit(1);
}
Now you can call any of the aio operations, and wait for it to be in memory or on disk on all replicas:
现在你可以调用任意一种aio操作了，然后等它出现在内存、所有复制所在的硬盘里：
err = rados_aio_write(io, "foo", comp, "bar", 3, 0);
if (err < 0) {
        fprintf(stderr, "%s: could not schedule aio write: %s\n", argv[0], strerror(-err));
        rados_aio_release(comp);
        rados_ioctx_destroy(io);
        rados_shutdown(cluster);
        exit(1);
}
rados_wait_for_complete(comp); // in memory
rados_wait_for_safe(comp); // on disk
Finally, we need to free the memory used by the completion with rados_aio_release():
最后，用rados_aio_release()释放内存：
rados_aio_release(comp);
You can use the callbacks to tell your application when writes are durable, or when read buffers are full. For example, if you wanted to measure the latency of each operation when appending to several objects, you could schedule several writes and store the ack and commit time in the corresponding callback, then wait for all of them to complete using rados_aio_flush() before analyzing the latencies:
你可以用callback告知应用程序何时可以持续写入、或何时读缓冲是满的。例如，如果你追加几个对象时想衡量每个操作的延时，可以调度几个写操作、并把确认和提交时间保存到相应callback，然后用rados_aio_flush()等它们完成，然后就可以分析延时了：
typedef struct {
        struct timeval start;
        struct timeval ack_end;
        struct timeval commit_end;
} req_duration;

void ack_callback(rados_completion_t comp, void *arg) {
        req_duration *dur = (req_duration *) arg;
        gettimeofday(&dur->ack_end, NULL);
}

void commit_callback(rados_completion_t comp, void *arg) {
        req_duration *dur = (req_duration *) arg;
        gettimeofday(&dur->commit_end, NULL);
}

int output_append_latency(rados_ioctx_t io, const char *data, size_t len, size_t num_writes) {
        req_duration times[num_writes];
        rados_completion_t comps[num_writes];
        for (size_t i = 0; i < num_writes; ++i) {
                gettimeofday(&times[i].start, NULL);
                int err = rados_aio_create_completion((void*) &times[i], ack_callback, commit_callback, &comps[i]);
                if (err < 0) {
                        fprintf(stderr, "Error creating rados completion: %s\n", strerror(-err));
                        return err;
                }
                char obj_name[100];
                snprintf(obj_name, sizeof(obj_name), "foo%ld", (unsigned long)i);
                err = rados_aio_append(io, obj_name, comps[i], data, len);
                if (err < 0) {
                        fprintf(stderr, "Error from rados_aio_append: %s", strerror(-err));
                        return err;
                }
        }
        // wait until all requests finish *and* the callbacks complete
        rados_aio_flush(io);
        // the latencies can now be analyzed
        printf("Request # | Ack latency (s) | Commit latency (s)\n");
        for (size_t i = 0; i < num_writes; ++i) {
                // don't forget to free the completions
                rados_aio_release(comps[i]);
                struct timeval ack_lat, commit_lat;
                timersub(&times[i].ack_end, &times[i].start, &ack_lat);
                timersub(&times[i].commit_end, &times[i].start, &commit_lat);
                printf("%9ld | %8ld.%06ld | %10ld.%06ld\n", (unsigned long) i, ack_lat.tv_sec, ack_lat.tv_usec, commit_lat.tv_sec, commit_lat.tv_usec);
        }
        return 0;
}
Note that all the rados_completion_t must be freed with rados_aio_release() to avoid leaking memory.
注意，所有rados_completion_t都必须用rados_aio_release()释放内存，以免造成内存泄漏。
 3.5.1.3  API调用
API calls
 3.5.1.3.1  rados_pool_stat_t数据结构
STRUCT RADOS_POOL_STAT_T
struct rados_pool_stat_t
Usage information for a pool.
存储池用法信息。
成员
MEMBERS
uint64_t num_bytes
	space used in bytes
	已用空间，字节数
uint64_t num_kb
	space used in KB
	已用空间，KB；
uint64_t num_objects
number of objects in the pool
	存储池里的对象数量；
uint64_t num_object_clones
	number of clones of objects
	对象的克隆数量；
uint64_t num_object_copies
num_objects * num_replicas
uint64_t num_objects_missing_on_primary
uint64_t num_objects_unfound
	number of objects found on no OSDs
	未在OSD上找到的对象数量；
uint64_t num_objects_degraded
	number of objects replicated fewer times than they should be (but found on at least one OSD)
	复制次数小于规定值的对象数量（但是发现于至少一个OSD）
uint64_t num_rd
uint64_t num_rd_kb
uint64_t num_wr
uint64_t num_wr_kb
 3.5.1.3.2  rados_cluster_stat_t数据结构
STRUCT RADOS_CLUSTER_STAT_T
struct rados_cluster_stat_t
Cluster-wide usage information.
集群范畴的用法信息。
成员
MEMBERS
uint64_t kb
uint64_t kb_used
uint64_t kb_avail
uint64_t num_objects
 3.5.1.3.3  定义
DEFINES
CEPH_OSD_TMAP_HDR
CEPH_OSD_TMAP_SET
CEPH_OSD_TMAP_CREATE
CEPH_OSD_TMAP_RM
LIBRADOS_VER_MAJOR
LIBRADOS_VER_MINOR
LIBRADOS_VER_EXTRA
LIBRADOS_VERSION
LIBRADOS_VERSION_CODE
LIBRADOS_SUPPORTS_WATCH
LIBRADOS_SNAP_HEAD
LIBRADOS_SNAP_DIR
 3.5.1.3.4  类
TYPES
rados_t
A handle for interacting with a RADOS cluster.
It encapsulates all RADOS client configuration, including username, key for authentication, logging, and debugging. Talking different clusters – or to the same cluster with different users – requires different cluster handles.
和RADOS集群交互的句柄。
它封装了所有RADOS客户端配置，包括用户名、认证的密钥、日志、调试的配置。与不同集群交互、或以不同用户身份访问同一集群要用不同句柄。
rados_config_t
rados_config_t
A handle for the ceph configuration context for the rados_t cluster instance. This can be used to share configuration context/state (e.g., logging configuration) between librados instance.
rados_t集群例程的配置上下文句柄，可用于librados之间共享配置内容/状态（例如日志配置）。

Warning The config context does not have independent reference counting. As such, a rados_config_t handle retrieved from a given rados_t is only valid as long as that rados_t.
警告：配置上下文没有独立的引用计数，这样，从某rados_t获取的rados_config_t句柄仅在rados_t存活的时候有效。

rados_ioctx_t
An io context encapsulates a few settings for all I/O operations done on it:
一个IO上下文封装了所有和其相关的I/O操作的一些配置：
pool - set when the io context is created (see rados_ioctx_create())
存储池——上下文创建时就设置了（见rados_ioctx_create()）
snapshot context for writes (see rados_ioctx_selfmanaged_snap_set_write_ctx())
用于写的快照上下文（见rados_ioctx_selfmanaged_snap_set_write_ctx()）
snapshot id to read from (see rados_ioctx_snap_set_read())
要读的快照ID（见rados_ioctx_snap_set_read()）
object locator for all single-object operations (see rados_ioctx_locator_set_key())
用于所有单对象操作的对象定位器（见rados_ioctx_locator_set_key())

Warning changing any of these settings is not thread-safe - librados users must synchronize any of these changes on their own, or use separate io contexts for each thread.
警告：这些设置的更改不是线程安全的，librados用户必须自己同步任何更改、或为每个线程使用单独的IO上下文。

rados_list_ctx_t
An iterator for listing the objects in a pool.
Used with rados_objects_list_open(),rados_objects_list_next(), andrados_objects_list_close().
用于列出存储池中对象的迭代器。
和rados_objects_list_open()、rados_objects_list_next()、andrados_objects_list_close()一起使用。

rados_snap_t
The id of a snapshot.
快照的ID。

rados_xattrs_iter_t
An iterator for listing extended attrbutes on an object.
Used with rados_getxattrs(),rados_getxattrs_next(), andrados_getxattrs_end().
列出一个对象扩展属性的迭代器。
和rados_getxattrs()、rados_getxattrs_next()、andrados_getxattrs_end()一起使用。

rados_completion_t
Represents the state of an asynchronous operation - it contains the return value once the operation completes, and can be used to block until the operation is complete or safe.
表达异步操作状态，操作完成后会包含返回值，也可用于阻塞，直到操作完成或变安全了。

rados_callback_t
Callbacks for asynchrous operations, take two parameters:
异步操作回调，需2个参数：
cb the completion that has finished
cb刚完成的操作；
arg application defined data made available to the callback function
arg回调函数可访问的应用程序数据

rados_watchcb_t
Callback activated when a notify is received on a watched object.
关于被监视对象的通知收到时，激活回调。
Parameters are:
参数有：
opcode undefined
opcode未定义；
ver version of the watched object
ver被监视对象的版本；
arg application-specific data
arg具体应用程序数据。
Note BUG: opcode is an internal detail that shouldn’t be exposed
注意：缺陷：opcode是不应该暴露的内部细节。
 3.5.1.3.5  函数
FUNCTIONS
void rados_version(int *major, int *minor, int *extra)
Get the version of librados.
The version number is major.minor.extra. Note that this is unrelated to the Ceph version number.
TODO: define version semantics, i.e.:
获取librados版本。
版本号依次为：主.次.额外。注意这和ceph版本无关。
待完成：定义版本语义，例如：

incrementing major is for backwards-incompatible changes
incrementing minor is for backwards-compatible changes
incrementing extra is for bug fixes
递增的主版本是向后不兼容的变更；
递增的次版本是向后兼容的变更；
递增的额外版本是缺陷修正。

Parameters:
major – where to store the major version number
minor – where to store the minor version number
extra – where to store the extra version number

int rados_create(rados_t *cluster, const char *const id)
Create a handle for communicating with a RADOS cluster.
创建句柄用于和RADOS集群通信。

Ceph environment variables are read when this is called, so if $CEPH_ARGS specifies everything you need to connect, no further configuration is necessary.
调用此函数时会读取ceph环境变量，所以如果$CEPH_ARGS给足了用于连接的信息，其他配置就不必要了。

Parameters:
cluster – where to store the handle
id – the user to connect as (i.e. admin, not client.admin)
Returns:
0 on success, negative error code on failure

int rados_create_with_context(rados_t *cluster, rados_config_t cct)
Initialize a cluster handle from an existing configuration.
根据已有配置初始化一个集群句柄。
Share configuration state with another rados_t instance.
和另外一个rados_t例程共享配置状态。
Parameters:
cluster – where to store the handle
cct_ – the existing configuration to use
Returns:
0 on success, negative error code on failure

int rados_connect(rados_t cluster)
Connect to the cluster.
连接到集群。
Note BUG: Before calling this, calling a function that communicates with the cluster will crash.
注意：缺陷：调用此函数前，调用和集群通信的函数将崩溃。
Precondition: The cluster handle is configured with at least a monitor address. If cephx is enabled, a client name and secret must also be set.
Postcondition: If this succeeds, any function in librados may be used
前提条件：集群句柄必须配置至少一个监视器地址。如果启用了cephx，客户端名字和密钥也必须设置。
后置条件：如果此函数成功了，librados里的任何函数都可用。

Parameters:
cluster – The cluster to connect to.
Returns:
0 on sucess, negative error code on failure

void rados_shutdown(rados_t cluster)
Disconnects from the cluster.
For clean up, this is only necessary after rados_connect()has succeeded.
断开集群连接。
要清理的话，这仅在rados_connect()成功时必要。
Warning This does not guarantee any asynchronous writes have completed. To do that, you must call rados_aio_flush()on all open io contexts.
警告：此函数不保证任何异步写已完成。要确认，必须对所有已开io上下文调用rados_aio_flush()。

Postcondition the cluster handle cannot be used again
后置条件：集群句柄不能再次使用。

Parameters:
cluster – the cluster to shutdown

int rados_conf_read_file(rados_t cluster, const char *path)
Configure the cluster handle using a Ceph config file.
If path is NULL, the default locations are searched, and the first found is used. The locations are:
用配置文件配置集群句柄。
如果路径为空，就搜索默认路径，并使用第一个找到的。搜索位置有：
$CEPH_CONF (environment variable)
/etc/ceph/ceph.conf
~/.ceph/config
ceph.conf (in the current working directory)
Precondition rados_connect()has not been called on the cluster handle
前提条件：集群句柄尚未调用rados_connect()。
Parameters:
cluster – cluster handle to configure
path – path to a Ceph configuration file
Returns:
0 on success, negative error code on failure

int rados_conf_parse_argv(rados_t cluster, int argc, const char **argv)
Configure the cluster handle with command line arguments.
用命令行参数配置集群句柄。

argv can contain any common Ceph command line option, including any configuration parameter prefixed by ‘–’ and replacing spaces with dashes or underscores. For example, the following options are equivalent:
argv可包含任何通用的ceph命令行选项，包括任何以'-'打头的配置参数、和用连字符或下划线替代空格。例如，下列选项是等价的：
--mon-host 10.0.0.1:6789
--mon_host 10.0.0.1:6789
-m 10.0.0.1:6789

Precondition rados_connect()has not been called on the cluster handle
前提条件：集群句柄尚未调用rados_connect()。

Parameters:
cluster – cluster handle to configure
argc – number of arguments in argv
argv – arguments to parse
Returns:
0 on success, negative error code on failure

int rados_conf_parse_env(rados_t cluster, const char *var)
Configure the cluster handle based on an environment variable.
The contents of the environment variable are parsed as if they were Ceph command line options. If var is NULL, the CEPH_ARGS environment variable is used.
用环境变量配置集群句柄。
如果环境变量内容像ceph命令行选项，它就会被分析。若var值为NULL，就会用到CEPH_ARGS环境变量。
Precondition rados_connect()has not been called on the cluster handle
前提条件：集群句柄尚未调用rados_connect()。
Note BUG: this is not threadsafe - it uses a static buffer
注意：缺陷：此函数不是线程安全的，它用静态缓冲区。
Parameters:
cluster – cluster handle to configure
var – name of the environment variable to read
Returns:
0 on success, negative error code on failure

int rados_conf_set(rados_t cluster, const char *option, const char *value)
Set a configuration option.
设置选项。

Precondition rados_connect()has not been called on the cluster handle
前提条件：集群句柄尚未调用rados_connect()。

Parameters:
cluster – cluster handle to configure
option – option to set
value – value of the option
Returns:
0 on success, negative error code on failure
-ENOENT when the option is not a Ceph configuration option

int rados_conf_get(rados_t cluster, const char *option, char *buf, size_t len)
Get the value of a configuration option.
获取配置选项值。

Parameters:
cluster – configuration to read
option – which option to read
buf – where to write the configuration value
len – the size of buf in bytes
Returns:
0 on success, negative error code on failure
-ENAMETOOLONG if the buffer is too short to contain the requested value

int rados_cluster_stat(rados_t cluster, struct rados_cluster_stat_t *result)
Read usage info about the cluster.
This tells you total space, space used, space available, and number of objects. These are not updated immediately when data is written, they are eventually consistent.
读取集群的用法信息。
它会告诉你总空间、已用空间、可用空间、和对象数量。这些不会在数据写入时立即更新，但最终会一致。
Parameters:
cluster – cluster to query
result – where to store the results
Returns:
0 on success, negative error code on failure

int rados_cluster_fsid(rados_t cluster, char *buf, size_t len)
Get the fsid of the cluster as a hexadecimal string.
The fsid is a unique id of an entire Ceph cluster.
获取集群的fsid，十六进制格式。
fsid是整个ceph集群的唯一ID。
Parameters:
cluster – where to get the fsid
buf – where to write the fsid
len – the size of buf in bytes (should be 37)
Returns:
0 on success, negative error code on failure
-ERANGE if the buffer is too short to contain the fsid

int rados_pool_list(rados_t cluster, char *buf, size_t len)
List objects in a pool.
Gets a list of pool names as NULL-terminated strings. The pool names will be placed in the supplied buffer one after another. After the last pool name, there will be two 0 bytes in a row.
If len is too short to fit all the pool name entries we need, we will fill as much as we can.
列出存储池里的对象。
获取存储池列表，它将以NULL结尾的字符串挨个放置于指定缓冲区。最后一个存储池名字后面会有一行、2个0字节。
如果len太小，放不下存储池名称，它会尽可能多地填充。

Parameters:
cluster – cluster handle
buf – output buffer
len – output buffer length
Returns:
length of the buffer we would need to list all pools

rados_config_t rados_cct(rados_t cluster)
Get a configuration handle for a rados cluster handle.
This handle is valid only as long as the cluster handle is valid.
为rados集群句柄获取一个配置句柄。
仅在集群句柄有效时此句柄才有效。
Parameters:
cluster – cluster handle
Returns:
config handle for this cluster

uint64_t rados_get_instance_id(rados_t cluster)
Get a global id for current instance.
This id is a unique representation of current connection to the cluster
为当前例程申请个全局ID。
此ID是当前和集群连接的唯一标识符。
Parameters:
cluster – cluster handle
Returns:
instance global id

int rados_ioctx_create(rados_t cluster, const char *pool_name, rados_ioctx_t *ioctx)
Create an io context.
The io context allows you to perform operations within a particular pool. For more details see rados_ioctx_t.
创建一个io上下文。
此io上下文允许你在特定存储池内进行操作，更多细节见rados_ioctx_t。
Parameters:
cluster – which cluster the pool is in
pool_name – name of the pool
ioctx – where to store the io context
Returns:
0 on success, negative error code on failure

void rados_ioctx_destroy(rados_ioctx_t io)
The opposite of rados_ioctx_create.
This just tells librados that you no longer need to use the io context. It may not be freed immediately if there are pending asynchronous requests on it, but you should not use an io context again after calling this function on it.
和rados_ioctx_create相反。
这只是告诉librados你不再需要io上下文了。如果还有关于它的未决异步请求，它就不会被立即释放，但是调用此函数后就不应该再在其上使用io上下文了。
Warning This does not guarantee any asynchronous writes have completed. You must call rados_aio_flush()on the io context before destroying it to do that.
警告：这不保证任何异步写已完成，要那样做，就得在杀死io上下文前先在其上调用rados_aio_flush()。
Parameters:
io – the io context to dispose of

rados_config_t rados_ioctx_cct(rados_ioctx_t io)
Get configuration handle for a pool handle.
为一个存储池获取配置句柄。
Parameters:
io – pool handle
Returns:
rados_config_t for this cluster

rados_t rados_ioctx_get_cluster(rados_ioctx_t io)
Get the cluster handle used by this rados_ioctx_t. Note that this is a weak reference, and should not be destroyed via rados_destroy().
获取rados_ioctx_t用着的集群句柄。注意这是一个弱引用，不应该通过rados_destroy()杀死。
Parameters:
io – the io context
Returns:
the cluster handle for this io context

int rados_ioctx_pool_stat(rados_ioctx_t io, struct rados_pool_stat_t *stats)
Get pool usage statistics.
Fills in a rados_pool_stat_t after querying the cluster.
获取存储池利用统计信息。
查询集群后填充rados_pool_stat_t。

Parameters:
io – determines which pool to query
stats – where to store the results
Returns:
0 on success, negative error code on failure

int64_t rados_pool_lookup(rados_t cluster, const char *pool_name)
Get the id of a pool.
获取存储池的ID。

Parameters:
cluster – which cluster the pool is in
pool_name – which pool to look up
Returns:
id of the pool
-ENOENT if the pool is not found

int rados_pool_reverse_lookup(rados_t cluster, int64_t id, char *buf, size_t maxlen)
Get the name of a pool.
获取存储池名字。

Parameters:
cluster – which cluster the pool is in
id – the id of the pool
buf – where to store the pool name
maxlen – size of buffer where name will be stored
Returns:
length of string stored, or -ERANGE if buffer too small

int rados_pool_create(rados_t cluster, const char *pool_name)
Create a pool with default settings.
The default owner is the admin user (auid 0). The default crush rule is rule 0.
用默认配置创建一个存储池。
默认所有者是admin用户（auid 0），默认crush规则是rule 0。

Parameters:
cluster – the cluster in which the pool will be created
pool_name – the name of the new pool
Returns:
0 on success, negative error code on failure

int rados_pool_create_with_auid(rados_t cluster, const char *pool_name, uint64_t auid)
Create a pool owned by a specific auid.
The auid is the authenticated user id to give ownership of the pool. TODO: document auid and the rest of the auth system
创建所有者为指定auid的存储池。
auid是存储池所有者的已认证用户ID。待完成：写作认证系统的auid和及其它。
Parameters:
cluster – the cluster in which the pool will be created
pool_name – the name of the new pool
auid – the id of the owner of the new pool
Returns:
0 on success, negative error code on failure

int rados_pool_create_with_crush_rule(rados_t cluster, const char *pool_name, __u8 crush_rule_num)
Create a pool with a specific CRUSH rule.
创建有指定CRUSH规则的存储池。
Parameters:
cluster – the cluster in which the pool will be created
pool_name – the name of the new pool
crush_rule_num – which rule to use for placement in the new pool1
Returns:
0 on success, negative error code on failure

int rados_pool_create_with_all(rados_t cluster, const char *pool_name, uint64_t auid, __u8 crush_rule_num)
Create a pool with a specific CRUSH rule and auid.
This is a combination of rados_pool_create_with_crush_rule() and rados_pool_create_with_auid().
创建有指定CRUSH规则和auid的存储池。
这是rados_pool_create_with_crush_rule()和rados_pool_create_with_auid()的组合。
Parameters:
cluster – the cluster in which the pool will be created
pool_name – the name of the new pool
crush_rule_num – which rule to use for placement in the new pool2
auid – the id of the owner of the new pool
Returns:
0 on success, negative error code on failure

int rados_pool_delete(rados_t cluster, const char *pool_name)
Delete a pool and all data inside it.
The pool is removed from the cluster immediately, but the actual data is deleted in the background.
删除存储池及其内所有数据。
存储池会立即从集群里删除，但是实际的数据将在后台删除。
Parameters:
cluster – the cluster the pool is in
pool_name – which pool to delete
Returns:
0 on success, negative error code on failure

int rados_ioctx_pool_set_auid(rados_ioctx_t io, uint64_t auid)
Attempt to change an io context’s associated auid “owner.”.
Requires that you have write permission on both the current and new auid.
尝试更改一个和io上下文相关的所有者auid。
要求你同时具有当前和新auid的写权限。
Parameters:
io – reference to the pool to change.
auid – the auid you wish the io to have.
Returns:
0 on success, negative error code on failure

int rados_ioctx_pool_get_auid(rados_ioctx_t io, uint64_t *auid)
Get the auid of a pool.
获取一个存储池的auid。
Parameters:
io – pool to query
auid – where to store the auid
Returns:
0 on success, negative error code on failure

int64_t rados_ioctx_get_id(rados_ioctx_t io)
Get the pool id of the io context.
获取io上下文的存储池ID。
Parameters:
io – the io context to query
Returns:
the id of the pool the io context uses

int rados_ioctx_get_pool_name(rados_ioctx_t io, char *buf, unsigned maxlen)
Get the pool name of the io context.
获取io上下文的存储池名字。
Parameters:
io – the io context to query
buf – pointer to buffer where name will be stored
maxlen – size of buffer where name will be stored
Returns:
length of string stored, or -ERANGE if buffer too small

void rados_ioctx_locator_set_key(rados_ioctx_t io, const char *key)
Set the key for mapping objects to pgs within an io context.
The key is used instead of the object name to determine which placement groups an object is put in. This affects all subsequent operations of the io context - until a different locator key is set, all objects in this io context will be placed in the same pg.
设置键名，用于把io上下文内的对象映射到归置组。
ceph用键名而非对象名来确定对象放到了哪个归置组，这影响io上下文的所有后续操作，除非设置了另一个定位键名，此io上下文里的所有对象都将放到相同归置组。
This is useful if you need to do clone_range operations, which must be done with the source and destination objects in the same pg.
这在做clone_range操作的时候有用，因为它只能在源、目的对象都在相同归置组时完成。
Parameters:
io – the io context to change
key – the key to use as the object locator, or NULL to discard any previously set key

int rados_objects_list_open(rados_ioctx_t io, rados_list_ctx_t *ctx)
Start listing objects in a pool.
开列存储池里的对象。

Parameters:
io – the pool to list from
ctx – the handle to store list context in
Returns:
0 on success, negative error code on failure

int rados_objects_list_next(rados_list_ctx_t ctx, const char **entry, const char **key)
Get the next object name and locator in the pool.
*entry and *key are valid until next call to rados_objects_list_*
获取存储池里下个对象的名字和定位符。
*entry和*key在再次调用rados_objects_list_*前都是有效的。
Parameters:
ctx – iterator marking where you are in the listing
entry – where to store the name of the entry
key – where to store the object locator (set to NULL to ignore)
Returns:
0 on success, negative error code on failure
-ENOENT when there are no more objects to list

void rados_objects_list_close(rados_list_ctx_t ctx)
Close the object listing handle.
This should be called when the handle is no longer needed. The handle should not be used after it has been closed.
关闭列出对象句柄。
句柄不再需要时应该调用此函数，且调用后不应该再使用此句柄。
Parameters:
ctx – the handle to close

int rados_ioctx_snap_create(rados_ioctx_t io, const char *snapname)
Create a pool-wide snapshot.
创建存储池快照。
Parameters:
io – the pool to snapshot
snapname – the name of the snapshot
Returns:
0 on success, negative error code on failure

int rados_ioctx_snap_remove(rados_ioctx_t io, const char *snapname)
Delete a pool snapshot.
删除存储池快照。
Parameters:
io – the pool to delete the snapshot from
snapname – which snapshot to delete
Returns:
0 on success, negative error code on failure

int rados_rollback(rados_ioctx_t io, const char *oid, const char *snapname)
Rollback an object to a pool snapshot.
The contents of the object will be the same as when the snapshot was taken.
根据存储池快照回滚一个对象。
对象内容应该和拍快照时的内容相同。
Parameters:
io – the pool in which the object is stored
oid – the name of the object to rollback
snapname – which snapshot to rollback to
Returns:
0 on success, negative error code on failure

void rados_ioctx_snap_set_read(rados_ioctx_t io, rados_snap_t snap)
Set the snapshot from which reads are performed.
Subsequent reads will return data as it was at the time of that snapshot.
指定从哪个快照执行读操作。
后续读操作将返回拍下快照时的数据。
Parameters:
io – the io context to change
snap – the id of the snapshot to set, or CEPH_NOSNAP for no snapshot (i.e. normal operation)

int rados_ioctx_selfmanaged_snap_create(rados_ioctx_t io, rados_snap_t *snapid)
Allocate an ID for a self-managed snapshot.
Get a unique ID to put in the snaphot context to create a snapshot. A clone of an object is not created until a write with the new snapshot context is completed.
给自管理的快照分配一个ID。
获取一个唯一ID，用于创建快照时放入上下文，在完成快照上下文的写操作前不会克隆对象。
Parameters:
io – the pool in which the snapshot will exist
snapid – where to store the newly allocated snapshot ID
Returns:
0 on success, negative error code on failure

int rados_ioctx_selfmanaged_snap_remove(rados_ioctx_t io, rados_snap_t snapid)
Remove a self-managed snapshot.
This increases the snapshot sequence number, which will cause snapshots to be removed lazily.
删除自管理的快照。
这会增加快照序列号，将导致懒散地删除快照。
Parameters:
io – the pool in which the snapshot will exist
snapid – where to store the newly allocated snapshot ID
Returns:
0 on success, negative error code on failure

int rados_ioctx_selfmanaged_snap_rollback(rados_ioctx_t io, const char *oid, rados_snap_t snapid)
Rollback an object to a self-managed snapshot.
The contents of the object will be the same as when the snapshot was taken.
把对象回滚到一个自管理快照。
对象内容将和拍下快照时一样。
Parameters:
io – the pool in which the object is stored
oid – the name of the object to rollback
snapid – which snapshot to rollback to
Returns:
0 on success, negative error code on failure

int rados_ioctx_selfmanaged_snap_set_write_ctx(rados_ioctx_t io, rados_snap_t seq, rados_snap_t *snaps, int num_snaps)
Set the snapshot context for use when writing to objects.
This is stored in the io context, and applies to all future writes.
设置写入对象时的快照上下文。
这会存储在io上下文内，并且应用到未来所有写操作里。
Parameters:
io – the io context to change
seq – the newest snapshot sequence number for the pool
snaps – array of snapshots in sorted by descending id
num_snaps – how many snaphosts are in the snaps array
Returns:
0 on success, negative error code on failure
-EINVAL if snaps are not in descending order

int rados_ioctx_snap_list(rados_ioctx_t io, rados_snap_t *snaps, int maxlen)
List all the ids of pool snapshots.
If the output array does not have enough space to fit all the snapshots, -ERANGE is returned and the caller should retry with a larger array.
列出存储池快照的所有ID。
如果输出阵列没有足够空间容纳所有快照，就会返回-ERANGE，调用者应该尝试更大的阵列。
Parameters:
io – the pool to read from
snaps – where to store the results
maxlen – the number of rados_snap_t that fit in the snaps array
Returns:
number of snapshots on success, negative error code on failure
-ERANGE is returned if the snaps array is too short

int rados_ioctx_snap_lookup(rados_ioctx_t io, const char *name, rados_snap_t *id)
Get the id of a pool snapshot.
获取存储池快照的ID。
Parameters:
io – the pool to read from
name – the snapshot to find
id – where to store the result
Returns:
0 on success, negative error code on failure

int rados_ioctx_snap_get_name(rados_ioctx_t io, rados_snap_t id, char *name, int maxlen)
Get the name of a pool snapshot.
获取存储池快照名字。
Parameters:
io – the pool to read from
id – the snapshot to find
name – where to store the result
maxlen – the size of the name array
Returns:
0 on success, negative error code on failure
-ERANGE if the name array is too small

int rados_ioctx_snap_get_stamp(rados_ioctx_t io, rados_snap_t id, time_t *t)
Find when a pool snapshot occurred.
查出存储池快照时间戳。
Parameters:
io – the pool the snapshot was taken in
id – the snapshot to lookup
t – where to store the result
Returns:
0 on success, negative error code on failure

uint64_t rados_get_last_version(rados_ioctx_t io)
Return the version of the last object read or written to.
This exposes the internal version number of the last object read or written via this io context
返回最后读取或写入对象的版本。
揭露通过此io上下文最后读取或写入对象的内部版本号。
Parameters:
io – the io context to check
Returns:
last read or written object version

int rados_write(rados_ioctx_t io, const char *oid, const char *buf, size_t len, uint64_t off)
Write data to an object.
把数据写入对象。
Parameters:
io – the io context in which the write will occur
oid – name of the object
buf – data to write
len – length of the data, in bytes
off – byte offset in the object to begin writing at
Returns:
number of bytes written on success, negative error code on failure

int rados_write_full(rados_ioctx_t io, const char *oid, const char *buf, size_t len)
Write an entire object.
The object is filled with the provided data. If the object exists, it is atomically truncated and then written.
写入一个完整对象。
用准备好的数据填充对象。如果对象已存在，它会自动删节、然后写入。
Parameters:
io – the io context in which the write will occur
oid – name of the object
buf – data to write
len – length of the data, in bytes
Returns:
0 on success, negative error code on failure

int rados_clone_range(rados_ioctx_t io, const char *dst, uint64_t dst_off, const char *src, uint64_t src_off, size_t len)
Efficiently copy a portion of one object to another.
If the underlying filesystem on the OSD supports it, this will be a copy-on-write clone.
高效地把一对象的一部分拷贝到别处。
如果OSD的底层文件系统支持，这会是写时复制克隆。
The src and dest objects must be in the same pg. To ensure this, the io context should have a locator key set (see rados_ioctx_locator_set_key()).
源和目的对象必须在同一归置组内，要保证这点，io上下文应该设置一个键定位器（见rados_ioctx_locator_set_key()）。
Parameters:
io – the context in which the data is cloned
dst – the name of the destination object
dst_off – the offset within the destination object (in bytes)
src – the name of the source object
src_off – the offset within the source object (in bytes)
len – how much data to copy
Returns:
0 on success, negative error code on failure

int rados_append(rados_ioctx_t io, const char *oid, const char *buf, size_t len)
Append data to an object.
把数据追加到一对象。
Parameters:
io – the context to operate in
oid – the name of the object
buf – the data to append
len – length of buf (in bytes)
Returns:
number of bytes written on success, negative error code on failure

int rados_read(rados_ioctx_t io, const char *oid, char *buf, size_t len, uint64_t off)
Read data from an object.
The io context determines the snapshot to read from, if any was set by rados_ioctx_snap_set_read().
从一对象读数据。
如果rados_ioctx_snap_set_read()设置过了，那就由io上下文决定从哪个快照读。
Parameters:
io – the context in which to perform the read
oid – the name of the object to read from
buf – where to store the results
len – the number of bytes to read
off – the offset to start reading from in the object
Returns:
number of bytes read on success, negative error code on failure

int rados_remove(rados_ioctx_t io, const char *oid)
Delete an object.
删除一对象。
Note This does not delete any snapshots of the object.
注意：这不会删除任何快照里的对应对象。
Parameters:
io – the pool to delete the object from
oid – the name of the object to delete
Returns:
0 on success, negative error code on failure

int rados_trunc(rados_ioctx_t io, const char *oid, uint64_t size)
Resize an object.
If this enlarges the object, the new area is logically filled with zeroes. If this shrinks the object, the excess data is removed.
调整对象大小。
若是扩大对象，新区域是逻辑上用0填充的；若是缩小对象，多余数据将被删除。
Parameters:
io – the context in which to truncate
oid – the name of the object
size – the new size of the object in bytes
Returns:
0 on success, negative error code on failure

int rados_getxattr(rados_ioctx_t io, const char *o, const char *name, char *buf, size_t len)
Get the value of an extended attribute on an object.
获取一对象的扩展属性值。
Parameters:
io – the context in which the attribute is read
o – name of the object
name – which extended attribute to read
buf – where to store the result
len – size of buf in bytes
Returns:
length of xattr value on success, negative error code on failure

int rados_setxattr(rados_ioctx_t io, const char *o, const char *name, const char *buf, size_t len)
Set an extended attribute on an object.
设置一对象的扩展属性。
Parameters:
io – the context in which xattr is set
o – name of the object
name – which extended attribute to set
buf – what to store in the xattr
len – the number of bytes in buf
Returns:
0 on success, negative error code on failure

int rados_rmxattr(rados_ioctx_t io, const char *o, const char *name)
Delete an extended attribute from an object.
删除一对象的扩展属性。
Parameters:
io – the context in which to delete the xattr
o – the name of the object
name – which xattr to delete
Returns:
0 on success, negative error code on failure

int rados_getxattrs(rados_ioctx_t io, const char *oid, rados_xattrs_iter_t *iter)
Start iterating over xattrs on an object.
Postcondition iter is a valid iterator
开始递归一对象的xattr。
后置条件：iter是合法的递归器。
Parameters:
io – the context in which to list xattrs
oid – name of the object
iter – where to store the iterator
Returns:
0 on success, negative error code on failure

int rados_getxattrs_next(rados_xattrs_iter_t iter, const char **name, const char **val, size_t *len)
Get the next xattr on the object.
获取对象的下个xattr。
Precondition iter is a valid iterator
Postcondition name is the NULL-terminated name of the next xattr, and val contains the value of the xattr, which is of length len. If the end of the list has been reached, name and val are NULL, and len is 0.
前提条件：iter是合法递归器。
后置条件：name是NULL结尾的下一个xattr名字；val包含xattr的值，它是长度len。到达列表末尾后，name和val为NULL、len为0。
Parameters:
iter – iterator to advance
name – where to store the name of the next xattr
val – where to store the value of the next xattr
len – the number of bytes in val
Returns:
0 on success, negative error code on failure

void rados_getxattrs_end(rados_xattrs_iter_t iter)
Close the xattr iterator.
iter should not be used after this is called.
关闭xattr递归器。
调用此函数后iter寿终。
Parameters:
iter – the iterator to close

int rados_stat(rados_ioctx_t io, const char *o, uint64_t *psize, time_t *pmtime)
Get object stats (size/mtime)
TODO: when are these set, and by whom? can they be out of date?
获取对象状态：尺寸、修改时间。
待完成：何时设置、谁设置的、它们可以失真么？
Parameters:
io – ioctx
o – object name
psize – where to store object size
pmtime – where to store modification time
Returns:
0 on success, negative error code on failure

int rados_tmap_update(rados_ioctx_t io, const char *o, const char *cmdbuf, size_t cmdbuflen)
Update tmap (trivial map)
Do compound update to a tmap object, inserting or deleting some number of records. cmdbuf is a series of operation byte codes, following by command payload. Each command is a single-byte command code, whose value is one of CEPH_OSD_TMAP_*.
更新tmap（普通map）。
混合更新一个tmap对象，插入或删除一些记录。cmdbuf是一系列操作字节码，其后是命令载荷。每个命令都是一个单字节命令代码，其值是CEPH_OSD_TMAP_*之一。
update tmap ‘header’
1 byte = CEPH_OSD_TMAP_HDR
4 bytes = data length (little endian)
N bytes = data
insert/update one key/value pair
1 byte = CEPH_OSD_TMAP_SET
4 bytes = key name length (little endian)
N bytes = key name
4 bytes = data length (little endian)
M bytes = data
insert one key/value pair; return -EEXIST if it already exists.
1 byte = CEPH_OSD_TMAP_CREATE
4 bytes = key name length (little endian)
N bytes = key name
4 bytes = data length (little endian)
M bytes = data
remove one key/value pair
1 byte = CEPH_OSD_TMAP_RM
4 bytes = key name length (little endian)
N bytes = key name
Restrictions:
The HDR update must preceed any key/value updates.
All key/value updates must be in lexicographically sorted order in cmdbuf.
You can read/write to a tmap object via the regular APIs, but you should be careful not to corrupt it. Also be aware that the object format may change without notice.
限制条件：
HDR更新必须先于key/value更新。
所有key/value更新必须按cmdbuf里的字典顺序进行。
你可以通过正常的API读写tmap对象，但要注意不要伤害它。注意，对象格式可能在未通知的情况下更改。
Parameters:
io – ioctx
o – object name
cmdbuf – command buffer
cmdbuflen – command buffer length in bytes
Returns:
0 on success, negative error code on failure

int rados_tmap_put(rados_ioctx_t io, const char *o, const char *buf, size_t buflen)
Store complete tmap (trivial map) object.
Put a full tmap object into the store, replacing what was there.
存储完整的tmap（寻常map）对象。
放入一个完整的tmap对象，取代先前的。
The format of buf is:
buf格式为：
4 bytes - length of header (little endian)
N bytes - header data
4 bytes - number of keys (little endian)
and for each key,
以及每个键：
4 bytes - key name length (little endian)
N bytes - key name
4 bytes - value length (little endian)
M bytes - value data
Parameters:
io – ioctx
o – object name
buf – buffer
buflen – buffer length in bytes
Returns:
0 on success, negative error code on failure

int rados_tmap_get(rados_ioctx_t io, const char *o, char *buf, size_t buflen)
Fetch complete tmap (trivial map) object.
Read a full tmap object. See rados_tmap_put() for the format the data is returned in.
取来完整的tmap（寻常map）对象。
读取一个完整tmap对象，返回的数据格式见rados_tmap_put()。

Parameters:
io – ioctx
o – object name
buf – buffer
buflen – buffer length in bytes
Returns:
0 on success, negative error code on failure
-ERANGE if buf isn’t big enough

int rados_exec(rados_ioctx_t io, const char *oid, const char *cls, const char *method, const char *in_buf, size_t in_len, char *buf, size_t out_len)
Execute an OSD class method on an object.
The OSD has a plugin mechanism for performing complicated operations on an object atomically. These plugins are called classes. This function allows librados users to call the custom methods. The input and output formats are defined by the class. Classes in ceph.git can be found in src/cls_*.cc
对一对象执行OSD类方法。
OSD有个插件机制，用于在一个对象上原子地执行复杂操作，这些插件叫类。此函数允许librados用户调用定制的方法。输入和输出格式由类定义，类位于ceph.git的src/cls_*.cc下。
Parameters:
io – the context in which to call the method
oid – the object to call the method on
cls – the name of the class
method – the name of the method
in_buf – where to find input
in_len – length of in_buf in bytes
buf – where to store output
out_len – length of buf in bytes
Returns:
the length of the output, or -ERANGE if out_buf does not have enough space to store it (For methods that return data). For methods that don’t return data, the return value is method-specific.

int rados_aio_create_completion(void *cb_arg, rados_callback_t cb_complete, rados_callback_t cb_safe, rados_completion_t *pc)
Constructs a completion to use with asynchronous operations.
The complete and safe callbacks correspond to operations being acked and committed, respectively. The callbacks are called in order of receipt, so the safe callback may be triggered before the complete callback, and vice versa. This is affected by journalling on the OSDs.
构造异步操作需要的完成。
操作对应的complete和safe回调会被分别确认、提交，回调按照其收到顺序调用，所以safe回调可能早于complete而触发，反之亦然。它会被OSD上的日志影响。
TODO: more complete documentation of this elsewhere (in the RADOS docs?)
待完成：关于complete的更详细文档（在RADOS文档里？）
Note: Read operations only get a complete callback.
BUG: this should check for ENOMEM instead of throwing an exception
注意：读操作只能得到一个complete回调。
缺陷：此函数应该检查ENOMEM而不是抛出异常。
Parameters:
cb_arg – application-defined data passed to the callback functions
cb_complete – the function to be called when the operation is in memory on all relpicas
cb_safe – the function to be called when the operation is on stable storage on all replicas
pc – where to store the completion
Returns:
0

int rados_aio_wait_for_complete(rados_completion_t c)
Block until an operation completes.
This means it is in memory on all replicas.
操作完成前一直阻塞。
这意味着有关它的所有复制都在内存里。
Note BUG: this should be void
注意：缺陷：这应该是空的。
Parameters:
c – operation to wait for
Returns:
0

int rados_aio_wait_for_safe(rados_completion_t c)
Block until an operation is safe.
This means it is on stable storage on all replicas.
某操作安全前一直阻塞。
这意味着有关它的所有复制都在稳定的存储器上。
Note BUG: this should be void
注意：缺陷：这应该为空。
Parameters:
c – operation to wait for
Returns:
0

int rados_aio_is_complete(rados_completion_t c)
Has an asynchronous operation completed?
异步操作是否完成？
Warning This does not imply that the complete callback has finished
警告：这并不意味着完成回调已结束。
Parameters:
c – async operation to inspect
Returns:
whether c is complete

int rados_aio_is_safe(rados_completion_t c)
Is an asynchronous operation safe?
某异步操作是否安全？
Warning This does not imply that the safe callback has finished
警告：这并不意味着安全回调已完成。
Parameters:
c – async operation to inspect
Returns:
whether c is safe

int rados_aio_wait_for_complete_and_cb(rados_completion_t c)
Block until an operation completes and callback completes.
This means it is in memory on all replicas and can be read.
在操作完成、其回调完成前一直阻塞。
这意思是关于它的所有复制都在内存里，且可读。
Note BUG: this should be void
注意：缺陷：这应该为空。
Parameters:
c – operation to wait for
Returns:
0

int rados_aio_wait_for_safe_and_cb(rados_completion_t c)
Block until an operation is safe and callback has completed.
在操作安全且其回调完成前一直阻塞。
This means it is on stable storage on all replicas.
这意思是关于它的所有复制都在稳定存储器上。
Note BUG: this should be void
注意：缺陷：这应该为空。
Parameters:
c – operation to wait for
Returns:
0

int rados_aio_is_complete_and_cb(rados_completion_t c)
Has an asynchronous operation and callback completed.
异步操作及其回调是否完成。
Parameters:
c – async operation to inspect
Returns:
whether c is complete

int rados_aio_is_safe_and_cb(rados_completion_t c)
Is an asynchronous operation safe and has the callback completed.
异步操作是否安全、其回调是否完成。
Parameters:
c – async operation to inspect
Returns:
whether c is safe

int rados_aio_get_return_value(rados_completion_t c)
Get the return value of an asychronous operation.
The return value is set when the operation is complete or safe, whichever comes first.
获取一异步操作的返回值。
操作完成或安全时就设置返回值，先到为准。
Precondition The operation is safe or complete
前提条件：操作安全或完成。
Note BUG: complete callback may never be called when the safe message is received before the complete message
注意：缺陷：在安全消息先于完成消息收到前，不该再调用完成回调。
Parameters:
c – async operation to inspect
Returns:
return value of the operation

void rados_aio_release(rados_completion_t c)
Release a completion.
Call this when you no longer need the completion. It may not be freed immediately if the operation is not acked and committed.
释放一个完成。
你不再需要完成回馈时可调用这个。如果操作未被确认和提交，它就不会立即释放。
Parameters:
c – completion to release

int rados_aio_write(rados_ioctx_t io, const char *oid, rados_completion_t completion, const char *buf, size_t len, uint64_t off)
Write data to an object asynchronously.
Queues the write and returns. The return value of the completion will be 0 on success, negative error code on failure.
异步地把数据写入对象。
把写加入队列然后返回，完成时若返回0，则成功；负数错误代码则失败。
Parameters:
io – the context in which the write will occur
oid – name of the object
completion – what to do when the write is safe and complete
buf – data to write
len – length of the data, in bytes
off – byte offset in the object to begin writing at
Returns:
0 on success, -EROFS if the io context specifies a snap_seq other than CEPH_NOSNAP

int rados_aio_append(rados_ioctx_t io, const char *oid, rados_completion_t completion, const char *buf, size_t len)
Asychronously append data to an object.
Queues the append and returns.
The return value of the completion will be 0 on success, negative error code on failure.
异步地追加数据到对象。
把追加放入队列然后返回。
成功时返回值为0，失败时为负错误代码。
Parameters:
io – the context to operate in
oid – the name of the object
completion – what to do when the append is safe and complete
buf – the data to append
len – length of buf (in bytes)
Returns:
0 on success, -EROFS if the io context specifies a snap_seq other than CEPH_NOSNAP

int rados_aio_write_full(rados_ioctx_t io, const char *oid, rados_completion_t completion, const char *buf, size_t len)
Asychronously write an entire object.
The object is filled with the provided data. If the object exists, it is atomically truncated and then written. Queues the write_full and returns.
The return value of the completion will be 0 on success, negative error code on failure.
异步写整个对象。
用提供的数据填充对象，若对象存在，它会被自动删节、然后写入。把write_full排队并返回。
成功时完成返回值为0，失败时为负数错误代码。
Parameters:
io – the io context in which the write will occur
oid – name of the object
completion – what to do when the write_full is safe and complete
buf – data to write
len – length of the data, in bytes
Returns:
0 on success, -EROFS if the io context specifies a snap_seq other than CEPH_NOSNAP

int rados_aio_remove(rados_ioctx_t io, const char *oid, rados_completion_t completion)
Asychronously remove an object.
Queues the remove and returns.
The return value of the completion will be 0 on success, negative error code on failure.
异步删除对象。
排列删除并返回。
成功时完成返回值为0，失败时为负数错误代码。
Parameters:
io – the context to operate in
oid – the name of the object
completion – what to do when the remove is safe and complete
Returns:
0 on success, -EROFS if the io context specifies a snap_seq other than CEPH_NOSNAP

int rados_aio_read(rados_ioctx_t io, const char *oid, rados_completion_t completion, char *buf, size_t len, uint64_t off)
Asychronously read data from an object.
The io context determines the snapshot to read from, if any was set by rados_ioctx_snap_set_read().
The return value of the completion will be number of bytes read on success, negative error code on failure.
从一对象异步读数据。
如果rados_ioctx_snap_set_read()设置过，io上下文可决定从哪个快照读取。
完成返回值是成功读取的字节数，失败时为负数错误代码。
Note only the ‘complete’ callback of the completion will be called.
注意：只有完成的'complete'回调会被调用。
Parameters:
io – the context in which to perform the read
oid – the name of the object to read from
completion – what to do when the read is complete
buf – where to store the results
len – the number of bytes to read
off – the offset to start reading from in the object
Returns:
0 on success, negative error code on failure

int rados_aio_flush(rados_ioctx_t io)
Block until all pending writes in an io context are safe.
This is not equivalent to calling rados_aio_wait_for_safe()on all write completions, since this waits for the associated callbacks to complete as well.
确定io上下文里的未决写入安全前一直阻塞着。
此函数和在所有完成上调用rados_aio_wait_for_safe()不等价，因为它也等着相关回调完成。
Note BUG: always returns 0, should be void or accept a timeout
注意：缺陷：总是返回0，应该为空或接受超时。
Parameters:
io – the context to flush
Returns:
0 on success, negative error code on failure

int rados_aio_flush_async(rados_ioctx_t io, rados_completion_t completion)
Schedule a callback for when all currently pending aio writes are safe.
This is a non-blocking version of rados_aio_flush().
Parameters:
io – the context to flush
completion – what to do when the writes are safe
Returns:
0 on success, negative error code on failure

int rados_aio_stat(rados_ioctx_t io, const char *o, rados_completion_t completion, uint64_t *psize, time_t *pmtime)
Asynchronously get object stats (size/mtime)
Parameters:
io – ioctx
o – object name
psize – where to store object size
pmtime – where to store modification time
Returns:
0 on success, negative error code on failure

int rados_watch(rados_ioctx_t io, const char *o, uint64_t ver, uint64_t *handle, rados_watchcb_t watchcb, void *arg)
Register an interest in an object.
A watch operation registers the client as being interested in notifications on an object. OSDs keep track of watches on persistent storage, so they are preserved across cluster changes by the normal recovery process. If the client loses its connection to the primary OSD for a watched object, the watch will be removed after 30 seconds. Watches are automatically reestablished when a new connection is made, or a placement group switches OSDs.
关注对象。
关注操作注册了客户端对关于某对象的通知感兴趣。OSD把关注记录在永久存储器上，所以在常规的集群恢复后仍会保留。如果客户端到关注对象所在OSD的连接断开，则相应关注会在30秒后删除。新连接建立时、或归置组跑到另外OSD时关注会自动重建。

Note BUG: watch timeout should be configurable
BUG: librados should provide a way for watchers to notice connection resets
注意：缺陷：关注超时值应该可配置。
缺陷：librados库应该提供一种方法让关注者通知连接重置。

BUG: the ver parameter does not work, and -ERANGE will never be returned (
缺陷：ver参数无效，且-ERANGE永不返回。
)
Parameters:
io – the pool the object is in
o – the object to watch
ver – expected version of the object
handle – where to store the internal id assigned to this watch
watchcb – what to do when a notify is received on this object
arg – application defined data to pass when watchcb is called
Returns:
0 on success, negative error code on failure
-ERANGE if the version of the object is greater than ver

int rados_unwatch(rados_ioctx_t io, const char *o, uint64_t handle)
Unregister an interest in an object.
Once this completes, no more notifies will be sent to us for this watch. This should be called to clean up unneeded watchers.
注销到对象的关注。
它完成后，不会再有相关通知发来。应该用于清理不必要的关注。

Parameters:
io – the pool the object is in
o – the name of the watched object
handle – which watch to unregister
Returns:
0 on success, negative error code on failure

int rados_notify(rados_ioctx_t io, const char *o, uint64_t ver, const char *buf, int buf_len)
Sychronously notify watchers of an object.
This blocks until all watchers of the object have received and reacted to the notify, or a timeout is reached.
同步通知某对象的关注者。
它会一直阻塞着，直到对象的所有关注者收到并回馈了通知、或超时。

Note BUG: the timeout is not changeable via the C API
BUG: the bufferlist is inaccessible in a rados_watchcb_t
注意：缺陷：通过C API不能更改超时值。
缺陷：rados_watchcb_t内的缓冲区列表不可访问。

Parameters:
io – the pool the object is in
o – the name of the object
ver – obsolete - just pass zero
buf – data to send to watchers
buf_len – length of buf in bytes
Returns:
0 on success, negative error code on failure
 3.5.2  libradospp (C++)
Todo write me!
待完成。
 4  CEPH FS
The Ceph FS file system is a POSIX-compliant file system that uses a RADOS cluster to store its data. Ceph FS uses the same RADOS object storage device system as RADOS block devices and RADOS object stores such as the RADOS gateway with its S3 and Swift APIs, or native bindings. Using Ceph FS requires at least one metadata server in your ceph.conf configuration file.
Ceph FS文件系统是个POSIX兼容的文件系统，它使用RADOS集群存储其数据。Ceph FS使用的对象存储设备和RADOS块设备及RADOS对象存储相同，就像RADOS网关也有S3和Swift API、或本地API一样。使用Ceph FS要求ceph.conf配置文件里至少有一个元数据服务器。
 4.1  用内核驱动挂载ceph文件系统
Mount ceph fs with the kernel driver
To mount the Ceph file system you may use the mount command if you know the monitor host IP address(es), or use the mount.ceph utility to resolve the monitor host name(s) into IP address(es) for you. For example:
要挂载ceph文件系统，如果你知道监视器IP地址可以用mount命令、或者用mount.ceph工具来自动解析监视器IP地址。例如：
sudo mkdir /mnt/mycephfs
sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs

To mount the Ceph file system with cephx authentication enabled, you must specify a user name and a secret.
要挂载启用了cephx认证的ceph文件系统，你必须指定用户名、密码。
sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==

The foregoing usage leaves the secret in the Bash history. A more secure approach reads the secret from a file. For example:
前述用法会把密码遗留在bash历史里，更安全的方法是从文件读密码。例如：
sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secretfile=/etc/ceph/admin.secret

See Authentication for details on cephx.
关于cephx参见认证概览。

To unmount the Ceph file system, you may use the umount command. For example:
要卸载ceph文件系统，可以用unmount命令，例如：
sudo umount /mnt/mycephfs

Tip Ensure that you are not within the file system directories before executing this command.
提示：执行此命令前确保你不在此文件系统目录下。

See mount.ceph for details.
详情参见mount.ceph。
 4.2  用户空间挂载ceph文件系统
Mount ceph FS as a fuse
To mount the Ceph file system as a File System in User Space (FUSE), you may use the ceph-fuse command. For example:
要把ceph文件系统挂载为用户空间文件系统（File System in User Space, FUSE），可以用ceph-fuse命令，例如：
sudo mkdir /home/usernname/cephfs
sudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs

If cephx authentication is on, ceph-fuse will retrieve the name and secret from the key ring automatically.
如果启用了cephx认证，ceph-fuse会从密钥环自动检索用户名、密码。

See ceph-fuse for details.
详情参见ceph-fuse——ceph的用户空间客户端。
 4.3  从fstab挂载
Mount ceph fs in your file systems table
If you mount Ceph FS in your file systems table, the Ceph file system will mount automatically on startup. To mount Ceph FS in your file systems table, add the following to /etc/fstab:
如果你从文件系统表挂载，ceph文件系统将在启动时自动挂载。要从文件系统表挂载Ceph FS，按下列格式添加到/etc/fstab：
{ipaddress}:{port}:/ {mount}/{mountpoint} {filesystem-name}     [name=username,secret=secretkey|secretfile=/path/to/secretfile],[{mount.options}]

For example:
例如：
10.10.10.10:6789:/     /mnt/ceph    ceph    name=admin,secretfile=/etc/ceph/secret.key,noauto,rw,noexec,nodev,noatime,nodiratime    0       2

Important The name and secret or secretfile options are mandatory when you have Ceph authentication running. See Authentication for details.
重要：启用了认证时，name及secret或secretfile选项是强制的。详情参见认证概览。
 4.4  让hadoop使用cephfs
Using hadoop with cephfs
 4.4.1  hadoop配置
HADOOP CONFIGURATION
This section describes the Hadoop configuration options used to control Ceph. These options are intended to be set in the Hadoop configuration file conf/core-site.xml.
本段描述了用于控制ceph的hadoop选项，这些选项应该写于Hadoop配置文件conf/core-site.xml。

Property
Value
Notes
fs.default.name
Ceph URI
ceph:///
ceph.conf.file
Local path to ceph.conf
/etc/ceph/ceph.conf
ceph.conf.options
Comma separated list of Ceph configuration key/value pairs
opt1=val1,opt2=val2
ceph.root.dir
Mount root directory
Default value: /
ceph.object.size
Default file object size in bytes
Default value (64MB): 67108864
ceph.data.pools
List of Ceph data pools for storing file.
Default value: default Ceph pool.
ceph.localize.reads
Allow reading from file replica objects
Default value: true
 4.4.2  对每文件定制复制的支持
Support For Per-file Custom Replication
Hadoop users may specify a custom replication factor (e.g. 3 copies of each block) when creating a file. However, object replication factors are controlled on a per-pool basis in Ceph, and by default a Ceph file system will contain a pre-configured pool. In order to support per-file replication Hadoop can be configured to select from alternative pools when creating new files.
Hadoop用户可在创建文件时指定一个定制的复制因子（例如每块3个副本）。然而对象复制因子在ceph里是以每存储池为基数进行控制的，并且ceph文件系统默认会包含一个预配置的存储池。为支持每文件复制策略，Hadoop可配置为创建新文件时选择另一个存储池。

Additional data pools can be specified using the ceph.data.pools configuration option. The value of the option is a comma separated list of pool names. The default Ceph pool will be used automatically if this configuration option is omitted or the value is empty. For example, the following configuration setting will consider the three pools listed.
额外的数据存储池可用ceph.data.pools指定，此选项的值是逗号分隔的一溜存储池名字。此选项被忽略或为空时将使用默认ceph存储池，例如，下面配置了3个存储池：
<property>
  <name>ceph.data.pools</name>
  <value>pool1,pool2,pool5</value>
</property>

Hadoop will not create pools automatically. In order to create a new pool with a specific replication factor use the ceph osd pool create command, and then set the size property on the pool using the ceph osd pool set command. For more information on creating and configuring pools see the RADOS Pool documentation.
Hadoop不会自动创建存储池，要创建有指定复制因子的存储池，可用ceph osd pool create命令、然后用ceph osd pool set命令设置存储池的size属性。更多的创建、配置手册见存储池。

Once a pool has been created and configured the metadata service must be told that the new pool may be used to store file data. A pool can be made available for storing file system data using the ceph mds add_data_pool command.
存储池创建、配置完毕后，新存储池可用于存储文件数据的消息必须告知元数据服务，可用ceph mds add_data_pool命令告知，这样存储池就可存储文件系统数据了。

First, create the pool. In this example we create the hadoop1 pool with replication factor 1.
首先，创建存储池。本例中，我们创建hadoop1存储池，其复制因子为1。
ceph osd pool create hadoop1 100
ceph osd pool set hadoop1 size 1

Next, determine the pool id. This can be done using the ceph osd dump command. For example, we can look for the newly created hadoop1 pool.
下一步，找出存储池ID，命令为ceph osd dump。例如，找出刚创建的hadoop1存储池：
ceph osd dump | grep hadoop1

The output should resemble:
输出应该类似：
pool 3 'hadoop1' rep size 1 min_size 1 crush_ruleset 0...

where 3 is the pool id. Next we will use the pool id reference to register the pool as a data pool for storing file system data.
其中，3是存储池id。下面我们用前述ID把存储池注册为数据存储池，用于存储文件系统数据。
ceph mds add_data_pool 3

The final step is to configure Hadoop to consider this data pool when selecting the target pool for new files.
最后配置Hadoop，让它在为新文件选择目标存储池时考虑此存储池。
<property>
        <name>ceph.data.pools</name>
        <value>hadoop1</value>
</property>
 4.4.3  存储池选择语义
Pool Selection Semantics
The following semantics describe the rules by which Hadoop will choose a pool given a desired replication factor and the set of pools specified using the ceph.data.pools configuration option.
下面的语义描述了Hadoop根据期望复制因子用以从ceph.data.pools配置中选择一个存储池的规则。

1. When no custom pools are specified the default Ceph data pool is used.
未指定存储池时用ceph的默认data存储池。
2. A custom pool with the same replication factor as the default Ceph data pool will override the default.
复制因子相同时，定制存储池优先于ceph的默认data存储池。
3. A pool with a replication factor that matches the desired replication will be chosen if it exists.
复制因子和期望值相同的存储池会被选择。
4. Otherwise, a pool with at least the desired replication factor will be chosen, or the maximum possible.
否则，选择复制因子和期望值最接近的存储池，或者复制因子最大的。
 4.4.4  存储池选择调试
Debugging Pool Selection
Hadoop will produce log file entry when it cannot determine the replication factor of a pool (e.g. it is not configured as a data pool). The log message will appear as follows:
Hadoop不确定存储池复制因子时会产生日志（如它未被配置为数据存储池），日志消息长相如下：
Error looking up replication of pool: <pool name>

Hadoop will also produce a log entry when it wasn’t able to select an exact match for replication. This log entry will appear as follows:
未能选到复制数准确匹配的存储池时Hadoop也会产生日志，其长相如下：
selectDataPool path=<path> pool:repl=<name>:<value> wanted=<value>
 4.5  mds配置参考
mds config reference
mds max file size
Description:
Type:	64-bit Integer Unsigned
Default:	1ULL << 40

mds cache size
Description:
Type:	32-bit Integer
Default:	100000

mds cache mid
Description:
Type:	Float
Default:	0.7

mds mem max
Description:	// KB
Type:	32-bit Integer
Default:	1048576

mds dir commit ratio
Description:
Type:	Float
Default:	0.5

mds dir max commit size
Description:	// MB
Type:	32-bit Integer
Default:	90

mds decay halflife
Description:
Type:	Float
Default:	5

mds beacon interval
Description:
Type:	Float
Default:	4

mds beacon grace
Description:
Type:	Float
Default:	15

mds blacklist interval
Description:	// how long to blacklist failed nodes
Type:	Float
Default:	24.0*60.0
描述：多久把失败节点加入黑名单。

mds session timeout
Description:	// cap bits and leases time out if client idle
Type:	Float
Default:	60
描述：客户端空闲的能力位和租期。

mds session autoclose
Description:	// autoclose idle session
Type:	Float
Default:	300
描述：自动关闭空闲会话。

mds reconnect timeout
Description:	// secs to wait for clients during mds restart
Type:	Float
Default:	45
描述：mds重启期间客户端等待时间，秒。

mds tick interval
Description:
Type:	Float
Default:	5

mds dirstat min interval
Description:	//try to avoid propagating more often than x
Type:	Float
Default:	1
描述：试图使传播频率低于x。

mds scatter nudge interval
Description:	// how quickly dirstat changes propagate up
Type:	Float
Default:	5
描述：dirstat变更传播多快。

mds client prealloc inos
Description:
Type:	32-bit Integer
Default:	1000

mds early reply
Description:
Type:	Boolean
Default:	true

mds use tmap
Description:	// use trivialmap for dir updates
Type:	Boolean
Default:	true

mds default dir hash
Description:	CEPH STR HASH RJENKINS
Type:	32-bit Integer
Default:

mds log
Description:
Type:	Boolean
Default:	true

mds log skip corrupt events
Description:
Type:	Boolean
Default:	false

mds log max events
Description:
Type:	32-bit Integer
Default:	-1

mds log max segments
Description:	// segment size defined by FileLayout above
Type:	32-bit Integer
Default:	30
描述： 上述FileLayout定义的段尺寸。

mds log max expiring
Description:
Type:	32-bit Integer
Default:	20

mds log eopen size
Description:	// # open inodes per log entry
Type:	32-bit Integer
Default:	100
描述：每个日志条目打开一个inode。

mds bal sample interval
Description:	// every 5 seconds
Type:	Float
Default:	3

mds bal replicate threshold
Description:
Type:	Float
Default:	8000

mds bal unreplicate threshold
Description:
Type:	Float
Default:	0

mds bal frag
Description:
Type:	Boolean
Default:	false

mds bal split size
Description:
Type:	32-bit Integer
Default:	10000

mds bal split rd
Description:
Type:	Float
Default:	25000

mds bal split wr
Description:
Type:	Float
Default:	10000

mds bal split bits
Description:
Type:	32-bit Integer
Default:	3

mds bal merge size
Description:
Type:	32-bit Integer
Default:	50

mds bal merge rd
Description:
Type:	Float
Default:	1000

mds bal merge wr
Description:
Type:	Float
Default:	1000

mds bal interval
Description:	// seconds
Type:	32-bit Integer
Default:	10

mds bal fragment interval
Description:	// seconds
Type:	32-bit Integer
Default:	5

mds bal idle threshold
Description:
Type:	Float
Default:	0

mds bal max
Description:
Type:	32-bit Integer
Default:	-1

mds bal max until
Description:
Type:	32-bit Integer
Default:	-1

mds bal mode
Description:
Type:	32-bit Integer
Default:	0

mds bal min rebalance
Description:	// must be x above avg before we export
Type:	Float
Default:	0.1
描述：导出前必须大于avg。

mds bal min start
Description:	// if we need less x. we don’t do anything
Type:	Float
Default:	0.2
描述：如果我们需要小点的x，则放任自流。

mds bal need min
Description:	// take within this range of what we need
Type:	Float
Default:	0.8
描述：拉回到我们需要的范围。

mds bal need max
Description:
Type:	Float
Default:	1.2

mds bal midchunk
Description:	// any sub bigger than this taken in full
Type:	Float
Default:	0.3
描述：把小于这个的子块当作满了。

mds bal minchunk
Description:	// never take anything smaller than this
Type:	Float
Default:	0.001

mds bal target removal min
Description:	// min bal iters before old target is removed
Type:	32-bit Integer
Default:	5

mds bal target removal max
Description:	// max bal iters before old target is removed
Type:	32-bit Integer
Default:	10

mds replay interval
Description:	// time to wait before starting replay again
Type:	Float
Default:	1

mds shutdown check
Description:
Type:	32-bit Integer
Default:	0

mds thrash exports
Description:
Type:	32-bit Integer
Default:	0

mds thrash fragments
Description:
Type:	32-bit Integer
Default:	0

mds dump cache on map
Description:
Type:	Boolean
Default:	false

mds dump cache after rejoin
Description:
Type:	Boolean
Default:	false

mds verify scatter
Description:
Type:	Boolean
Default:	false

mds debug scatterstat
Description:
Type:	Boolean
Default:	false

mds debug frag
Description:
Type:	Boolean
Default:	false

mds debug auth pins
Description:
Type:	Boolean
Default:	false

mds debug subtrees
Description:
Type:	Boolean
Default:	false

mds kill mdstable at
Description:
Type:	32-bit Integer
Default:	0

mds kill export at
Description:
Type:	32-bit Integer
Default:	0

mds kill import at
Description:
Type:	32-bit Integer
Default:	0

mds kill link at
Description:
Type:	32-bit Integer
Default:	0

mds kill rename at
Description:
Type:	32-bit Integer
Default:	0

mds wipe sessions
Description:
Type:	Boolean
Default:	0

mds wipe ino prealloc
Description:
Type:	Boolean
Default:	0

mds skip ino
Description:
Type:	32-bit Integer
Default:	0

max mds
Description:
Type:	32-bit Integer
Default:	1

mds standby for name
Description:
Type:	String
Default:

mds standby for rank
Description:
Type:	32-bit Integer
Default:	-1

mds standby replay
Description:
Type:	Boolean
Default:	false
 4.6  cephfs——ceph文件系统选项工具
cephfs – ceph file system options utility
 4.6.1  概述
SYNOPSIS
cephfs [ path command options ]
 4.6.2  描述
DESCRIPTION
cephfs is a control utility for accessing and manipulating file layout and location data in the Ceph distributed file system.
cephfs是个控制工具，用于访问和修改ceph分布式文件系统内的文件布局及位置信息。

Choose one of the following three commands:
可用命令有下面3个：

show_layout View the layout information on a file or directory
set_layout Set the layout information on a file or directory
show_location View the location information on a file

show_layout查看一个文件或目录的布局信息；
set_layout 设置一个文件或目录的布局信息；
show_location 查看一个文件的位置信息；
map 查看一个文件分片信息，包括其对象及所在归置组、OSD。
 4.6.3  选项
OPTIONS
Your applicable options differ depending on whether you are setting or viewing layout/location.
可用选项因你是否在设置或查看布局、位置而不同。
查看选项
VIEWING OPTIONS:
-l --offset
Specify an offset for which to retrieve location data
指定检索位置数据的偏移量。
设置选项
SETTING OPTIONS:
-u --stripe_unit
Set the size of each stripe
设置每个条带的大小

-c --stripe_count
Set the number of stripes per object
设置每对象条带数量

-s --object_size
Set the size of the objects to stripe across
设置条带化的对象大小。

-p --pool
Set the pool (by numeric value, not name!) to use
指定要使用的存储池（数字，不是名字！）

-o --osd
Set the preferred OSD to use as the primary
设置优先用的主OSD。
 4.6.4  限制条件
LIMITATIONS
When setting layout data, the specified stripe unit and stripe count must multiply to the size of an object. Any parameters you don’t set explicitly are left at the system defaults.
设置布局数据时，指定的条带单位和条带数必须乘以对象大小。任何未显式设置的参数都会按默认计。

Obviously setting the layout of a file and a directory means different things. Setting the layout of a file specifies exactly how to place the individual file. This must be done before writing any data to it. Truncating a file does not allow you to change the layout either.
很明显，设置一文件和一目录的布局含义不同。设置一文件的布局指示如何放置此文件，这必须在写入数据前完成；删节文件也不会更改其布局。

Setting the layout of a directory sets the “default layout”, which is used to set the file layouts on any files subsequently created in the directory (or any subdirectory). Pre-existing files do not have their layouts changed.
设置一目录的布局实际上设置了“默认布局”，此设置会影响此目录下后续创建的所有文件、子目录。已经存在的文件其布局不会变动。

You’ll notice that the layout information allows you to specify a preferred OSD for placement. This is allowed but is not recommended since it can dramatically unbalance your storage cluster’s space utilization.
你也许注意到了，布局信息允许你指定归置的优先OSD，这是可以的但不推荐，因为它会极大地破坏集群的使用空间均衡。
 4.6.5  可用范围
AVAILABILITY
cephfs is part of the Ceph distributed file system. Please refer to the Ceph documentation at http://ceph.com/docs for more information.
cephfs是ceph分布式文件系统的一部分，请参考位于http://ceph.com/docs 的ceph文档。

SEE ALSO¶
ceph(8)
 4.7  ceph-fuse——ceph的用户空间客户端
ceph-fuse – fuse-based client for ceph
 4.7.1  概述
SYNOPSIS
ceph-fuse [ -m monaddr:port ] mountpoint [ fuse options ]
 4.7.2  描述
DESCRIPTION
ceph-fuse is a FUSE (File system in USErspace) client for Ceph distributed file system. It will mount a ceph file system (specified via the -m option for described by ceph.conf (see below) at the specific mount point.
ceph-fuse是一个Ceph分布式文件系统的用户空间（FUSE, File system in USErspace）客户端，它会把ceph文件系统（用-m选项指定）挂载到指定挂载点。

The file system can be unmounted with:
用下面的命令卸载文件系统：
fusermount -u mountpoint

or by sending SIGINT to the ceph-fuse process.
或者向ceph-fuse进程发送SIGINT信号。
 4.7.3  选项
OPTIONS
Any options not recognized by ceph-fuse will be passed on to libfuse.
ceph-fuse不认识的选项会接着传向libfuse。

-d
Detach from console and daemonize after startup.
启动后从控制台分离，作为守护进程。

-c ceph.conf, --conf=ceph.conf
Use ceph.conf configuration file instead of the default /etc/ceph/ceph.conf to determine monitor addresses during startup.
用指定ceph.conf配置文件而非默认的/etc/ceph/ceph.conf来找出启动时需要的监视器地址。

-m monaddress[:port]
Connect to specified monitor (instead of looking through ceph.conf).
连接到指定监视器（而非通过ceph.conf寻找）。

-r root_directory
Use root_directory as the mounted root, rather than the full Ceph tree.
以root_directory作为挂载的根，而不是整个Ceph树。
 4.7.4  可用范围
AVAILABILITY
ceph-fuse is part of the Ceph distributed file system. Please refer to the Ceph documentation at http://ceph.com/docs for more information.
ceph-fuse是Ceph分布式文件系统的一部分，更多信息参见 http://ceph.com/docs。

SEE ALSO
fusermount(8), ceph(8)
 4.8  mount.ceph——挂载ceph文件系统
mount.ceph – mount a ceph file system
 4.8.1  概述
SYNOPSIS
mount.ceph monaddr1[,monaddr2,...]:/[subdir] dir [ -o options ]
 4.8.2  描述
DESCRIPTION
mount.ceph is a simple helper for mounting the Ceph file system on a Linux host. It serves to resolve monitor hostname(s) into IP addresses and read authentication keys from disk; the Linux kernel client component does most of the real work. In fact, it is possible to mount a non-authenticated Ceph file system without mount.ceph by specifying monitor address(es) by IP:
mount.ceph是个简单的助手程序，用于在Linux主机上挂载Ceph文件系统。其作用是把监视器主机名解析为IP地址、并从硬盘读取认证密钥，Linux内核客户端组件完成了大多数实际工作。事实上，如果用IP指定监视器，不需要mount.ceph就可以挂载无需认证的ceph文件系统：
mount -t ceph 1.2.3.4:/ mountpoint

Each monitor address monaddr takes the form host[:port]. If the port is not specified, the Ceph default of 6789 is assumed.
每个监视器地址monaddr格式都是host[:port]，如果未指定端口，就用默认端口6789。

Multiple monitor addresses can be separated by commas. Only one responsible monitor is needed to successfully mount; the client will learn about all monitors from any responsive monitor. However, it is a good idea to specify more than one in case one happens to be down at the time of mount.
多个监视器地址用逗号分隔。要成功地挂载，只要有一个可响应的监视器即可；客户端会从有响应的监视器学习到所有监视器。即便如此，最好还是指定多个监视器，以防挂载时它碰巧挂了。

A subdirectory subdir may be specified if a subset of the file system is to be mounted.
如果只想挂载一个文件系统子集，可以指定子目录subdir。

Mount helper application conventions dictate that the first two options are device to be mounted and destination path. Options must be passed only after these fixed arguments.
mount助手程序惯例要求头两个选项必须是要挂载的设备和目的路径，可选项必须在这些固定参数之后传入。
 4.8.3  选项
OPTIONS
wsize
int, max write size. Default: none (writeback uses smaller of wsize and stripe unit)
整数，最大写尺寸。默认：无（回写用了较小的wsize和条带单元）
rsize
int (bytes), max readahead, multiple of 1024, Default: 524288 (512*1024)
整数（字节），最大预读，1024的倍数。默认：524288（512*1024）
osdtimeout
int (seconds), Default: 60
整数，秒。默认60
osdkeepalivetimeout
int, Default: 5
mount_timeout
int (seconds), Default: 60
osd_idle_ttl
int (seconds), Default: 60
caps_wanted_delay_min
int, cap release delay, Default: 5
整数，能力释放延时。默认5
caps_wanted_delay_max
int, cap release delay, Default: 60
cap_release_safety
int, Default: calculated
readdir_max_entries
int, Default: 1024
readdir_max_bytes
int, Default: 524288 (512*1024)
write_congestion_kb
int (kb), max writeback in flight. scale with available memory. Default: calculated from available memory
整数（kb），行进中最大回写。随可用内存伸缩。默认：根据可用内存计算。
snapdirname
string, set the name of the hidden snapdir. Default: .snap
字符串，设置隐藏snapdir的名字。默认：.snap
name
RADOS user to authenticate as when using cephx. Default: guest
使用cephx是用以认证的RADOS用户名，默认：guest。
secret
secret key for use with cephx. This option is insecure because it exposes the secret on the command line. To avoid this, use the secretfile option.
用于cephx的密码，此选项不安全，因为它在命令行展示密码。要避免这种情况，改用cecretfile选项。
secretfile
path to file containing the secret key to use with cephx
用于cephx、包含着密码的文件路径。
ip
my ip
noshare
create a new client instance, instead of sharing an existing instance of a client mounting the same cluster
创建新客户端例程，而不是共享挂载了相同集群的已存在客户端例程。
dirstat
funky cat dirname for stats, Default: off
臭名昭著的cat dirname，用以获取状态。默认：off
nodirstat
no funky cat dirname for stats
不用臭名昭著的cat dirname获取状态信息。
rbytes
Report the recursive size of the directory contents for st_size on directories. Default: on
向目录的st_size报告目录内容的递归尺寸，默认：on
norbytes
Do not report the recursive size of the directory contents for st_size on directories.
不向目录的st_size报告目录内容的递归尺寸。
nocrc
no data crc on writes
写时不做数据crc校验
noasyncreaddir
no dcache readdir
 4.8.4  实例
EXAMPLES
Mount the full file system:
挂载整个文件系统：
mount.ceph monhost:/ /mnt/foo

If there are multiple monitors:
若有多个监视器：
mount.ceph monhost1,monhost2,monhost3:/ /mnt/foo

If ceph-mon(8) is running on a non-standard port:
如果ceph-mon(8)运行在非标准端口上：
mount.ceph monhost1:7000,monhost2:7000,monhost3:7000:/ /mnt/foo

To mount only part of the namespace:
只挂载名字空间的一部分：
mount.ceph monhost1:/some/small/thing /mnt/thing

Assuming mount.ceph(8) is installed properly, it should be automatically invoked by mount(8) like so:
假设mount.ceph(8)正确安装了，它会被mount(8)自动调用，像这样：
mount -t ceph monhost:/ /mnt/foo
 4.8.5  可用范围
AVAILABILITY
mount.ceph is part of the Ceph distributed file system. Please refer to the Ceph documentation at http://ceph.com/docs for more information.

SEE ALSO
ceph-fuse(8), ceph(8)
 4.9  libcephfs (javadoc)
View the auto-generated JavaDoc pages for the CephFS Java bindings.
浏览自动生成的文档： JavaDoc pages for the CephFS Java bindings。
 5  块设备
Block devices
A block is a sequence of bytes (for example, a 512-byte block of data). Block-based storage interfaces are the most common way to store data with rotating media such as hard disks, CDs, floppy disks, and even traditional 9-track tape. The ubiquity of block device interfaces makes a virtual block device an ideal candidate to interact with a mass data storage system like Ceph.
块是一个字节序列（例如，一个512字节的一块数据），基于块的存储接口是最常见的存储数据方法，它们基于旋转媒体，像硬盘、CD、软盘、甚至传统的9磁道磁带。无处不在的块设备接口使虚拟块设备成为与ceph这样的海量存储系统交互的理想之选。

Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. Ceph block devices leverage RADOS capabilities such as snapshotting, replication and consistency. Ceph’s RADOS Block Devices (RBD) interact with OSDs using kernel modules or the librbd library.
ceph块设备是精简的、大小可调且数据条带化到集群内的多个OSD。ceph块设备均衡多个RADOS能力，如快照、复制和一致性，ceph的RADOS块设备（RADOS Block Devices, RBD）用内核模块或librbd库与OSD交互。


Note: Kernel modules can use Linux page caching. For librbd-based applications, Ceph supports RBD Caching.
注意：内核模块可使用Linux页缓存。对基于librbd的应用程序，ceph可提供RBD缓存。

Ceph’s block devices deliver high performance with infinite scalability to kernel modules, or to KVMs such as Qemu, and cloud-based computing systems like OpenStack and CloudStack that rely on libvirt and Qemu to integrate with Ceph block devices. You can use the same cluster to operate the Ceph RADOS Gateway, the Ceph FS filesystem, and Ceph block devices simultaneously.
Ceph块设备靠无限伸缩性提供了高性能，如向内核模块、或向KVM（如Qemu、依赖libvirt和Qemu的OpenStack和CloudStack云计算系统都可与Ceph块设备集成）。你可以用同一个集群同时运行Ceph RADOS网关、CephFS文件系统、和ceph块设备。

Important To use RBD, you must have a running Ceph cluster.
重要：要使用RBD，你必须有一个在运行的ceph集群。
Commands
Kernel Modules
Snapshots
QEMU
libvirt
Cache Settings
OpenStack
CloudStack
Manpage rbd
Manpage ceph-rbdnamer
librbd

后续将补。。。
 6  RADOS网关
RADOS gateway
RADOS Gateway is an object storage interface built on top of librados to provide applications with a RESTful gateway to RADOS clusters. The RADOS Gateway supports two interfaces:
RADOS网关是构建于librados之上的对象存储接口，可为应用程序提供到RADOS集群的RESTful网关，它现在提供了2个接口。

S3-compatible: Provides block storage functionality with an interface that is compatible with a large subset of the Amazon S3 RESTful API.
Swift-compatible: Provides block storage functionality with an interface that is compatible with a large subset of the OpenStack Swift API.
S3兼容：用兼容大量亚马逊S3 RESTful API的接口提供了块存储功能。
Swift兼容：用兼容大量OpenStack Swift API的接口提供了块存储功能。

RADOS Gateway is a FastCGI module for interacting with librados. Since it provides interfaces compatible with OpenStack Swift and Amazon S3, RADOS Gateway has its own user management. RADOS Gateway can store data in the same RADOS cluster used to store data from Ceph FS clients or RADOS block devices. The S3 and Swift APIs share a common namespace, so you may write data with one API and retrieve it with the other.
RADOS网关是个与librados交互的FastCGI模块。因为它提供了与OpenStack Swift和Amazon S3兼容的接口，RADOS要有它自己的用户管理。RADOS网关可在存储Ceph FS客户端或RADOS块设备数据的集群里存储数据。S3和Swift API共用一个名字空间，所以你可以用一个API写、然后用另一个检索。



Note RADOS Gateway does NOT use the CephFS metadata server.
注意：RADOS网关不使用CephFS元数据服务器。

Manual Install
Configuration
Config Reference
Purging Temp Data
S3 API
Swift API
Admin API
Troubleshooting
Manpage radosgw
Manpage radosgw-admin
后续将补。。。
 7  API文档
后续将补。。。
 8  体系结构
Architecture
Ceph provides an infinitely scalable object storage system. It is based upon RADOS, which you can read about in RADOS - A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters. Its high-level features include providing a native interface to the object storage system via librados, and a number of service interfaces built on top of librados. These include:
CEPH提供了一个可无限伸缩的对象存储系统，它基于RADOS，见论文RADOS - A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters。它的高级功能包括：基于librados的对象存储系统原生接口、和多种服务接口，它们有：

Block Devices: The RADOS Block Device (RBD) service provides resizable, thin-provisioned block devices with snapshotting and cloning. Ceph stripes a block device across the cluster for high performance. Ceph supports both kernel objects (KO) and a QEMU hypervisor that uses librbd directly–avoiding the kernel object overhead for virtualized systems.
块设备：RBD服务提供了大小可调、精炼、支持快照和克隆的块设备。为提供高性能，ceph把块设备条带化。ceph同时支持直接使用librbd的内核对象（KO）和QEMU管理程序——避免了虚拟系统上的内核模块开销。
RESTful Gateway: The RADOS Gateway (RGW) service provides RESTful APIs with interfaces that are compatible with Amazon S3 and OpenStack Swift.
RESTful网关： RADOS网关（RADOS Gateway, RGW）服务提供了和Amazon S3和OpenStack Swift兼容的RESTful API。
Ceph FS: The Ceph Filesystem (CephFS) service provides a POSIX compliant filesystem usable with mount or as a filesytem in user space (FUSE).
Ceph文件系统：Ceph文件系统兼容POSIX，可以直接挂载或挂载为用户空间文件系统（FUSE）。

Ceph OSDs store all data–whether it comes through RBD, RGW, or CephFS–as objects in the object storage system. Ceph can run additional instances of OSDs, MDSs, and monitors for scalability and high availability. The following diagram depicts the high-level architecture.
Ceph的OSD把所有数据以对象形式保存在对象存储系统中——不论它来自RBD、RGW还是CephFS。Ceph可运行多个OSD、MDS和监视器例程，以保证伸缩性和高可用性。下面图解了其高级体系结构。

 8.1  消除局限性
Removing limitations

Today’s storage systems have demonstrated an ability to scale out, but with some significant limitations: interfaces, session managers, and stateful sessions with a centralized point of access often limit the scalability of today’s storage architectures. Furthermore, a centralized interface that dispatches requests from clients to server nodes within a cluster and subsequently routes responses from those server nodes back to clients will hit a scalability and/or performance limitation.
当今的存储系统已经表现出了扩张力，但是有着明显的局限性：接口、会话管理、和有状态会话都基于中央访问点，它通常限制了存储系统架构的伸缩性，另外，中央化的接口要把客户端请求调度到集群内的服务器节点，并把后续响应从服务器路由给客户端，这通常会碰到伸缩和/或性能问题。

Another problem for storage systems is the need to manually rebalance data when increasing or decreasing the size of a data cluster. Manual rebalancing works fine on small scales, but it is a nightmare at larger scales because hardware additions are common and hardware failure becomes an expectation rather than an exception when operating at the petabyte scale and beyond.
存储系统的另一个问题是增加、或减少数据集群尺寸时要手动重均衡数据，对小型集群来说手动均衡没什么问题，但对大型集群来说是噩梦，因为在PB级及更大的集群里，硬件增加和失败都是常态而非异常。

The operational challenges of managing legacy technologies with the burgeoning growth in the demand for unstructured storage makes legacy technologies inadequate for scaling into petabytes. Some legacy technologies (e.g., SAN) can be considerably more expensive, and more challenging to maintain when compared to using commodity hardware. Ceph uses commodity hardware, because it is substantially less expensive to purchase (or to replace), and it only requires standard system administration skills to use it.
急剧增长的非结构化存储需求使得用老技术不足以扩展到PB级。一些老技术（如SAN）和普通硬件相比相当昂贵、且不易维护，而ceph用普通硬件，因为购买（或替换）它很便宜，而且只需要基本的系统管理技能即可。
 8.2  ceph如何伸缩
How ceph scales
In traditional architectures, clients talk to a centralized component (e.g., a gateway, broker, API, facade, etc.), which acts as a single point of entry to a complex subsystem. This imposes a limit to both performance and scalability, while introducing a single point of failure (i.e., if the centralized component goes down, the whole system goes down, too).
在传统架构里，客户端沟通中央化的组件（如网关、中间件、API、facade等等），它作为一个复杂子系统的单接触点，它引入单故障点的同时，也压制了性能和伸缩性（例如，如果中央化组件挂了，整个系统就挂了）。

Ceph uses a new and innovative approach. Ceph clients contact a Ceph monitor and retrieve a copy of the cluster map. The CRUSH algorithm allows a client to compute where data should be stored, and enables the client to contact the primary OSD to store or retrieve the data. The OSD also uses the CRUSH algorithm, but the OSD uses it to compute where replicas of data should be stored (and for rebalancing). For a detailed discussion of CRUSH, see CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data
Ceph用了全新的方法，其客户端联系ceph监视器并获得一份集群运行图拷贝。CRUSH算法让客户端计算数据应该存在哪里、并允许它联系主OSD来存储或检索数据。OSD也用CRUSH算法，但是用于计算数据副本应该存到哪里（或用于重均衡）。关于CRUSH的详细讨论，见CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data。

The Ceph storage system supports the notion of ‘Pools’, which are logical partitions for storing object data. Pools set ownership/access, the number of object replicas, the number of placement groups, and the CRUSH rule set to use. Each pool has a number of placement groups that are mapped dynamically to OSDs. When clients store data, CRUSH maps the object data to placement groups. The following diagram depicts how CRUSH maps objects to placement groups, and placement groups to OSDs.
Ceph存储系统支持“池”概念，它是存储对象数据的逻辑分区。存储池设置了所有者及访问权限、对象副本数、归置组数量、要用的CRUSH规则集。每个存储池都有一定数量的归置组动态地映射到OSD，客户端存数据时，CRUSH把对象数据映射到归置组。下图描述了CRUSH如何把对象映射到归置组、然后归置组到OSD。



Mapping objects to placement groups instead of directly to OSDs creates a layer of indirection between the OSD and the client. The cluster must be able to grow (or shrink) and rebalance data dynamically. If the client “knew” which OSD had the data, that would create a tight coupling between the client and the OSD. Instead, the CRUSH algorithm maps the data to a placement group and then maps the placement group to one or more OSDs. This layer of indirection allows Ceph to rebalance dynamically when new OSDs come online.
把对象映射到归置组而不是直接到OSD，由此在OSD和客户端间产生了一个间接层。由于集群必须能增大或缩小、并动态地重均衡数据，如果客户端“知道”哪个OSD有数据，这将会导致客户端和OSD间密耦合，相反，CRUSH算法把数据映射到归置组、然后再把归置组映射到一或多个OSD。这个间接层可以让ceph在OSD上线时动态地重均衡。

With a copy of the cluster map and the CRUSH algorithm, the client can compute exactly which OSD to use when reading or writing a particular piece of data.
用一份集群运行图的拷贝和CRUSH算法，客户端能准确计算出到哪个OSD读、写数据片段。

In a typical write scenario, a client uses the CRUSH algorithm to compute where to store data, maps the data to a placement group, then looks at the CRUSH map to identify the primary OSD for the placement group. Clients write data to the identified placement group in the primary OSD. Then, the primary OSD with its own copy of the CRUSH map identifies the secondary and tertiary OSDs for replication purposes, and replicates the data to the appropriate placement groups in the secondary and tertiary OSDs (as many OSDs as additional replicas), and responds to the client once it has confirmed the data was stored successfully.
在一个典型的写入场景中，一客户端用CRUSH算法计算往哪里存数据、映射数据到归置组、然后参阅CRUSH图找到归置组的主OSD；客户端把数据写入目标归置组的主OSD，然后这个主OSD再用它的CRUSH图副本找出用于放副本的第二、第三个OSD，并把数据复制到适当的归置组所对应的第二、第三OSD（要多少副本就有多少OSD），最终，确认数据成功存储后反馈给客户端。



Since any network device has a limit to the number of concurrent connections it can support, a centralized system has a low physical limit at high scales. By enabling clients to contact nodes directly, Ceph increases both performance and total system capacity simultaneously, while removing a single point of failure. Ceph clients can maintain a session when they need to, and with a particular OSD instead of a centralized server.
由于任何网络设备都有其支持的最大并发连接限制，规模巨大时一个中央化的系统其物理局限性就暴露了。Ceph允许客户端直接和节点联系，这在消除单故障点的同时，提升了性能和系统总容量。Ceph客户端可按需维护和某OSD的会话，而不是一个中央服务器。
 8.3  邻居感应节点
Peer-aware nodes
Ceph’s cluster map determines whether a node in a network is in the Ceph cluster or out of the Ceph cluster.
Ceph的集群图可确定网络中的一个节点是在集群内还是在集群外。



In many clustered architectures, the primary purpose of cluster membership is so that a centralized interface knows which hosts it can access. Ceph takes it a step further: Ceph’s nodes are cluster aware. Each node knows about other nodes in the cluster. This enables Ceph’s monitor, OSD, and metadata server daemons to interact directly with each other. One major benefit of this approach is that Ceph can utilize the CPU and RAM of its nodes to easily perform tasks that would bog down a centralized server.
在很多集群化体系结构中，设计集群成员的主要目的是让中央接口知道它能访问哪些主机。ceph向前迈进了一步：ceph的节点会感知集群，每个节点都知道集群内的其它节点，这使得ceph的监视器、OSD、和元数据服务器守护进程可以直接互相沟通。这种方法的一个主要优势是，ceph能很容易地利用其节点的CPU和内存资源执行任务，而这样的任务会拖垮中央服务器。

Todo Explain OSD maps, Monitor Maps, MDS maps
待完成：解释OSD图、监视器图、MDS图。
 8.4  智能OSD
Smart osds
Ceph OSDs join a cluster and report on their status. At the lowest level, the OSD status is up or down reflecting whether or not it is running and able to service requests. If an OSD is down and in the cluster, this status may indicate the failure of the OSD.
Ceph的OSD加入集群后会持续报告自己的状态。在最低水平，OSD状态为up或down，反应了它是否在运行、能否提供服务。如果一个OSD状态为down且in，表明OSD可能有故障。

With peer awareness, OSDs can communicate with other OSDs and monitors to perform tasks. OSDs take client requests to read data from or write data to pools, which have placement groups. When a client makes a request to write data to a primary OSD, the primary OSD knows how to determine which OSDs have the placement groups for the replica copies, and then update those OSDs. This means that OSDs can also take requests from other OSDs. With multiple replicas of data across OSDs, OSDs can also “peer” to ensure that the placement groups are in sync. See Placement Group States and Placement Group Concepts for details.
有了邻居感知功能，为执行任务OSD能和其它OSD和监视器通讯。OSD根据客户端请求向存储池内的归置组读取或写入数据。当一客户端向某个主OSD请求写入数据时，主OSD知道如何确定哪个OSD持有用于副本的归置组，继而更新那些OSD。这意味着OSD也能接受其他OSD的请求，用散布于多个OSD的多份数据副本，OSD也能通过相互查询来确保归置组同步无误。详情见 Placement Group States和Placement Group Concepts。

If an OSD is not running (e.g., it crashes), the OSD cannot notify the monitor that it is down. The monitor can ping an OSD periodically to ensure that it is running. However, Ceph also empowers OSDs to determine if a neighboring OSD is down, to update the cluster map and to report it to the monitor(s). When an OSD is down, the data in the placement group is said to be degraded. If the OSD is down and in, but subsequently taken out of the cluster, the OSDs receive an update to the cluster map and rebalance the placement groups within the cluster automatically.
如果一OSD没在运行（比如，它崩溃了），这个OSD就不能告诉监视器它挂了。监视器可以周期性地ping一个OSD来确保它在运行，即便这样，ceph也授权OSD们探测它的邻居OSD是否还活着，以此来更新集群运行图并报告给监视器。一个OSD挂了时，对应归置组里的数据就降级了，如果OSD状态为down且in，但随后被踢出了集群，其余OSD会收到一个集群运行图更新，并自动重均衡集群内的归置组。

OSDs store all data as objects in a flat namespace (e.g., no hierarchy of directories). An object has an identifier, binary data, and metadata consisting of a set of name/value pairs. The semantics are completely up to the client. For example, CephFS uses metadata to store file attributes such as the file owner, created date, last modified date, and so forth.
OSD在扁平的名字空间里把所有数据存储为对象（例如，没有目录层次）。对象包含一个标识符、二进制数据、和由名字/值配对组成的元数据，语义完全取决于客户端。例如，CephFS用元数据存储文件属性，如文件所有者、创建日期、最后修改日期等等。



As part of maintaining data consistency and cleanliness, Ceph OSDs can also scrub the data. That is, Ceph OSDs can compare object metadata across replicas to catch OSD bugs or filesystem errors (daily). OSDs can also do deeper scrubbing by comparing data in objects bit-for-bit to find bad sectors on a disk that weren’t apparent in a light scrub (weekly).
作为维护工作的一部分，不但要保证数据的一致性和清洁性，OSD也能洗刷数据，就是说，Ceph OSD能比较对象元数据的几个副本以捕捉OSD缺陷或文件系统错误（每天）。OSD也能做深度洗刷，即按字节比较对象中的数据以找出轻度洗刷（每周）时未发现的硬盘坏扇区。

Todo explain “classes”
待完成：解释“classes”
 8.5  监视器法定人数
Monitor quorums
Ceph’s monitors maintain a master copy of the cluster map. So Ceph daemons and clients merely contact the monitor periodically to ensure they have the most recent copy of the cluster map. Ceph monitors are light-weight processes, but for added reliability and fault tolerance, Ceph supports distributed monitors. Ceph must have agreement among various monitor instances regarding the state of the cluster. To establish a consensus, Ceph always uses an odd number of monitors (e.g., 1, 3, 5, 7, etc) and the Paxos algorithm in order to establish a consensus.
ceph的监视器维护着集群运行图的原稿，所以ceph守护进程和客户端只是周期性地联系监视器以保证他们有运行图的最新副本。ceph监视器是轻量级进程，但是增强了可靠性和容错能力，ceph支持分布式监视器。不管Ceph集群状态如何，其不同监视器例程必须达成一致，为此，ceph总是使用奇数个监视器（如：1、3、5、7……）和Paxos算法来达成一致意见。
 8.6  mds
The Ceph filesystem service is provided by a daemon called ceph-mds. It uses RADOS to store all the filesystem metadata (directories, file ownership, access modes, etc), and directs clients to access RADOS directly for the file contents. The Ceph filesystem aims for POSIX compatibility. ceph-mds can run as a single process, or it can be distributed out to multiple physical machines, either for high availability or for scalability.
Ceph文件系统服务由名为ceph-mds的守护进程提供，它使用RADOS存储所有文件系统元数据（目录、文件所有者、访问权限等等），并指导客户端直接访问RADOS获取文件内容。Ceph力争兼容POSIX。ceph-mds可以只运行一个，也可以分布于多台物理机器，以获得高可用性或伸缩性。

High Availability: The extra ceph-mds instances can be standby, ready to take over the duties of any failed ceph-mds that was active. This is easy because all the data, including the journal, is stored on RADOS. The transition is triggered automatically by ceph-mon.
高可用性：多余的ceph-mds例程可处于standby（待命）状态，随时准备替下之前处于active（活跃）状态的失败ceph-mds。这可以轻易做到，因为所有数据、包括日志都存储在RADOS上，这个转换过程由ceph-mon自动触发。
Scalability: Multiple ceph-mds instances can be active, and they will split the directory tree into subtrees (and shards of a single busy directory), effectively balancing the load amongst all active servers.
可伸缩性：多个ceph-mds例程可以处于active状态，并且它们会把目录树分割为子树（和单个忙碌目录的碎片），在所有活跃服务器间高效地均衡负载。
译者曰：虽然文档这么说，但实践中还不推荐这样做，mds稳定性尚不理想。多个活跃的mds远没一个稳定，即便如此，您也应该先配置起几个mds备用。

Combinations of standby and active etc are possible, for example running 3 active ceph-mds instances for scaling, and one standby intance for high availability.
待命和活跃MDS可组合，例如，运行3个活跃ceph-mds例程以实现扩展、和1个待命例程以实现高可用性。
 8.7  客户端接口
Client interfaces
 8.7.1  认证和授权
Authentication and authorization
Ceph clients can authenticate their users with Ceph monitors, OSDs and metadata servers. Authenticated users gain authorization to read, write and execute Ceph commands. The Cephx authentication system is similar to Kerberos, but avoids a single point of failure to ensure scalability and high availability. For details on Cephx, see Ceph Authentication and Authorization.
ceph客户端能通过ceph监视器、OSD和元数据服务器认证它们的用户，通过认证的用户获得读、写和执行ceph命令的授权。cephx认证系统和Kerberos相似，但消除了单故障点，进而保证了可扩展性和高可用性。关于cephx的细节，参见认证概览。
 8.7.2  librados
Todo Snapshotting, Import/Export, Backup
Todo native APIs
待完成：拍快照、导入/导出、备份。
待完成：原生API。
 8.7.3  RBD
RBD stripes a block device image over multiple objects in the cluster, where each object gets mapped to a placement group and distributed, and the placement groups are spread across separate ceph-osd daemons throughout the cluster.
RBD把一个设备映像条带化到集群内的多个对象，其中每个对象都被映射到一个归置组并分布出去，这些归置组会散播到整个集群的某些ceph-osd守护进程。

Important Striping allows RBD block devices to perform better than a single server could!
重要：条带化会使RBD块设备比单台服务器运行的更好。

RBD’s thin-provisioned snapshottable block devices are an attractive option for virtualization and cloud computing. In virtual machine scenarios, people typically deploy RBD with the rbd network storage driver in Qemu/KVM, where the host machine uses librbd to provide a block device service to the guest. Many cloud computing stacks use libvirt to integrate with hypervisors. You can use RBD thin-provisioned block devices with Qemu and libvirt to support OpenStack and CloudStack among other solutions.
RBD的简配、可快照块设备对虚拟化和云计算很有吸引力。在虚拟机场景中，人们一般会用Qemu/KVM中的rbd网络驱动部署RBD，虚拟主机用librbd向客户提供块设备服务；很多云计算堆栈用libvirt和管理程序集成，你可以用RBD的简配块设备搭配Qemu和libvirt来支持OpenStack和CloudStack。

While we do not provide librbd support with other hypervisors at this time, you may also use RBD kernel objects to provide a block device to a client. Other virtualization technologies such as Xen can access the RBD kernel object(s). This is done with the command-line tool rbd.
现在我们还没对其它虚拟机管理程序提供librbd支持，你可以用RBD内核对象把块设备提供给客户端。其它虚拟化技术，像Xen能访问RBD内核对象，这可以用命令行工具rbd来实现。
 8.7.4  RGW
The RADOS Gateway daemon, radosgw, is a FastCGI service that provides a RESTful HTTP API to store objects and metadata. It layers on top of RADOS with its own data formats, and maintains its own user database, authentication, and access control. The RADOS Gateway uses a unified namespace, which means you can use either the OpenStack Swift-compatible API or the Amazon S3-compatible API. For example, you can write data using the S3-comptable API with one application and then read data using the Swift-compatible API with another application.
RADOS网关守护进程是radosgw，它是一个FastCGI服务，提供了RESTful HTTP API用于存储对象和元数据。它坐落于RADOS之上，有自己的数据格式，并维护着自己的用户数据库、认证、和访问控制。RADOS网关使用统一的名字空间，也就是说，你可以用OpenStack Swift兼容的API或者Amazon S3兼容的API；例如，你可以用一个程序以S3兼容API写入数据、然后用另一个程序以Swift兼容API读出。

See RADOS Gateway for details.
详情参见RADOS网关。
 8.7.5  cephfs
Todo cephfs, ceph-fuse
待完成：cephfs、ceph-fuse
